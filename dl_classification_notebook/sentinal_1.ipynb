{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f202b00-6e43-4747-9e26-6758489ba623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import gc\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "gc.collect()  # Clear unused memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b6d296-31f2-4240-a57b-2b4e89421b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .h5 file into memory once\n",
    "h5_file_path_train = r\"C:\\Users\\nadav.k\\Documents\\DS\\DL_classification\\classification_data\\training.h5\"\n",
    "h5_file_path_test = r\"C:\\Users\\nadav.k\\Documents\\DS\\DL_classification\\classification_data\\testing.h5\"\n",
    "\n",
    "# Open the H5 files\n",
    "h5_train = h5py.File(h5_file_path_train, 'r')\n",
    "h5_test = h5py.File(h5_file_path_test, 'r')\n",
    "\n",
    "# Extract datasets\n",
    "train_sen1_data = h5_train['sen1']\n",
    "#train_sen2_data = h5_train['sen2']\n",
    "train_labels = h5_train['new_labels']\n",
    "\n",
    "test_sen1_data = h5_test['sen1']\n",
    "#test_sen2_data = h5_test['sen2']\n",
    "test_labels = h5_test['new_labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc587cb-aa92-4542-becd-93fc0d964e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentinel1Dataset(Dataset):\n",
    "    def __init__(self, sen1_data, labels):\n",
    "        self.sen1_data = sen1_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract Sentinel-1 image and label\n",
    "        sen1_image = self.sen1_data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        sen1_image = torch.tensor(sen1_image, dtype=torch.float32).permute(2, 0, 1)\n",
    "\n",
    "        # Convert one-hot encoded label to class index\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        label = torch.argmax(label).long()\n",
    "\n",
    "        return sen1_image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7290165f-767a-4cb0-8d32-484a54edda9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = Sentinel1Dataset(train_sen1_data, train_labels)\n",
    "test_dataset = Sentinel1Dataset(test_sen1_data, test_labels)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af72eee8-93ad-4e93-8731-2907c8cc6804",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentinel1ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=9):\n",
    "        super(Sentinel1ConvNet, self).__init__()\n",
    "\n",
    "        # Sentinel-1 branch\n",
    "        self.conv1 = nn.Conv2d(8, 32, kernel_size=3, padding=1)\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.dropout2 = nn.Dropout(p=0.25)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n",
    "        self.fc1_dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc2_dropout = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sentinel-1 forward pass\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc1_dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc2_dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bc19835-511d-4c36-979e-45bca8aa419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with visualization and memory clearing\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    train_losses = []  # List to store training loss for visualization\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i, (sen1, labels) in enumerate(train_loader):\n",
    "            sen1, labels = sen1.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sen1)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Clear memory for each batch (optional but not usually necessary here)\n",
    "            del outputs, loss\n",
    "\n",
    "            if i % 100 == 99:  # Print every 100 mini-batches\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Average loss for the epoch\n",
    "        epoch_loss /= len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}] Average Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        # Clear unused memory after each epoch\n",
    "        torch.cuda.empty_cache()  # Clear GPU memory\n",
    "        gc.collect()  # Trigger garbage collection for CPU memory\n",
    "\n",
    "    print('Training complete')\n",
    "\n",
    "    # Visualization of training loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o', label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2a944cd-1dd6-499f-a423-c896286faee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sen1, labels in test_loader:\n",
    "            sen1, labels = sen1.to(device), labels.to(device)\n",
    "            outputs = model(sen1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    print(f'Average Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Visualization (optional)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(['Loss', 'Accuracy'], [avg_loss, accuracy])\n",
    "    plt.title('Evaluation Results')\n",
    "    plt.ylabel('Value')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f2058e0-4b17-47f1-9625-5d906d23089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = Sentinel1ConvNet(num_classes=9)  # Adjust num_classes based on your dataset\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5ee8278-26b1-4b53-9f2d-7eb8f1778b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [100/5013], Loss: 1.8074\n",
      "Epoch [1/50], Step [200/5013], Loss: 1.5318\n",
      "Epoch [1/50], Step [300/5013], Loss: 1.4269\n",
      "Epoch [1/50], Step [400/5013], Loss: 1.3683\n",
      "Epoch [1/50], Step [500/5013], Loss: 1.2941\n",
      "Epoch [1/50], Step [600/5013], Loss: 1.2783\n",
      "Epoch [1/50], Step [700/5013], Loss: 1.2619\n",
      "Epoch [1/50], Step [800/5013], Loss: 1.2138\n",
      "Epoch [1/50], Step [900/5013], Loss: 1.1875\n",
      "Epoch [1/50], Step [1000/5013], Loss: 1.1614\n",
      "Epoch [1/50], Step [1100/5013], Loss: 1.1264\n",
      "Epoch [1/50], Step [1200/5013], Loss: 1.1470\n",
      "Epoch [1/50], Step [1300/5013], Loss: 1.1238\n",
      "Epoch [1/50], Step [1400/5013], Loss: 1.1060\n",
      "Epoch [1/50], Step [1500/5013], Loss: 1.1020\n",
      "Epoch [1/50], Step [1600/5013], Loss: 1.1039\n",
      "Epoch [1/50], Step [1700/5013], Loss: 1.0654\n",
      "Epoch [1/50], Step [1800/5013], Loss: 1.0846\n",
      "Epoch [1/50], Step [1900/5013], Loss: 1.0502\n",
      "Epoch [1/50], Step [2000/5013], Loss: 1.0604\n",
      "Epoch [1/50], Step [2100/5013], Loss: 1.0384\n",
      "Epoch [1/50], Step [2200/5013], Loss: 1.0754\n",
      "Epoch [1/50], Step [2300/5013], Loss: 1.0390\n",
      "Epoch [1/50], Step [2400/5013], Loss: 1.0130\n",
      "Epoch [1/50], Step [2500/5013], Loss: 1.0257\n",
      "Epoch [1/50], Step [2600/5013], Loss: 1.0149\n",
      "Epoch [1/50], Step [2700/5013], Loss: 1.0524\n",
      "Epoch [1/50], Step [2800/5013], Loss: 1.0345\n",
      "Epoch [1/50], Step [2900/5013], Loss: 0.9962\n",
      "Epoch [1/50], Step [3000/5013], Loss: 1.0051\n",
      "Epoch [1/50], Step [3100/5013], Loss: 1.0282\n",
      "Epoch [1/50], Step [3200/5013], Loss: 1.0014\n",
      "Epoch [1/50], Step [3300/5013], Loss: 0.9988\n",
      "Epoch [1/50], Step [3400/5013], Loss: 0.9966\n",
      "Epoch [1/50], Step [3500/5013], Loss: 0.9822\n",
      "Epoch [1/50], Step [3600/5013], Loss: 0.9886\n",
      "Epoch [1/50], Step [3700/5013], Loss: 1.0017\n",
      "Epoch [1/50], Step [3800/5013], Loss: 0.9959\n",
      "Epoch [1/50], Step [3900/5013], Loss: 0.9578\n",
      "Epoch [1/50], Step [4000/5013], Loss: 0.9850\n",
      "Epoch [1/50], Step [4100/5013], Loss: 0.9717\n",
      "Epoch [1/50], Step [4200/5013], Loss: 0.9706\n",
      "Epoch [1/50], Step [4300/5013], Loss: 0.9787\n",
      "Epoch [1/50], Step [4400/5013], Loss: 0.9934\n",
      "Epoch [1/50], Step [4500/5013], Loss: 0.9687\n",
      "Epoch [1/50], Step [4600/5013], Loss: 0.9515\n",
      "Epoch [1/50], Step [4700/5013], Loss: 0.9513\n",
      "Epoch [1/50], Step [4800/5013], Loss: 0.9383\n",
      "Epoch [1/50], Step [4900/5013], Loss: 0.9614\n",
      "Epoch [1/50], Step [5000/5013], Loss: 0.9493\n",
      "Epoch [1/50] Average Loss: 1.0872\n",
      "Epoch [2/50], Step [100/5013], Loss: 0.9607\n",
      "Epoch [2/50], Step [200/5013], Loss: 0.9484\n",
      "Epoch [2/50], Step [300/5013], Loss: 0.9443\n",
      "Epoch [2/50], Step [400/5013], Loss: 0.9384\n",
      "Epoch [2/50], Step [500/5013], Loss: 0.9341\n",
      "Epoch [2/50], Step [600/5013], Loss: 0.9497\n",
      "Epoch [2/50], Step [700/5013], Loss: 0.9292\n",
      "Epoch [2/50], Step [800/5013], Loss: 0.9186\n",
      "Epoch [2/50], Step [900/5013], Loss: 0.9364\n",
      "Epoch [2/50], Step [1000/5013], Loss: 0.9323\n",
      "Epoch [2/50], Step [1100/5013], Loss: 0.9443\n",
      "Epoch [2/50], Step [1200/5013], Loss: 0.9326\n",
      "Epoch [2/50], Step [1300/5013], Loss: 0.9299\n",
      "Epoch [2/50], Step [1400/5013], Loss: 0.9259\n",
      "Epoch [2/50], Step [1500/5013], Loss: 0.9508\n",
      "Epoch [2/50], Step [1600/5013], Loss: 0.9125\n",
      "Epoch [2/50], Step [1700/5013], Loss: 0.9372\n",
      "Epoch [2/50], Step [1800/5013], Loss: 0.9215\n",
      "Epoch [2/50], Step [1900/5013], Loss: 0.9499\n",
      "Epoch [2/50], Step [2000/5013], Loss: 0.9218\n",
      "Epoch [2/50], Step [2100/5013], Loss: 0.8983\n",
      "Epoch [2/50], Step [2200/5013], Loss: 0.9216\n",
      "Epoch [2/50], Step [2300/5013], Loss: 0.9444\n",
      "Epoch [2/50], Step [2400/5013], Loss: 0.9226\n",
      "Epoch [2/50], Step [2500/5013], Loss: 0.9054\n",
      "Epoch [2/50], Step [2600/5013], Loss: 0.9377\n",
      "Epoch [2/50], Step [2700/5013], Loss: 0.9096\n",
      "Epoch [2/50], Step [2800/5013], Loss: 0.9005\n",
      "Epoch [2/50], Step [2900/5013], Loss: 0.9036\n",
      "Epoch [2/50], Step [3000/5013], Loss: 0.9148\n",
      "Epoch [2/50], Step [3100/5013], Loss: 0.8743\n",
      "Epoch [2/50], Step [3200/5013], Loss: 0.9001\n",
      "Epoch [2/50], Step [3300/5013], Loss: 0.9377\n",
      "Epoch [2/50], Step [3400/5013], Loss: 0.9083\n",
      "Epoch [2/50], Step [3500/5013], Loss: 0.9142\n",
      "Epoch [2/50], Step [3600/5013], Loss: 0.9046\n",
      "Epoch [2/50], Step [3700/5013], Loss: 0.8977\n",
      "Epoch [2/50], Step [3800/5013], Loss: 0.9072\n",
      "Epoch [2/50], Step [3900/5013], Loss: 0.8976\n",
      "Epoch [2/50], Step [4000/5013], Loss: 0.8970\n",
      "Epoch [2/50], Step [4100/5013], Loss: 0.9094\n",
      "Epoch [2/50], Step [4200/5013], Loss: 0.8850\n",
      "Epoch [2/50], Step [4300/5013], Loss: 0.9074\n",
      "Epoch [2/50], Step [4400/5013], Loss: 0.8845\n",
      "Epoch [2/50], Step [4500/5013], Loss: 0.9490\n",
      "Epoch [2/50], Step [4600/5013], Loss: 0.8919\n",
      "Epoch [2/50], Step [4700/5013], Loss: 0.8707\n",
      "Epoch [2/50], Step [4800/5013], Loss: 0.8724\n",
      "Epoch [2/50], Step [4900/5013], Loss: 0.9028\n",
      "Epoch [2/50], Step [5000/5013], Loss: 0.8850\n",
      "Epoch [2/50] Average Loss: 0.9174\n",
      "Epoch [3/50], Step [100/5013], Loss: 0.8638\n",
      "Epoch [3/50], Step [200/5013], Loss: 0.8678\n",
      "Epoch [3/50], Step [300/5013], Loss: 0.8691\n",
      "Epoch [3/50], Step [400/5013], Loss: 0.8559\n",
      "Epoch [3/50], Step [500/5013], Loss: 0.9000\n",
      "Epoch [3/50], Step [600/5013], Loss: 0.8726\n",
      "Epoch [3/50], Step [700/5013], Loss: 0.8827\n",
      "Epoch [3/50], Step [800/5013], Loss: 0.8576\n",
      "Epoch [3/50], Step [900/5013], Loss: 0.9043\n",
      "Epoch [3/50], Step [1000/5013], Loss: 0.8653\n",
      "Epoch [3/50], Step [1100/5013], Loss: 0.8600\n",
      "Epoch [3/50], Step [1200/5013], Loss: 0.8786\n",
      "Epoch [3/50], Step [1300/5013], Loss: 0.8789\n",
      "Epoch [3/50], Step [1400/5013], Loss: 0.8868\n",
      "Epoch [3/50], Step [1500/5013], Loss: 0.8821\n",
      "Epoch [3/50], Step [1600/5013], Loss: 0.8648\n",
      "Epoch [3/50], Step [1700/5013], Loss: 0.8692\n",
      "Epoch [3/50], Step [1800/5013], Loss: 0.8848\n",
      "Epoch [3/50], Step [1900/5013], Loss: 0.8746\n",
      "Epoch [3/50], Step [2000/5013], Loss: 0.8614\n",
      "Epoch [3/50], Step [2100/5013], Loss: 0.9276\n",
      "Epoch [3/50], Step [2200/5013], Loss: 0.8608\n",
      "Epoch [3/50], Step [2300/5013], Loss: 0.8491\n",
      "Epoch [3/50], Step [2400/5013], Loss: 0.8747\n",
      "Epoch [3/50], Step [2500/5013], Loss: 0.8586\n",
      "Epoch [3/50], Step [2600/5013], Loss: 0.8487\n",
      "Epoch [3/50], Step [2700/5013], Loss: 0.8698\n",
      "Epoch [3/50], Step [2800/5013], Loss: 0.8601\n",
      "Epoch [3/50], Step [2900/5013], Loss: 0.8801\n",
      "Epoch [3/50], Step [3000/5013], Loss: 0.8773\n",
      "Epoch [3/50], Step [3100/5013], Loss: 0.8799\n",
      "Epoch [3/50], Step [3200/5013], Loss: 0.8659\n",
      "Epoch [3/50], Step [3300/5013], Loss: 0.8911\n",
      "Epoch [3/50], Step [3400/5013], Loss: 0.8865\n",
      "Epoch [3/50], Step [3500/5013], Loss: 0.8670\n",
      "Epoch [3/50], Step [3600/5013], Loss: 0.8702\n",
      "Epoch [3/50], Step [3700/5013], Loss: 0.8521\n",
      "Epoch [3/50], Step [3800/5013], Loss: 0.8521\n",
      "Epoch [3/50], Step [3900/5013], Loss: 0.8511\n",
      "Epoch [3/50], Step [4000/5013], Loss: 0.8624\n",
      "Epoch [3/50], Step [4100/5013], Loss: 0.8268\n",
      "Epoch [3/50], Step [4200/5013], Loss: 0.8509\n",
      "Epoch [3/50], Step [4300/5013], Loss: 0.8705\n",
      "Epoch [3/50], Step [4400/5013], Loss: 0.8582\n",
      "Epoch [3/50], Step [4500/5013], Loss: 0.8425\n",
      "Epoch [3/50], Step [4600/5013], Loss: 0.8583\n",
      "Epoch [3/50], Step [4700/5013], Loss: 0.8266\n",
      "Epoch [3/50], Step [4800/5013], Loss: 0.8305\n",
      "Epoch [3/50], Step [4900/5013], Loss: 0.8662\n",
      "Epoch [3/50], Step [5000/5013], Loss: 0.8644\n",
      "Epoch [3/50] Average Loss: 0.8671\n",
      "Epoch [4/50], Step [100/5013], Loss: 0.8361\n",
      "Epoch [4/50], Step [200/5013], Loss: 0.8438\n",
      "Epoch [4/50], Step [300/5013], Loss: 0.8797\n",
      "Epoch [4/50], Step [400/5013], Loss: 0.8455\n",
      "Epoch [4/50], Step [500/5013], Loss: 0.8523\n",
      "Epoch [4/50], Step [600/5013], Loss: 0.8348\n",
      "Epoch [4/50], Step [700/5013], Loss: 0.8596\n",
      "Epoch [4/50], Step [800/5013], Loss: 0.8534\n",
      "Epoch [4/50], Step [900/5013], Loss: 0.8480\n",
      "Epoch [4/50], Step [1000/5013], Loss: 0.8285\n",
      "Epoch [4/50], Step [1100/5013], Loss: 0.8401\n",
      "Epoch [4/50], Step [1200/5013], Loss: 0.8401\n",
      "Epoch [4/50], Step [1300/5013], Loss: 0.8402\n",
      "Epoch [4/50], Step [1400/5013], Loss: 0.8597\n",
      "Epoch [4/50], Step [1500/5013], Loss: 0.8393\n",
      "Epoch [4/50], Step [1600/5013], Loss: 0.8446\n",
      "Epoch [4/50], Step [1700/5013], Loss: 0.8265\n",
      "Epoch [4/50], Step [1800/5013], Loss: 0.8425\n",
      "Epoch [4/50], Step [1900/5013], Loss: 0.8317\n",
      "Epoch [4/50], Step [2000/5013], Loss: 0.8425\n",
      "Epoch [4/50], Step [2100/5013], Loss: 0.8573\n",
      "Epoch [4/50], Step [2200/5013], Loss: 0.8650\n",
      "Epoch [4/50], Step [2300/5013], Loss: 0.7956\n",
      "Epoch [4/50], Step [2400/5013], Loss: 0.8280\n",
      "Epoch [4/50], Step [2500/5013], Loss: 0.8387\n",
      "Epoch [4/50], Step [2600/5013], Loss: 0.8404\n",
      "Epoch [4/50], Step [2700/5013], Loss: 0.8465\n",
      "Epoch [4/50], Step [2800/5013], Loss: 0.8198\n",
      "Epoch [4/50], Step [2900/5013], Loss: 0.8179\n",
      "Epoch [4/50], Step [3000/5013], Loss: 0.8316\n",
      "Epoch [4/50], Step [3100/5013], Loss: 0.8268\n",
      "Epoch [4/50], Step [3200/5013], Loss: 0.8477\n",
      "Epoch [4/50], Step [3300/5013], Loss: 0.8298\n",
      "Epoch [4/50], Step [3400/5013], Loss: 0.8398\n",
      "Epoch [4/50], Step [3500/5013], Loss: 0.8401\n",
      "Epoch [4/50], Step [3600/5013], Loss: 0.8363\n",
      "Epoch [4/50], Step [3700/5013], Loss: 0.8326\n",
      "Epoch [4/50], Step [3800/5013], Loss: 0.8167\n",
      "Epoch [4/50], Step [3900/5013], Loss: 0.8360\n",
      "Epoch [4/50], Step [4000/5013], Loss: 0.8617\n",
      "Epoch [4/50], Step [4100/5013], Loss: 0.8366\n",
      "Epoch [4/50], Step [4200/5013], Loss: 0.8398\n",
      "Epoch [4/50], Step [4300/5013], Loss: 0.8338\n",
      "Epoch [4/50], Step [4400/5013], Loss: 0.8477\n",
      "Epoch [4/50], Step [4500/5013], Loss: 0.8361\n",
      "Epoch [4/50], Step [4600/5013], Loss: 0.8087\n",
      "Epoch [4/50], Step [4700/5013], Loss: 0.8283\n",
      "Epoch [4/50], Step [4800/5013], Loss: 0.8099\n",
      "Epoch [4/50], Step [4900/5013], Loss: 0.8254\n",
      "Epoch [4/50], Step [5000/5013], Loss: 0.8302\n",
      "Epoch [4/50] Average Loss: 0.8378\n",
      "Epoch [5/50], Step [100/5013], Loss: 0.8099\n",
      "Epoch [5/50], Step [200/5013], Loss: 0.8103\n",
      "Epoch [5/50], Step [300/5013], Loss: 0.8184\n",
      "Epoch [5/50], Step [400/5013], Loss: 0.8282\n",
      "Epoch [5/50], Step [500/5013], Loss: 0.8466\n",
      "Epoch [5/50], Step [600/5013], Loss: 0.8312\n",
      "Epoch [5/50], Step [700/5013], Loss: 0.8215\n",
      "Epoch [5/50], Step [800/5013], Loss: 0.8302\n",
      "Epoch [5/50], Step [900/5013], Loss: 0.8414\n",
      "Epoch [5/50], Step [1000/5013], Loss: 0.8147\n",
      "Epoch [5/50], Step [1100/5013], Loss: 0.8242\n",
      "Epoch [5/50], Step [1200/5013], Loss: 0.8291\n",
      "Epoch [5/50], Step [1300/5013], Loss: 0.8113\n",
      "Epoch [5/50], Step [1400/5013], Loss: 0.8204\n",
      "Epoch [5/50], Step [1500/5013], Loss: 0.8258\n",
      "Epoch [5/50], Step [1600/5013], Loss: 0.8088\n",
      "Epoch [5/50], Step [1700/5013], Loss: 0.8148\n",
      "Epoch [5/50], Step [1800/5013], Loss: 0.8294\n",
      "Epoch [5/50], Step [1900/5013], Loss: 0.8286\n",
      "Epoch [5/50], Step [2000/5013], Loss: 0.8150\n",
      "Epoch [5/50], Step [2100/5013], Loss: 0.8063\n",
      "Epoch [5/50], Step [2200/5013], Loss: 0.7995\n",
      "Epoch [5/50], Step [2300/5013], Loss: 0.7866\n",
      "Epoch [5/50], Step [2400/5013], Loss: 0.8062\n",
      "Epoch [5/50], Step [2500/5013], Loss: 0.7915\n",
      "Epoch [5/50], Step [2600/5013], Loss: 0.8193\n",
      "Epoch [5/50], Step [2700/5013], Loss: 0.7966\n",
      "Epoch [5/50], Step [2800/5013], Loss: 0.8123\n",
      "Epoch [5/50], Step [2900/5013], Loss: 0.8084\n",
      "Epoch [5/50], Step [3000/5013], Loss: 0.8277\n",
      "Epoch [5/50], Step [3100/5013], Loss: 0.8563\n",
      "Epoch [5/50], Step [3200/5013], Loss: 0.8233\n",
      "Epoch [5/50], Step [3300/5013], Loss: 0.8148\n",
      "Epoch [5/50], Step [3400/5013], Loss: 0.8104\n",
      "Epoch [5/50], Step [3500/5013], Loss: 0.8172\n",
      "Epoch [5/50], Step [3600/5013], Loss: 0.8291\n",
      "Epoch [5/50], Step [3700/5013], Loss: 0.8418\n",
      "Epoch [5/50], Step [3800/5013], Loss: 0.8147\n",
      "Epoch [5/50], Step [3900/5013], Loss: 0.8091\n",
      "Epoch [5/50], Step [4000/5013], Loss: 0.8121\n",
      "Epoch [5/50], Step [4100/5013], Loss: 0.8173\n",
      "Epoch [5/50], Step [4200/5013], Loss: 0.8111\n",
      "Epoch [5/50], Step [4300/5013], Loss: 0.8220\n",
      "Epoch [5/50], Step [4400/5013], Loss: 0.8086\n",
      "Epoch [5/50], Step [4500/5013], Loss: 0.7933\n",
      "Epoch [5/50], Step [4600/5013], Loss: 0.8176\n",
      "Epoch [5/50], Step [4700/5013], Loss: 0.8121\n",
      "Epoch [5/50], Step [4800/5013], Loss: 0.8306\n",
      "Epoch [5/50], Step [4900/5013], Loss: 0.8484\n",
      "Epoch [5/50], Step [5000/5013], Loss: 0.8256\n",
      "Epoch [5/50] Average Loss: 0.8187\n",
      "Epoch [6/50], Step [100/5013], Loss: 0.8239\n",
      "Epoch [6/50], Step [200/5013], Loss: 0.8179\n",
      "Epoch [6/50], Step [300/5013], Loss: 0.7953\n",
      "Epoch [6/50], Step [400/5013], Loss: 0.8006\n",
      "Epoch [6/50], Step [500/5013], Loss: 0.7907\n",
      "Epoch [6/50], Step [600/5013], Loss: 0.8085\n",
      "Epoch [6/50], Step [700/5013], Loss: 0.7811\n",
      "Epoch [6/50], Step [800/5013], Loss: 0.8187\n",
      "Epoch [6/50], Step [900/5013], Loss: 0.7854\n",
      "Epoch [6/50], Step [1000/5013], Loss: 0.7942\n",
      "Epoch [6/50], Step [1100/5013], Loss: 0.8222\n",
      "Epoch [6/50], Step [1200/5013], Loss: 0.8147\n",
      "Epoch [6/50], Step [1300/5013], Loss: 0.8182\n",
      "Epoch [6/50], Step [1400/5013], Loss: 0.8152\n",
      "Epoch [6/50], Step [1500/5013], Loss: 0.8361\n",
      "Epoch [6/50], Step [1600/5013], Loss: 0.7917\n",
      "Epoch [6/50], Step [1700/5013], Loss: 0.8211\n",
      "Epoch [6/50], Step [1800/5013], Loss: 0.8134\n",
      "Epoch [6/50], Step [1900/5013], Loss: 0.8110\n",
      "Epoch [6/50], Step [2000/5013], Loss: 0.8342\n",
      "Epoch [6/50], Step [2100/5013], Loss: 0.8014\n",
      "Epoch [6/50], Step [2200/5013], Loss: 0.8061\n",
      "Epoch [6/50], Step [2300/5013], Loss: 0.8036\n",
      "Epoch [6/50], Step [2400/5013], Loss: 0.8224\n",
      "Epoch [6/50], Step [2500/5013], Loss: 0.8044\n",
      "Epoch [6/50], Step [2600/5013], Loss: 0.8263\n",
      "Epoch [6/50], Step [2700/5013], Loss: 0.8013\n",
      "Epoch [6/50], Step [2800/5013], Loss: 0.7914\n",
      "Epoch [6/50], Step [2900/5013], Loss: 0.7933\n",
      "Epoch [6/50], Step [3000/5013], Loss: 0.8079\n",
      "Epoch [6/50], Step [3100/5013], Loss: 0.8060\n",
      "Epoch [6/50], Step [3200/5013], Loss: 0.7941\n",
      "Epoch [6/50], Step [3300/5013], Loss: 0.7943\n",
      "Epoch [6/50], Step [3400/5013], Loss: 0.8023\n",
      "Epoch [6/50], Step [3500/5013], Loss: 0.8101\n",
      "Epoch [6/50], Step [3600/5013], Loss: 0.8081\n",
      "Epoch [6/50], Step [3700/5013], Loss: 0.7986\n",
      "Epoch [6/50], Step [3800/5013], Loss: 0.8015\n",
      "Epoch [6/50], Step [3900/5013], Loss: 0.8174\n",
      "Epoch [6/50], Step [4000/5013], Loss: 0.7915\n",
      "Epoch [6/50], Step [4100/5013], Loss: 0.8023\n",
      "Epoch [6/50], Step [4200/5013], Loss: 0.8128\n",
      "Epoch [6/50], Step [4300/5013], Loss: 0.7945\n",
      "Epoch [6/50], Step [4400/5013], Loss: 0.8088\n",
      "Epoch [6/50], Step [4500/5013], Loss: 0.8046\n",
      "Epoch [6/50], Step [4600/5013], Loss: 0.7940\n",
      "Epoch [6/50], Step [4700/5013], Loss: 0.7988\n",
      "Epoch [6/50], Step [4800/5013], Loss: 0.7999\n",
      "Epoch [6/50], Step [4900/5013], Loss: 0.7654\n",
      "Epoch [6/50], Step [5000/5013], Loss: 0.7899\n",
      "Epoch [6/50] Average Loss: 0.8049\n",
      "Epoch [7/50], Step [100/5013], Loss: 0.7772\n",
      "Epoch [7/50], Step [200/5013], Loss: 0.7990\n",
      "Epoch [7/50], Step [300/5013], Loss: 0.7808\n",
      "Epoch [7/50], Step [400/5013], Loss: 0.7936\n",
      "Epoch [7/50], Step [500/5013], Loss: 0.7947\n",
      "Epoch [7/50], Step [600/5013], Loss: 0.7958\n",
      "Epoch [7/50], Step [700/5013], Loss: 0.8121\n",
      "Epoch [7/50], Step [800/5013], Loss: 0.8043\n",
      "Epoch [7/50], Step [900/5013], Loss: 0.8011\n",
      "Epoch [7/50], Step [1000/5013], Loss: 0.7961\n",
      "Epoch [7/50], Step [1100/5013], Loss: 0.7934\n",
      "Epoch [7/50], Step [1200/5013], Loss: 0.8205\n",
      "Epoch [7/50], Step [1300/5013], Loss: 0.7984\n",
      "Epoch [7/50], Step [1400/5013], Loss: 0.8025\n",
      "Epoch [7/50], Step [1500/5013], Loss: 0.7793\n",
      "Epoch [7/50], Step [1600/5013], Loss: 0.7973\n",
      "Epoch [7/50], Step [1700/5013], Loss: 0.8149\n",
      "Epoch [7/50], Step [1800/5013], Loss: 0.8417\n",
      "Epoch [7/50], Step [1900/5013], Loss: 0.7777\n",
      "Epoch [7/50], Step [2000/5013], Loss: 0.7932\n",
      "Epoch [7/50], Step [2100/5013], Loss: 0.8029\n",
      "Epoch [7/50], Step [2200/5013], Loss: 0.8075\n",
      "Epoch [7/50], Step [2300/5013], Loss: 0.7851\n",
      "Epoch [7/50], Step [2400/5013], Loss: 0.8179\n",
      "Epoch [7/50], Step [2500/5013], Loss: 0.7818\n",
      "Epoch [7/50], Step [2600/5013], Loss: 0.7934\n",
      "Epoch [7/50], Step [2700/5013], Loss: 0.7996\n",
      "Epoch [7/50], Step [2800/5013], Loss: 0.7855\n",
      "Epoch [7/50], Step [2900/5013], Loss: 0.8185\n",
      "Epoch [7/50], Step [3000/5013], Loss: 0.8091\n",
      "Epoch [7/50], Step [3100/5013], Loss: 0.8011\n",
      "Epoch [7/50], Step [3200/5013], Loss: 0.7984\n",
      "Epoch [7/50], Step [3300/5013], Loss: 0.7820\n",
      "Epoch [7/50], Step [3400/5013], Loss: 0.7901\n",
      "Epoch [7/50], Step [3500/5013], Loss: 0.7910\n",
      "Epoch [7/50], Step [3600/5013], Loss: 0.7854\n",
      "Epoch [7/50], Step [3700/5013], Loss: 0.8064\n",
      "Epoch [7/50], Step [3800/5013], Loss: 0.7817\n",
      "Epoch [7/50], Step [3900/5013], Loss: 0.7947\n",
      "Epoch [7/50], Step [4000/5013], Loss: 0.7998\n",
      "Epoch [7/50], Step [4100/5013], Loss: 0.7780\n",
      "Epoch [7/50], Step [4200/5013], Loss: 0.7820\n",
      "Epoch [7/50], Step [4300/5013], Loss: 0.8100\n",
      "Epoch [7/50], Step [4400/5013], Loss: 0.8034\n",
      "Epoch [7/50], Step [4500/5013], Loss: 0.7861\n",
      "Epoch [7/50], Step [4600/5013], Loss: 0.9562\n",
      "Epoch [7/50], Step [4700/5013], Loss: 0.7911\n",
      "Epoch [7/50], Step [4800/5013], Loss: 0.7920\n",
      "Epoch [7/50], Step [4900/5013], Loss: 0.7701\n",
      "Epoch [7/50], Step [5000/5013], Loss: 0.8641\n",
      "Epoch [7/50] Average Loss: 0.8008\n",
      "Epoch [8/50], Step [100/5013], Loss: 0.7834\n",
      "Epoch [8/50], Step [200/5013], Loss: 0.7991\n",
      "Epoch [8/50], Step [300/5013], Loss: 0.7808\n",
      "Epoch [8/50], Step [400/5013], Loss: 0.7739\n",
      "Epoch [8/50], Step [500/5013], Loss: 0.7642\n",
      "Epoch [8/50], Step [600/5013], Loss: 0.7672\n",
      "Epoch [8/50], Step [700/5013], Loss: 0.7663\n",
      "Epoch [8/50], Step [800/5013], Loss: 0.7899\n",
      "Epoch [8/50], Step [900/5013], Loss: 0.7590\n",
      "Epoch [8/50], Step [1000/5013], Loss: 0.7891\n",
      "Epoch [8/50], Step [1100/5013], Loss: 0.7956\n",
      "Epoch [8/50], Step [1200/5013], Loss: 0.7750\n",
      "Epoch [8/50], Step [1300/5013], Loss: 0.7694\n",
      "Epoch [8/50], Step [1400/5013], Loss: 0.7811\n",
      "Epoch [8/50], Step [1500/5013], Loss: 0.7788\n",
      "Epoch [8/50], Step [1600/5013], Loss: 0.7759\n",
      "Epoch [8/50], Step [1700/5013], Loss: 0.7659\n",
      "Epoch [8/50], Step [1800/5013], Loss: 0.7976\n",
      "Epoch [8/50], Step [1900/5013], Loss: 0.7616\n",
      "Epoch [8/50], Step [2000/5013], Loss: 0.7882\n",
      "Epoch [8/50], Step [2100/5013], Loss: 0.7773\n",
      "Epoch [8/50], Step [2200/5013], Loss: 0.7796\n",
      "Epoch [8/50], Step [2300/5013], Loss: 0.7759\n",
      "Epoch [8/50], Step [2400/5013], Loss: 0.7855\n",
      "Epoch [8/50], Step [2500/5013], Loss: 0.7871\n",
      "Epoch [8/50], Step [2600/5013], Loss: 0.7923\n",
      "Epoch [8/50], Step [2700/5013], Loss: 0.7733\n",
      "Epoch [8/50], Step [2800/5013], Loss: 0.7975\n",
      "Epoch [8/50], Step [2900/5013], Loss: 0.7990\n",
      "Epoch [8/50], Step [3000/5013], Loss: 0.7875\n",
      "Epoch [8/50], Step [3100/5013], Loss: 0.7815\n",
      "Epoch [8/50], Step [3200/5013], Loss: 0.7952\n",
      "Epoch [8/50], Step [3300/5013], Loss: 0.7881\n",
      "Epoch [8/50], Step [3400/5013], Loss: 0.7780\n",
      "Epoch [8/50], Step [3500/5013], Loss: 0.7693\n",
      "Epoch [8/50], Step [3600/5013], Loss: 0.7875\n",
      "Epoch [8/50], Step [3700/5013], Loss: 0.7844\n",
      "Epoch [8/50], Step [3800/5013], Loss: 0.7998\n",
      "Epoch [8/50], Step [3900/5013], Loss: 0.7725\n",
      "Epoch [8/50], Step [4000/5013], Loss: 0.7854\n",
      "Epoch [8/50], Step [4100/5013], Loss: 0.8018\n",
      "Epoch [8/50], Step [4200/5013], Loss: 0.7642\n",
      "Epoch [8/50], Step [4300/5013], Loss: 0.7778\n",
      "Epoch [8/50], Step [4400/5013], Loss: 0.7730\n",
      "Epoch [8/50], Step [4500/5013], Loss: 0.8334\n",
      "Epoch [8/50], Step [4600/5013], Loss: 0.7758\n",
      "Epoch [8/50], Step [4700/5013], Loss: 0.8046\n",
      "Epoch [8/50], Step [4800/5013], Loss: 0.7740\n",
      "Epoch [8/50], Step [4900/5013], Loss: 0.7828\n",
      "Epoch [8/50], Step [5000/5013], Loss: 0.7801\n",
      "Epoch [8/50] Average Loss: 0.7826\n",
      "Epoch [9/50], Step [100/5013], Loss: 0.7639\n",
      "Epoch [9/50], Step [200/5013], Loss: 0.7945\n",
      "Epoch [9/50], Step [300/5013], Loss: 0.7435\n",
      "Epoch [9/50], Step [400/5013], Loss: 0.7904\n",
      "Epoch [9/50], Step [500/5013], Loss: 0.7806\n",
      "Epoch [9/50], Step [600/5013], Loss: 0.7940\n",
      "Epoch [9/50], Step [700/5013], Loss: 0.7878\n",
      "Epoch [9/50], Step [800/5013], Loss: 0.7644\n",
      "Epoch [9/50], Step [900/5013], Loss: 0.7695\n",
      "Epoch [9/50], Step [1000/5013], Loss: 0.7873\n",
      "Epoch [9/50], Step [1100/5013], Loss: 0.8030\n",
      "Epoch [9/50], Step [1200/5013], Loss: 0.7769\n",
      "Epoch [9/50], Step [1300/5013], Loss: 0.7597\n",
      "Epoch [9/50], Step [1400/5013], Loss: 0.7582\n",
      "Epoch [9/50], Step [1500/5013], Loss: 0.7984\n",
      "Epoch [9/50], Step [1600/5013], Loss: 0.7622\n",
      "Epoch [9/50], Step [1700/5013], Loss: 0.8059\n",
      "Epoch [9/50], Step [1800/5013], Loss: 0.8031\n",
      "Epoch [9/50], Step [1900/5013], Loss: 0.7915\n",
      "Epoch [9/50], Step [2000/5013], Loss: 0.7668\n",
      "Epoch [9/50], Step [2100/5013], Loss: 0.7521\n",
      "Epoch [9/50], Step [2200/5013], Loss: 0.7848\n",
      "Epoch [9/50], Step [2300/5013], Loss: 0.8066\n",
      "Epoch [9/50], Step [2400/5013], Loss: 0.7531\n",
      "Epoch [9/50], Step [2500/5013], Loss: 0.7555\n",
      "Epoch [9/50], Step [2600/5013], Loss: 0.7962\n",
      "Epoch [9/50], Step [2700/5013], Loss: 0.7640\n",
      "Epoch [9/50], Step [2800/5013], Loss: 0.7573\n",
      "Epoch [9/50], Step [2900/5013], Loss: 0.7756\n",
      "Epoch [9/50], Step [3000/5013], Loss: 0.7794\n",
      "Epoch [9/50], Step [3100/5013], Loss: 0.7700\n",
      "Epoch [9/50], Step [3200/5013], Loss: 0.7686\n",
      "Epoch [9/50], Step [3300/5013], Loss: 0.7874\n",
      "Epoch [9/50], Step [3400/5013], Loss: 0.7624\n",
      "Epoch [9/50], Step [3500/5013], Loss: 0.7443\n",
      "Epoch [9/50], Step [3600/5013], Loss: 0.7843\n",
      "Epoch [9/50], Step [3700/5013], Loss: 0.7486\n",
      "Epoch [9/50], Step [3800/5013], Loss: 0.7749\n",
      "Epoch [9/50], Step [3900/5013], Loss: 0.7711\n",
      "Epoch [9/50], Step [4000/5013], Loss: 0.7454\n",
      "Epoch [9/50], Step [4100/5013], Loss: 0.7597\n",
      "Epoch [9/50], Step [4200/5013], Loss: 0.7764\n",
      "Epoch [9/50], Step [4300/5013], Loss: 0.7524\n",
      "Epoch [9/50], Step [4400/5013], Loss: 0.7730\n",
      "Epoch [9/50], Step [4500/5013], Loss: 0.7882\n",
      "Epoch [9/50], Step [4600/5013], Loss: 0.7944\n",
      "Epoch [9/50], Step [4700/5013], Loss: 0.7891\n",
      "Epoch [9/50], Step [4800/5013], Loss: 0.7552\n",
      "Epoch [9/50], Step [4900/5013], Loss: 0.7578\n",
      "Epoch [9/50], Step [5000/5013], Loss: 0.7882\n",
      "Epoch [9/50] Average Loss: 0.7743\n",
      "Epoch [10/50], Step [100/5013], Loss: 0.7735\n",
      "Epoch [10/50], Step [200/5013], Loss: 0.7629\n",
      "Epoch [10/50], Step [300/5013], Loss: 0.7510\n",
      "Epoch [10/50], Step [400/5013], Loss: 0.7560\n",
      "Epoch [10/50], Step [500/5013], Loss: 0.7700\n",
      "Epoch [10/50], Step [600/5013], Loss: 0.7401\n",
      "Epoch [10/50], Step [700/5013], Loss: 0.7472\n",
      "Epoch [10/50], Step [800/5013], Loss: 0.7587\n",
      "Epoch [10/50], Step [900/5013], Loss: 0.7852\n",
      "Epoch [10/50], Step [1000/5013], Loss: 0.7508\n",
      "Epoch [10/50], Step [1100/5013], Loss: 0.7692\n",
      "Epoch [10/50], Step [1200/5013], Loss: 0.7453\n",
      "Epoch [10/50], Step [1300/5013], Loss: 0.7382\n",
      "Epoch [10/50], Step [1400/5013], Loss: 0.7717\n",
      "Epoch [10/50], Step [1500/5013], Loss: 0.7532\n",
      "Epoch [10/50], Step [1600/5013], Loss: 0.7675\n",
      "Epoch [10/50], Step [1700/5013], Loss: 0.7603\n",
      "Epoch [10/50], Step [1800/5013], Loss: 0.7372\n",
      "Epoch [10/50], Step [1900/5013], Loss: 0.7607\n",
      "Epoch [10/50], Step [2000/5013], Loss: 0.7827\n",
      "Epoch [10/50], Step [2100/5013], Loss: 0.7554\n",
      "Epoch [10/50], Step [2200/5013], Loss: 0.7412\n",
      "Epoch [10/50], Step [2300/5013], Loss: 0.7558\n",
      "Epoch [10/50], Step [2400/5013], Loss: 0.7507\n",
      "Epoch [10/50], Step [2500/5013], Loss: 0.7537\n",
      "Epoch [10/50], Step [2600/5013], Loss: 0.7675\n",
      "Epoch [10/50], Step [2700/5013], Loss: 0.7824\n",
      "Epoch [10/50], Step [2800/5013], Loss: 0.7988\n",
      "Epoch [10/50], Step [2900/5013], Loss: 0.7499\n",
      "Epoch [10/50], Step [3000/5013], Loss: 0.7461\n",
      "Epoch [10/50], Step [3100/5013], Loss: 0.7611\n",
      "Epoch [10/50], Step [3200/5013], Loss: 0.7631\n",
      "Epoch [10/50], Step [3300/5013], Loss: 0.7832\n",
      "Epoch [10/50], Step [3400/5013], Loss: 0.7540\n",
      "Epoch [10/50], Step [3500/5013], Loss: 0.7568\n",
      "Epoch [10/50], Step [3600/5013], Loss: 0.7694\n",
      "Epoch [10/50], Step [3700/5013], Loss: 0.8160\n",
      "Epoch [10/50], Step [3800/5013], Loss: 0.7582\n",
      "Epoch [10/50], Step [3900/5013], Loss: 0.7502\n",
      "Epoch [10/50], Step [4000/5013], Loss: 0.7832\n",
      "Epoch [10/50], Step [4100/5013], Loss: 0.8586\n",
      "Epoch [10/50], Step [4200/5013], Loss: 0.7944\n",
      "Epoch [10/50], Step [4300/5013], Loss: 0.7547\n",
      "Epoch [10/50], Step [4400/5013], Loss: 0.7650\n",
      "Epoch [10/50], Step [4500/5013], Loss: 0.7521\n",
      "Epoch [10/50], Step [4600/5013], Loss: 0.7665\n",
      "Epoch [10/50], Step [4700/5013], Loss: 0.7665\n",
      "Epoch [10/50], Step [4800/5013], Loss: 0.7579\n",
      "Epoch [10/50], Step [4900/5013], Loss: 0.7678\n",
      "Epoch [10/50], Step [5000/5013], Loss: 0.7665\n",
      "Epoch [10/50] Average Loss: 0.7646\n",
      "Epoch [11/50], Step [100/5013], Loss: 0.7532\n",
      "Epoch [11/50], Step [200/5013], Loss: 0.7473\n",
      "Epoch [11/50], Step [300/5013], Loss: 0.7412\n",
      "Epoch [11/50], Step [400/5013], Loss: 0.7328\n",
      "Epoch [11/50], Step [500/5013], Loss: 0.7484\n",
      "Epoch [11/50], Step [600/5013], Loss: 0.7568\n",
      "Epoch [11/50], Step [700/5013], Loss: 0.7514\n",
      "Epoch [11/50], Step [800/5013], Loss: 0.7596\n",
      "Epoch [11/50], Step [900/5013], Loss: 0.7638\n",
      "Epoch [11/50], Step [1000/5013], Loss: 0.7357\n",
      "Epoch [11/50], Step [1100/5013], Loss: 0.7199\n",
      "Epoch [11/50], Step [1200/5013], Loss: 0.7398\n",
      "Epoch [11/50], Step [1300/5013], Loss: 0.7580\n",
      "Epoch [11/50], Step [1400/5013], Loss: 0.7441\n",
      "Epoch [11/50], Step [1500/5013], Loss: 0.7493\n",
      "Epoch [11/50], Step [1600/5013], Loss: 0.7637\n",
      "Epoch [11/50], Step [1700/5013], Loss: 0.7668\n",
      "Epoch [11/50], Step [1800/5013], Loss: 0.7537\n",
      "Epoch [11/50], Step [1900/5013], Loss: 0.7619\n",
      "Epoch [11/50], Step [2000/5013], Loss: 0.7559\n",
      "Epoch [11/50], Step [2100/5013], Loss: 0.7805\n",
      "Epoch [11/50], Step [2200/5013], Loss: 0.7678\n",
      "Epoch [11/50], Step [2300/5013], Loss: 0.7714\n",
      "Epoch [11/50], Step [2400/5013], Loss: 0.7543\n",
      "Epoch [11/50], Step [2500/5013], Loss: 0.7634\n",
      "Epoch [11/50], Step [2600/5013], Loss: 0.7581\n",
      "Epoch [11/50], Step [2700/5013], Loss: 0.7704\n",
      "Epoch [11/50], Step [2800/5013], Loss: 0.7690\n",
      "Epoch [11/50], Step [2900/5013], Loss: 0.7745\n",
      "Epoch [11/50], Step [3000/5013], Loss: 0.7631\n",
      "Epoch [11/50], Step [3100/5013], Loss: 0.7461\n",
      "Epoch [11/50], Step [3200/5013], Loss: 0.7853\n",
      "Epoch [11/50], Step [3300/5013], Loss: 0.7549\n",
      "Epoch [11/50], Step [3400/5013], Loss: 0.7706\n",
      "Epoch [11/50], Step [3500/5013], Loss: 0.7624\n",
      "Epoch [11/50], Step [3600/5013], Loss: 0.7603\n",
      "Epoch [11/50], Step [3700/5013], Loss: 0.7661\n",
      "Epoch [11/50], Step [3800/5013], Loss: 0.7485\n",
      "Epoch [11/50], Step [3900/5013], Loss: 0.7722\n",
      "Epoch [11/50], Step [4000/5013], Loss: 0.7504\n",
      "Epoch [11/50], Step [4100/5013], Loss: 0.7381\n",
      "Epoch [11/50], Step [4200/5013], Loss: 0.7650\n",
      "Epoch [11/50], Step [4300/5013], Loss: 0.7739\n",
      "Epoch [11/50], Step [4400/5013], Loss: 0.7695\n",
      "Epoch [11/50], Step [4500/5013], Loss: 0.7308\n",
      "Epoch [11/50], Step [4600/5013], Loss: 0.7573\n",
      "Epoch [11/50], Step [4700/5013], Loss: 0.7594\n",
      "Epoch [11/50], Step [4800/5013], Loss: 0.7796\n",
      "Epoch [11/50], Step [4900/5013], Loss: 0.7547\n",
      "Epoch [11/50], Step [5000/5013], Loss: 0.7540\n",
      "Epoch [11/50] Average Loss: 0.7574\n",
      "Epoch [12/50], Step [100/5013], Loss: 0.7445\n",
      "Epoch [12/50], Step [200/5013], Loss: 0.7410\n",
      "Epoch [12/50], Step [300/5013], Loss: 0.7433\n",
      "Epoch [12/50], Step [400/5013], Loss: 0.7269\n",
      "Epoch [12/50], Step [500/5013], Loss: 0.7441\n",
      "Epoch [12/50], Step [600/5013], Loss: 0.7687\n",
      "Epoch [12/50], Step [700/5013], Loss: 0.7428\n",
      "Epoch [12/50], Step [800/5013], Loss: 0.7226\n",
      "Epoch [12/50], Step [900/5013], Loss: 0.7511\n",
      "Epoch [12/50], Step [1000/5013], Loss: 0.7308\n",
      "Epoch [12/50], Step [1100/5013], Loss: 0.7630\n",
      "Epoch [12/50], Step [1200/5013], Loss: 0.7591\n",
      "Epoch [12/50], Step [1300/5013], Loss: 0.7659\n",
      "Epoch [12/50], Step [1400/5013], Loss: 0.7165\n",
      "Epoch [12/50], Step [1500/5013], Loss: 0.7462\n",
      "Epoch [12/50], Step [1600/5013], Loss: 0.7620\n",
      "Epoch [12/50], Step [1700/5013], Loss: 0.7444\n",
      "Epoch [12/50], Step [1800/5013], Loss: 0.7540\n",
      "Epoch [12/50], Step [1900/5013], Loss: 0.7697\n",
      "Epoch [12/50], Step [2000/5013], Loss: 0.7687\n",
      "Epoch [12/50], Step [2100/5013], Loss: 0.7656\n",
      "Epoch [12/50], Step [2200/5013], Loss: 0.7693\n",
      "Epoch [12/50], Step [2300/5013], Loss: 0.7503\n",
      "Epoch [12/50], Step [2400/5013], Loss: 0.7667\n",
      "Epoch [12/50], Step [2500/5013], Loss: 0.7533\n",
      "Epoch [12/50], Step [2600/5013], Loss: 0.7301\n",
      "Epoch [12/50], Step [2700/5013], Loss: 0.7328\n",
      "Epoch [12/50], Step [2800/5013], Loss: 0.7658\n",
      "Epoch [12/50], Step [2900/5013], Loss: 0.7509\n",
      "Epoch [12/50], Step [3000/5013], Loss: 0.7578\n",
      "Epoch [12/50], Step [3100/5013], Loss: 0.7475\n",
      "Epoch [12/50], Step [3200/5013], Loss: 0.7443\n",
      "Epoch [12/50], Step [3300/5013], Loss: 0.7405\n",
      "Epoch [12/50], Step [3400/5013], Loss: 0.7623\n",
      "Epoch [12/50], Step [3500/5013], Loss: 0.7643\n",
      "Epoch [12/50], Step [3600/5013], Loss: 0.7391\n",
      "Epoch [12/50], Step [3700/5013], Loss: 0.7490\n",
      "Epoch [12/50], Step [3800/5013], Loss: 0.7772\n",
      "Epoch [12/50], Step [3900/5013], Loss: 0.7649\n",
      "Epoch [12/50], Step [4000/5013], Loss: 0.7681\n",
      "Epoch [12/50], Step [4100/5013], Loss: 0.7558\n",
      "Epoch [12/50], Step [4200/5013], Loss: 0.7348\n",
      "Epoch [12/50], Step [4300/5013], Loss: 0.7414\n",
      "Epoch [12/50], Step [4400/5013], Loss: 0.7401\n",
      "Epoch [12/50], Step [4500/5013], Loss: 0.7635\n",
      "Epoch [12/50], Step [4600/5013], Loss: 0.7568\n",
      "Epoch [12/50], Step [4700/5013], Loss: 0.7492\n",
      "Epoch [12/50], Step [4800/5013], Loss: 0.7771\n",
      "Epoch [12/50], Step [4900/5013], Loss: 0.7485\n",
      "Epoch [12/50], Step [5000/5013], Loss: 0.7387\n",
      "Epoch [12/50] Average Loss: 0.7515\n",
      "Epoch [13/50], Step [100/5013], Loss: 0.7494\n",
      "Epoch [13/50], Step [200/5013], Loss: 0.7388\n",
      "Epoch [13/50], Step [300/5013], Loss: 0.7453\n",
      "Epoch [13/50], Step [400/5013], Loss: 0.7256\n",
      "Epoch [13/50], Step [500/5013], Loss: 0.7369\n",
      "Epoch [13/50], Step [600/5013], Loss: 0.7229\n",
      "Epoch [13/50], Step [700/5013], Loss: 0.7475\n",
      "Epoch [13/50], Step [800/5013], Loss: 0.7321\n",
      "Epoch [13/50], Step [900/5013], Loss: 0.7394\n",
      "Epoch [13/50], Step [1000/5013], Loss: 0.7546\n",
      "Epoch [13/50], Step [1100/5013], Loss: 0.7628\n",
      "Epoch [13/50], Step [1200/5013], Loss: 0.7321\n",
      "Epoch [13/50], Step [1300/5013], Loss: 0.7414\n",
      "Epoch [13/50], Step [1400/5013], Loss: 0.7249\n",
      "Epoch [13/50], Step [1500/5013], Loss: 0.7436\n",
      "Epoch [13/50], Step [1600/5013], Loss: 0.7302\n",
      "Epoch [13/50], Step [1700/5013], Loss: 0.7417\n",
      "Epoch [13/50], Step [1800/5013], Loss: 0.7466\n",
      "Epoch [13/50], Step [1900/5013], Loss: 0.7209\n",
      "Epoch [13/50], Step [2000/5013], Loss: 0.7522\n",
      "Epoch [13/50], Step [2100/5013], Loss: 0.7601\n",
      "Epoch [13/50], Step [2200/5013], Loss: 0.7659\n",
      "Epoch [13/50], Step [2300/5013], Loss: 0.7301\n",
      "Epoch [13/50], Step [2400/5013], Loss: 0.7455\n",
      "Epoch [13/50], Step [2500/5013], Loss: 0.8104\n",
      "Epoch [13/50], Step [2600/5013], Loss: 0.7697\n",
      "Epoch [13/50], Step [2700/5013], Loss: 0.7662\n",
      "Epoch [13/50], Step [2800/5013], Loss: 0.7479\n",
      "Epoch [13/50], Step [2900/5013], Loss: 0.7475\n",
      "Epoch [13/50], Step [3000/5013], Loss: 0.7639\n",
      "Epoch [13/50], Step [3100/5013], Loss: 0.7343\n",
      "Epoch [13/50], Step [3200/5013], Loss: 0.7441\n",
      "Epoch [13/50], Step [3300/5013], Loss: 0.7324\n",
      "Epoch [13/50], Step [3400/5013], Loss: 0.7377\n",
      "Epoch [13/50], Step [3500/5013], Loss: 0.7515\n",
      "Epoch [13/50], Step [3600/5013], Loss: 0.7579\n",
      "Epoch [13/50], Step [3700/5013], Loss: 0.7828\n",
      "Epoch [13/50], Step [3800/5013], Loss: 0.7563\n",
      "Epoch [13/50], Step [3900/5013], Loss: 0.7457\n",
      "Epoch [13/50], Step [4000/5013], Loss: 0.7522\n",
      "Epoch [13/50], Step [4100/5013], Loss: 0.7345\n",
      "Epoch [13/50], Step [4200/5013], Loss: 0.8049\n",
      "Epoch [13/50], Step [4300/5013], Loss: 0.8079\n",
      "Epoch [13/50], Step [4400/5013], Loss: 0.7479\n",
      "Epoch [13/50], Step [4500/5013], Loss: 0.7677\n",
      "Epoch [13/50], Step [4600/5013], Loss: 0.7536\n",
      "Epoch [13/50], Step [4700/5013], Loss: 0.7468\n",
      "Epoch [13/50], Step [4800/5013], Loss: 0.7683\n",
      "Epoch [13/50], Step [4900/5013], Loss: 0.7426\n",
      "Epoch [13/50], Step [5000/5013], Loss: 0.7436\n",
      "Epoch [13/50] Average Loss: 0.7501\n",
      "Epoch [14/50], Step [100/5013], Loss: 0.7469\n",
      "Epoch [14/50], Step [200/5013], Loss: 0.7339\n",
      "Epoch [14/50], Step [300/5013], Loss: 0.7285\n",
      "Epoch [14/50], Step [400/5013], Loss: 0.7376\n",
      "Epoch [14/50], Step [500/5013], Loss: 0.7501\n",
      "Epoch [14/50], Step [600/5013], Loss: 0.7509\n",
      "Epoch [14/50], Step [700/5013], Loss: 0.7383\n",
      "Epoch [14/50], Step [800/5013], Loss: 0.7403\n",
      "Epoch [14/50], Step [900/5013], Loss: 0.7254\n",
      "Epoch [14/50], Step [1000/5013], Loss: 0.7373\n",
      "Epoch [14/50], Step [1100/5013], Loss: 0.7519\n",
      "Epoch [14/50], Step [1200/5013], Loss: 0.7188\n",
      "Epoch [14/50], Step [1300/5013], Loss: 0.7223\n",
      "Epoch [14/50], Step [1400/5013], Loss: 0.7257\n",
      "Epoch [14/50], Step [1500/5013], Loss: 0.7444\n",
      "Epoch [14/50], Step [1600/5013], Loss: 0.7433\n",
      "Epoch [14/50], Step [1700/5013], Loss: 0.7289\n",
      "Epoch [14/50], Step [1800/5013], Loss: 0.7319\n",
      "Epoch [14/50], Step [1900/5013], Loss: 0.7189\n",
      "Epoch [14/50], Step [2000/5013], Loss: 0.7204\n",
      "Epoch [14/50], Step [2100/5013], Loss: 0.7318\n",
      "Epoch [14/50], Step [2200/5013], Loss: 0.7120\n",
      "Epoch [14/50], Step [2300/5013], Loss: 0.7608\n",
      "Epoch [14/50], Step [2400/5013], Loss: 0.7606\n",
      "Epoch [14/50], Step [2500/5013], Loss: 0.7301\n",
      "Epoch [14/50], Step [2600/5013], Loss: 0.7464\n",
      "Epoch [14/50], Step [2700/5013], Loss: 0.7474\n",
      "Epoch [14/50], Step [2800/5013], Loss: 0.7486\n",
      "Epoch [14/50], Step [2900/5013], Loss: 0.7146\n",
      "Epoch [14/50], Step [3000/5013], Loss: 0.7270\n",
      "Epoch [14/50], Step [3100/5013], Loss: 0.7213\n",
      "Epoch [14/50], Step [3200/5013], Loss: 0.7228\n",
      "Epoch [14/50], Step [3300/5013], Loss: 0.7375\n",
      "Epoch [14/50], Step [3400/5013], Loss: 0.7530\n",
      "Epoch [14/50], Step [3500/5013], Loss: 0.7470\n",
      "Epoch [14/50], Step [3600/5013], Loss: 0.7487\n",
      "Epoch [14/50], Step [3700/5013], Loss: 0.7406\n",
      "Epoch [14/50], Step [3800/5013], Loss: 0.7350\n",
      "Epoch [14/50], Step [3900/5013], Loss: 0.7347\n",
      "Epoch [14/50], Step [4000/5013], Loss: 0.7649\n",
      "Epoch [14/50], Step [4100/5013], Loss: 0.7391\n",
      "Epoch [14/50], Step [4200/5013], Loss: 0.7441\n",
      "Epoch [14/50], Step [4300/5013], Loss: 0.7243\n",
      "Epoch [14/50], Step [4400/5013], Loss: 0.7571\n",
      "Epoch [14/50], Step [4500/5013], Loss: 0.7273\n",
      "Epoch [14/50], Step [4600/5013], Loss: 0.7487\n",
      "Epoch [14/50], Step [4700/5013], Loss: 0.7529\n",
      "Epoch [14/50], Step [4800/5013], Loss: 0.7455\n",
      "Epoch [14/50], Step [4900/5013], Loss: 0.7534\n",
      "Epoch [14/50], Step [5000/5013], Loss: 0.7536\n",
      "Epoch [14/50] Average Loss: 0.7386\n",
      "Epoch [15/50], Step [100/5013], Loss: 0.7382\n",
      "Epoch [15/50], Step [200/5013], Loss: 0.7474\n",
      "Epoch [15/50], Step [300/5013], Loss: 0.7489\n",
      "Epoch [15/50], Step [400/5013], Loss: 0.7366\n",
      "Epoch [15/50], Step [500/5013], Loss: 0.7245\n",
      "Epoch [15/50], Step [600/5013], Loss: 0.7117\n",
      "Epoch [15/50], Step [700/5013], Loss: 0.7294\n",
      "Epoch [15/50], Step [800/5013], Loss: 0.7360\n",
      "Epoch [15/50], Step [900/5013], Loss: 0.7233\n",
      "Epoch [15/50], Step [1000/5013], Loss: 0.7248\n",
      "Epoch [15/50], Step [1100/5013], Loss: 0.7275\n",
      "Epoch [15/50], Step [1200/5013], Loss: 0.7222\n",
      "Epoch [15/50], Step [1300/5013], Loss: 0.7270\n",
      "Epoch [15/50], Step [1400/5013], Loss: 0.7362\n",
      "Epoch [15/50], Step [1500/5013], Loss: 0.7174\n",
      "Epoch [15/50], Step [1600/5013], Loss: 0.7372\n",
      "Epoch [15/50], Step [1700/5013], Loss: 0.7217\n",
      "Epoch [15/50], Step [1800/5013], Loss: 0.7359\n",
      "Epoch [15/50], Step [1900/5013], Loss: 0.7409\n",
      "Epoch [15/50], Step [2000/5013], Loss: 0.7491\n",
      "Epoch [15/50], Step [2100/5013], Loss: 0.7179\n",
      "Epoch [15/50], Step [2200/5013], Loss: 0.7647\n",
      "Epoch [15/50], Step [2300/5013], Loss: 0.7454\n",
      "Epoch [15/50], Step [2400/5013], Loss: 0.7472\n",
      "Epoch [15/50], Step [2500/5013], Loss: 0.7465\n",
      "Epoch [15/50], Step [2600/5013], Loss: 0.7420\n",
      "Epoch [15/50], Step [2700/5013], Loss: 0.7509\n",
      "Epoch [15/50], Step [2800/5013], Loss: 0.7390\n",
      "Epoch [15/50], Step [2900/5013], Loss: 0.7253\n",
      "Epoch [15/50], Step [3000/5013], Loss: 0.7469\n",
      "Epoch [15/50], Step [3100/5013], Loss: 0.7213\n",
      "Epoch [15/50], Step [3200/5013], Loss: 0.7425\n",
      "Epoch [15/50], Step [3300/5013], Loss: 0.7324\n",
      "Epoch [15/50], Step [3400/5013], Loss: 0.7290\n",
      "Epoch [15/50], Step [3500/5013], Loss: 0.7310\n",
      "Epoch [15/50], Step [3600/5013], Loss: 0.7470\n",
      "Epoch [15/50], Step [3700/5013], Loss: 0.7447\n",
      "Epoch [15/50], Step [3800/5013], Loss: 0.7595\n",
      "Epoch [15/50], Step [3900/5013], Loss: 0.7329\n",
      "Epoch [15/50], Step [4000/5013], Loss: 0.7392\n",
      "Epoch [15/50], Step [4100/5013], Loss: 0.7331\n",
      "Epoch [15/50], Step [4200/5013], Loss: 0.7539\n",
      "Epoch [15/50], Step [4300/5013], Loss: 0.7253\n",
      "Epoch [15/50], Step [4400/5013], Loss: 0.7330\n",
      "Epoch [15/50], Step [4500/5013], Loss: 0.7264\n",
      "Epoch [15/50], Step [4600/5013], Loss: 0.7496\n",
      "Epoch [15/50], Step [4700/5013], Loss: 0.7370\n",
      "Epoch [15/50], Step [4800/5013], Loss: 0.7409\n",
      "Epoch [15/50], Step [4900/5013], Loss: 0.7262\n",
      "Epoch [15/50], Step [5000/5013], Loss: 0.7367\n",
      "Epoch [15/50] Average Loss: 0.7362\n",
      "Epoch [16/50], Step [100/5013], Loss: 0.7300\n",
      "Epoch [16/50], Step [200/5013], Loss: 0.7205\n",
      "Epoch [16/50], Step [300/5013], Loss: 0.7379\n",
      "Epoch [16/50], Step [400/5013], Loss: 0.7076\n",
      "Epoch [16/50], Step [500/5013], Loss: 0.7468\n",
      "Epoch [16/50], Step [600/5013], Loss: 0.7201\n",
      "Epoch [16/50], Step [700/5013], Loss: 0.7352\n",
      "Epoch [16/50], Step [800/5013], Loss: 0.7127\n",
      "Epoch [16/50], Step [900/5013], Loss: 0.7299\n",
      "Epoch [16/50], Step [1000/5013], Loss: 0.7414\n",
      "Epoch [16/50], Step [1100/5013], Loss: 0.7480\n",
      "Epoch [16/50], Step [1200/5013], Loss: 0.7327\n",
      "Epoch [16/50], Step [1300/5013], Loss: 0.7117\n",
      "Epoch [16/50], Step [1400/5013], Loss: 0.7247\n",
      "Epoch [16/50], Step [1500/5013], Loss: 0.7190\n",
      "Epoch [16/50], Step [1600/5013], Loss: 0.7125\n",
      "Epoch [16/50], Step [1700/5013], Loss: 0.7247\n",
      "Epoch [16/50], Step [1800/5013], Loss: 0.7356\n",
      "Epoch [16/50], Step [1900/5013], Loss: 0.7267\n",
      "Epoch [16/50], Step [2000/5013], Loss: 0.7353\n",
      "Epoch [16/50], Step [2100/5013], Loss: 0.7126\n",
      "Epoch [16/50], Step [2200/5013], Loss: 0.7214\n",
      "Epoch [16/50], Step [2300/5013], Loss: 0.7298\n",
      "Epoch [16/50], Step [2400/5013], Loss: 0.7345\n",
      "Epoch [16/50], Step [2500/5013], Loss: 0.7370\n",
      "Epoch [16/50], Step [2600/5013], Loss: 0.7405\n",
      "Epoch [16/50], Step [2700/5013], Loss: 0.7588\n",
      "Epoch [16/50], Step [2800/5013], Loss: 0.7395\n",
      "Epoch [16/50], Step [2900/5013], Loss: 0.7358\n",
      "Epoch [16/50], Step [3000/5013], Loss: 0.7304\n",
      "Epoch [16/50], Step [3100/5013], Loss: 0.7434\n",
      "Epoch [16/50], Step [3200/5013], Loss: 0.7134\n",
      "Epoch [16/50], Step [3300/5013], Loss: 0.7343\n",
      "Epoch [16/50], Step [3400/5013], Loss: 0.7074\n",
      "Epoch [16/50], Step [3500/5013], Loss: 0.7176\n",
      "Epoch [16/50], Step [3600/5013], Loss: 0.7190\n",
      "Epoch [16/50], Step [3700/5013], Loss: 0.7362\n",
      "Epoch [16/50], Step [3800/5013], Loss: 0.7437\n",
      "Epoch [16/50], Step [3900/5013], Loss: 0.7508\n",
      "Epoch [16/50], Step [4000/5013], Loss: 0.7651\n",
      "Epoch [16/50], Step [4100/5013], Loss: 0.7400\n",
      "Epoch [16/50], Step [4200/5013], Loss: 0.7262\n",
      "Epoch [16/50], Step [4300/5013], Loss: 0.7353\n",
      "Epoch [16/50], Step [4400/5013], Loss: 0.7119\n",
      "Epoch [16/50], Step [4500/5013], Loss: 0.7514\n",
      "Epoch [16/50], Step [4600/5013], Loss: 0.7192\n",
      "Epoch [16/50], Step [4700/5013], Loss: 0.7174\n",
      "Epoch [16/50], Step [4800/5013], Loss: 0.7407\n",
      "Epoch [16/50], Step [4900/5013], Loss: 0.7241\n",
      "Epoch [16/50], Step [5000/5013], Loss: 0.7401\n",
      "Epoch [16/50] Average Loss: 0.7306\n",
      "Epoch [17/50], Step [100/5013], Loss: 0.7154\n",
      "Epoch [17/50], Step [200/5013], Loss: 0.7389\n",
      "Epoch [17/50], Step [300/5013], Loss: 0.7206\n",
      "Epoch [17/50], Step [400/5013], Loss: 0.7291\n",
      "Epoch [17/50], Step [500/5013], Loss: 0.7282\n",
      "Epoch [17/50], Step [600/5013], Loss: 0.7363\n",
      "Epoch [17/50], Step [700/5013], Loss: 0.7328\n",
      "Epoch [17/50], Step [800/5013], Loss: 0.7594\n",
      "Epoch [17/50], Step [900/5013], Loss: 0.7282\n",
      "Epoch [17/50], Step [1000/5013], Loss: 0.7322\n",
      "Epoch [17/50], Step [1100/5013], Loss: 0.7469\n",
      "Epoch [17/50], Step [1200/5013], Loss: 0.7291\n",
      "Epoch [17/50], Step [1300/5013], Loss: 0.7184\n",
      "Epoch [17/50], Step [1400/5013], Loss: 0.7238\n",
      "Epoch [17/50], Step [1500/5013], Loss: 0.7410\n",
      "Epoch [17/50], Step [1600/5013], Loss: 0.7275\n",
      "Epoch [17/50], Step [1700/5013], Loss: 0.7546\n",
      "Epoch [17/50], Step [1800/5013], Loss: 0.7332\n",
      "Epoch [17/50], Step [1900/5013], Loss: 0.7213\n",
      "Epoch [17/50], Step [2000/5013], Loss: 0.7181\n",
      "Epoch [17/50], Step [2100/5013], Loss: 0.7264\n",
      "Epoch [17/50], Step [2200/5013], Loss: 0.7157\n",
      "Epoch [17/50], Step [2300/5013], Loss: 0.7139\n",
      "Epoch [17/50], Step [2400/5013], Loss: 0.7244\n",
      "Epoch [17/50], Step [2500/5013], Loss: 0.7378\n",
      "Epoch [17/50], Step [2600/5013], Loss: 0.7365\n",
      "Epoch [17/50], Step [2700/5013], Loss: 0.7312\n",
      "Epoch [17/50], Step [2800/5013], Loss: 0.7191\n",
      "Epoch [17/50], Step [2900/5013], Loss: 0.7363\n",
      "Epoch [17/50], Step [3000/5013], Loss: 0.7279\n",
      "Epoch [17/50], Step [3100/5013], Loss: 0.7444\n",
      "Epoch [17/50], Step [3200/5013], Loss: 0.7437\n",
      "Epoch [17/50], Step [3300/5013], Loss: 0.7207\n",
      "Epoch [17/50], Step [3400/5013], Loss: 0.7223\n",
      "Epoch [17/50], Step [3500/5013], Loss: 0.7359\n",
      "Epoch [17/50], Step [3600/5013], Loss: 0.7365\n",
      "Epoch [17/50], Step [3700/5013], Loss: 0.7335\n",
      "Epoch [17/50], Step [3800/5013], Loss: 0.7414\n",
      "Epoch [17/50], Step [3900/5013], Loss: 0.7349\n",
      "Epoch [17/50], Step [4000/5013], Loss: 0.7194\n",
      "Epoch [17/50], Step [4100/5013], Loss: 0.7359\n",
      "Epoch [17/50], Step [4200/5013], Loss: 0.7121\n",
      "Epoch [17/50], Step [4300/5013], Loss: 0.7309\n",
      "Epoch [17/50], Step [4400/5013], Loss: 0.7354\n",
      "Epoch [17/50], Step [4500/5013], Loss: 0.7139\n",
      "Epoch [17/50], Step [4600/5013], Loss: 0.7278\n",
      "Epoch [17/50], Step [4700/5013], Loss: 0.7410\n",
      "Epoch [17/50], Step [4800/5013], Loss: 0.7340\n",
      "Epoch [17/50], Step [4900/5013], Loss: 0.7036\n",
      "Epoch [17/50], Step [5000/5013], Loss: 0.7259\n",
      "Epoch [17/50] Average Loss: 0.7299\n",
      "Epoch [18/50], Step [100/5013], Loss: 0.7148\n",
      "Epoch [18/50], Step [200/5013], Loss: 0.6981\n",
      "Epoch [18/50], Step [300/5013], Loss: 0.7209\n",
      "Epoch [18/50], Step [400/5013], Loss: 0.7304\n",
      "Epoch [18/50], Step [500/5013], Loss: 0.7136\n",
      "Epoch [18/50], Step [600/5013], Loss: 0.7174\n",
      "Epoch [18/50], Step [700/5013], Loss: 0.7269\n",
      "Epoch [18/50], Step [800/5013], Loss: 0.7094\n",
      "Epoch [18/50], Step [900/5013], Loss: 0.7638\n",
      "Epoch [18/50], Step [1000/5013], Loss: 0.7191\n",
      "Epoch [18/50], Step [1100/5013], Loss: 0.7376\n",
      "Epoch [18/50], Step [1200/5013], Loss: 0.7296\n",
      "Epoch [18/50], Step [1300/5013], Loss: 0.7243\n",
      "Epoch [18/50], Step [1400/5013], Loss: 0.7264\n",
      "Epoch [18/50], Step [1500/5013], Loss: 0.7054\n",
      "Epoch [18/50], Step [1600/5013], Loss: 0.7258\n",
      "Epoch [18/50], Step [1700/5013], Loss: 0.7025\n",
      "Epoch [18/50], Step [1800/5013], Loss: 0.7247\n",
      "Epoch [18/50], Step [1900/5013], Loss: 0.7037\n",
      "Epoch [18/50], Step [2000/5013], Loss: 0.7332\n",
      "Epoch [18/50], Step [2100/5013], Loss: 0.7157\n",
      "Epoch [18/50], Step [2200/5013], Loss: 0.7022\n",
      "Epoch [18/50], Step [2300/5013], Loss: 0.7174\n",
      "Epoch [18/50], Step [2400/5013], Loss: 0.7272\n",
      "Epoch [18/50], Step [2500/5013], Loss: 0.7654\n",
      "Epoch [18/50], Step [2600/5013], Loss: 0.7250\n",
      "Epoch [18/50], Step [2700/5013], Loss: 0.7529\n",
      "Epoch [18/50], Step [2800/5013], Loss: 0.7245\n",
      "Epoch [18/50], Step [2900/5013], Loss: 0.7165\n",
      "Epoch [18/50], Step [3000/5013], Loss: 0.7520\n",
      "Epoch [18/50], Step [3100/5013], Loss: 0.7307\n",
      "Epoch [18/50], Step [3200/5013], Loss: 0.7367\n",
      "Epoch [18/50], Step [3300/5013], Loss: 0.7219\n",
      "Epoch [18/50], Step [3400/5013], Loss: 0.7282\n",
      "Epoch [18/50], Step [3500/5013], Loss: 0.6996\n",
      "Epoch [18/50], Step [3600/5013], Loss: 0.6897\n",
      "Epoch [18/50], Step [3700/5013], Loss: 0.7218\n",
      "Epoch [18/50], Step [3800/5013], Loss: 0.6986\n",
      "Epoch [18/50], Step [3900/5013], Loss: 0.7321\n",
      "Epoch [18/50], Step [4000/5013], Loss: 0.7215\n",
      "Epoch [18/50], Step [4100/5013], Loss: 0.7001\n",
      "Epoch [18/50], Step [4200/5013], Loss: 0.7136\n",
      "Epoch [18/50], Step [4300/5013], Loss: 0.7205\n",
      "Epoch [18/50], Step [4400/5013], Loss: 0.7261\n",
      "Epoch [18/50], Step [4500/5013], Loss: 0.7354\n",
      "Epoch [18/50], Step [4600/5013], Loss: 0.7512\n",
      "Epoch [18/50], Step [4700/5013], Loss: 0.7279\n",
      "Epoch [18/50], Step [4800/5013], Loss: 0.7352\n",
      "Epoch [18/50], Step [4900/5013], Loss: 0.7279\n",
      "Epoch [18/50], Step [5000/5013], Loss: 0.7197\n",
      "Epoch [18/50] Average Loss: 0.7234\n",
      "Epoch [19/50], Step [100/5013], Loss: 0.7199\n",
      "Epoch [19/50], Step [200/5013], Loss: 0.7141\n",
      "Epoch [19/50], Step [300/5013], Loss: 0.7458\n",
      "Epoch [19/50], Step [400/5013], Loss: 0.6957\n",
      "Epoch [19/50], Step [500/5013], Loss: 0.7162\n",
      "Epoch [19/50], Step [600/5013], Loss: 0.7154\n",
      "Epoch [19/50], Step [700/5013], Loss: 0.7192\n",
      "Epoch [19/50], Step [800/5013], Loss: 0.7002\n",
      "Epoch [19/50], Step [900/5013], Loss: 0.7342\n",
      "Epoch [19/50], Step [1000/5013], Loss: 0.7210\n",
      "Epoch [19/50], Step [1100/5013], Loss: 0.7333\n",
      "Epoch [19/50], Step [1200/5013], Loss: 0.7081\n",
      "Epoch [19/50], Step [1300/5013], Loss: 0.6990\n",
      "Epoch [19/50], Step [1400/5013], Loss: 0.7412\n",
      "Epoch [19/50], Step [1500/5013], Loss: 0.7329\n",
      "Epoch [19/50], Step [1600/5013], Loss: 0.7192\n",
      "Epoch [19/50], Step [1700/5013], Loss: 0.7021\n",
      "Epoch [19/50], Step [1800/5013], Loss: 0.7489\n",
      "Epoch [19/50], Step [1900/5013], Loss: 0.7138\n",
      "Epoch [19/50], Step [2000/5013], Loss: 0.6952\n",
      "Epoch [19/50], Step [2100/5013], Loss: 0.7211\n",
      "Epoch [19/50], Step [2200/5013], Loss: 0.7139\n",
      "Epoch [19/50], Step [2300/5013], Loss: 0.7221\n",
      "Epoch [19/50], Step [2400/5013], Loss: 0.7152\n",
      "Epoch [19/50], Step [2500/5013], Loss: 0.7183\n",
      "Epoch [19/50], Step [2600/5013], Loss: 0.7343\n",
      "Epoch [19/50], Step [2700/5013], Loss: 0.7224\n",
      "Epoch [19/50], Step [2800/5013], Loss: 0.7209\n",
      "Epoch [19/50], Step [2900/5013], Loss: 0.7204\n",
      "Epoch [19/50], Step [3000/5013], Loss: 0.7217\n",
      "Epoch [19/50], Step [3100/5013], Loss: 0.7160\n",
      "Epoch [19/50], Step [3200/5013], Loss: 0.7176\n",
      "Epoch [19/50], Step [3300/5013], Loss: 0.7126\n",
      "Epoch [19/50], Step [3400/5013], Loss: 0.7040\n",
      "Epoch [19/50], Step [3500/5013], Loss: 0.7322\n",
      "Epoch [19/50], Step [3600/5013], Loss: 0.7215\n",
      "Epoch [19/50], Step [3700/5013], Loss: 0.7112\n",
      "Epoch [19/50], Step [3800/5013], Loss: 0.7411\n",
      "Epoch [19/50], Step [3900/5013], Loss: 0.7292\n",
      "Epoch [19/50], Step [4000/5013], Loss: 0.7134\n",
      "Epoch [19/50], Step [4100/5013], Loss: 0.6968\n",
      "Epoch [19/50], Step [4200/5013], Loss: 0.7205\n",
      "Epoch [19/50], Step [4300/5013], Loss: 0.6915\n",
      "Epoch [19/50], Step [4400/5013], Loss: 0.7093\n",
      "Epoch [19/50], Step [4500/5013], Loss: 0.7373\n",
      "Epoch [19/50], Step [4600/5013], Loss: 0.7124\n",
      "Epoch [19/50], Step [4700/5013], Loss: 0.7025\n",
      "Epoch [19/50], Step [4800/5013], Loss: 0.6875\n",
      "Epoch [19/50], Step [4900/5013], Loss: 0.7329\n",
      "Epoch [19/50], Step [5000/5013], Loss: 0.7466\n",
      "Epoch [19/50] Average Loss: 0.7185\n",
      "Epoch [20/50], Step [100/5013], Loss: 0.6989\n",
      "Epoch [20/50], Step [200/5013], Loss: 0.7190\n",
      "Epoch [20/50], Step [300/5013], Loss: 0.7144\n",
      "Epoch [20/50], Step [400/5013], Loss: 0.7015\n",
      "Epoch [20/50], Step [500/5013], Loss: 0.7062\n",
      "Epoch [20/50], Step [600/5013], Loss: 0.7185\n",
      "Epoch [20/50], Step [700/5013], Loss: 0.7039\n",
      "Epoch [20/50], Step [800/5013], Loss: 0.7120\n",
      "Epoch [20/50], Step [900/5013], Loss: 0.7109\n",
      "Epoch [20/50], Step [1000/5013], Loss: 0.7174\n",
      "Epoch [20/50], Step [1100/5013], Loss: 0.6756\n",
      "Epoch [20/50], Step [1200/5013], Loss: 0.7198\n",
      "Epoch [20/50], Step [1300/5013], Loss: 0.7085\n",
      "Epoch [20/50], Step [1400/5013], Loss: 0.7082\n",
      "Epoch [20/50], Step [1500/5013], Loss: 0.7221\n",
      "Epoch [20/50], Step [1600/5013], Loss: 1.2560\n",
      "Epoch [20/50], Step [1700/5013], Loss: 0.7411\n",
      "Epoch [20/50], Step [1800/5013], Loss: 0.7006\n",
      "Epoch [20/50], Step [1900/5013], Loss: 0.7096\n",
      "Epoch [20/50], Step [2000/5013], Loss: 0.7156\n",
      "Epoch [20/50], Step [2100/5013], Loss: 0.7256\n",
      "Epoch [20/50], Step [2200/5013], Loss: 0.7191\n",
      "Epoch [20/50], Step [2300/5013], Loss: 0.7216\n",
      "Epoch [20/50], Step [2400/5013], Loss: 0.6993\n",
      "Epoch [20/50], Step [2500/5013], Loss: 0.7258\n",
      "Epoch [20/50], Step [2600/5013], Loss: 0.7052\n",
      "Epoch [20/50], Step [2700/5013], Loss: 0.7200\n",
      "Epoch [20/50], Step [2800/5013], Loss: 0.7211\n",
      "Epoch [20/50], Step [2900/5013], Loss: 0.7159\n",
      "Epoch [20/50], Step [3000/5013], Loss: 0.7012\n",
      "Epoch [20/50], Step [3100/5013], Loss: 0.7002\n",
      "Epoch [20/50], Step [3200/5013], Loss: 0.7337\n",
      "Epoch [20/50], Step [3300/5013], Loss: 0.7142\n",
      "Epoch [20/50], Step [3400/5013], Loss: 0.7070\n",
      "Epoch [20/50], Step [3500/5013], Loss: 0.7347\n",
      "Epoch [20/50], Step [3600/5013], Loss: 0.7092\n",
      "Epoch [20/50], Step [3700/5013], Loss: 0.7157\n",
      "Epoch [20/50], Step [3800/5013], Loss: 0.7181\n",
      "Epoch [20/50], Step [3900/5013], Loss: 0.7251\n",
      "Epoch [20/50], Step [4000/5013], Loss: 0.7045\n",
      "Epoch [20/50], Step [4100/5013], Loss: 0.7042\n",
      "Epoch [20/50], Step [4200/5013], Loss: 0.7083\n",
      "Epoch [20/50], Step [4300/5013], Loss: 0.7400\n",
      "Epoch [20/50], Step [4400/5013], Loss: 0.7198\n",
      "Epoch [20/50], Step [4500/5013], Loss: 0.7246\n",
      "Epoch [20/50], Step [4600/5013], Loss: 0.7036\n",
      "Epoch [20/50], Step [4700/5013], Loss: 0.7182\n",
      "Epoch [20/50], Step [4800/5013], Loss: 0.7395\n",
      "Epoch [20/50], Step [4900/5013], Loss: 0.7354\n",
      "Epoch [20/50], Step [5000/5013], Loss: 0.7294\n",
      "Epoch [20/50] Average Loss: 0.7259\n",
      "Epoch [21/50], Step [100/5013], Loss: 0.7275\n",
      "Epoch [21/50], Step [200/5013], Loss: 0.6984\n",
      "Epoch [21/50], Step [300/5013], Loss: 0.7152\n",
      "Epoch [21/50], Step [400/5013], Loss: 0.7098\n",
      "Epoch [21/50], Step [500/5013], Loss: 0.6923\n",
      "Epoch [21/50], Step [600/5013], Loss: 0.6906\n",
      "Epoch [21/50], Step [700/5013], Loss: 0.7133\n",
      "Epoch [21/50], Step [800/5013], Loss: 0.7276\n",
      "Epoch [21/50], Step [900/5013], Loss: 0.6954\n",
      "Epoch [21/50], Step [1000/5013], Loss: 0.7166\n",
      "Epoch [21/50], Step [1100/5013], Loss: 0.6927\n",
      "Epoch [21/50], Step [1200/5013], Loss: 0.7058\n",
      "Epoch [21/50], Step [1300/5013], Loss: 0.7128\n",
      "Epoch [21/50], Step [1400/5013], Loss: 0.7034\n",
      "Epoch [21/50], Step [1500/5013], Loss: 0.7039\n",
      "Epoch [21/50], Step [1600/5013], Loss: 0.6890\n",
      "Epoch [21/50], Step [1700/5013], Loss: 0.7443\n",
      "Epoch [21/50], Step [1800/5013], Loss: 0.7176\n",
      "Epoch [21/50], Step [1900/5013], Loss: 0.7338\n",
      "Epoch [21/50], Step [2000/5013], Loss: 0.7266\n",
      "Epoch [21/50], Step [2100/5013], Loss: 0.7008\n",
      "Epoch [21/50], Step [2200/5013], Loss: 0.6813\n",
      "Epoch [21/50], Step [2300/5013], Loss: 0.7297\n",
      "Epoch [21/50], Step [2400/5013], Loss: 0.7251\n",
      "Epoch [21/50], Step [2500/5013], Loss: 0.7201\n",
      "Epoch [21/50], Step [2600/5013], Loss: 0.7238\n",
      "Epoch [21/50], Step [2700/5013], Loss: 0.7066\n",
      "Epoch [21/50], Step [2800/5013], Loss: 0.7337\n",
      "Epoch [21/50], Step [2900/5013], Loss: 0.7078\n",
      "Epoch [21/50], Step [3000/5013], Loss: 0.7121\n",
      "Epoch [21/50], Step [3100/5013], Loss: 0.7184\n",
      "Epoch [21/50], Step [3200/5013], Loss: 0.7772\n",
      "Epoch [21/50], Step [3300/5013], Loss: 0.7194\n",
      "Epoch [21/50], Step [3400/5013], Loss: 0.7385\n",
      "Epoch [21/50], Step [3500/5013], Loss: 0.7205\n",
      "Epoch [21/50], Step [3600/5013], Loss: 0.6786\n",
      "Epoch [21/50], Step [3700/5013], Loss: 0.7175\n",
      "Epoch [21/50], Step [3800/5013], Loss: 0.7078\n",
      "Epoch [21/50], Step [3900/5013], Loss: 0.7042\n",
      "Epoch [21/50], Step [4000/5013], Loss: 0.7103\n",
      "Epoch [21/50], Step [4100/5013], Loss: 0.7088\n",
      "Epoch [21/50], Step [4200/5013], Loss: 0.6992\n",
      "Epoch [21/50], Step [4300/5013], Loss: 0.7149\n",
      "Epoch [21/50], Step [4400/5013], Loss: 0.7290\n",
      "Epoch [21/50], Step [4500/5013], Loss: 0.7298\n",
      "Epoch [21/50], Step [4600/5013], Loss: 0.7139\n",
      "Epoch [21/50], Step [4700/5013], Loss: 0.7156\n",
      "Epoch [21/50], Step [4800/5013], Loss: 0.7142\n",
      "Epoch [21/50], Step [4900/5013], Loss: 0.7414\n",
      "Epoch [21/50], Step [5000/5013], Loss: 0.7158\n",
      "Epoch [21/50] Average Loss: 0.7146\n",
      "Epoch [22/50], Step [100/5013], Loss: 0.7181\n",
      "Epoch [22/50], Step [200/5013], Loss: 0.6993\n",
      "Epoch [22/50], Step [300/5013], Loss: 0.7338\n",
      "Epoch [22/50], Step [400/5013], Loss: 0.6842\n",
      "Epoch [22/50], Step [500/5013], Loss: 0.7019\n",
      "Epoch [22/50], Step [600/5013], Loss: 0.7114\n",
      "Epoch [22/50], Step [700/5013], Loss: 0.7190\n",
      "Epoch [22/50], Step [800/5013], Loss: 0.6966\n",
      "Epoch [22/50], Step [900/5013], Loss: 0.7014\n",
      "Epoch [22/50], Step [1000/5013], Loss: 0.6952\n",
      "Epoch [22/50], Step [1100/5013], Loss: 0.6879\n",
      "Epoch [22/50], Step [1200/5013], Loss: 0.6854\n",
      "Epoch [22/50], Step [1300/5013], Loss: 0.6998\n",
      "Epoch [22/50], Step [1400/5013], Loss: 0.7062\n",
      "Epoch [22/50], Step [1500/5013], Loss: 0.7157\n",
      "Epoch [22/50], Step [1600/5013], Loss: 0.6916\n",
      "Epoch [22/50], Step [1700/5013], Loss: 0.6699\n",
      "Epoch [22/50], Step [1800/5013], Loss: 0.6861\n",
      "Epoch [22/50], Step [1900/5013], Loss: 0.6977\n",
      "Epoch [22/50], Step [2000/5013], Loss: 0.7200\n",
      "Epoch [22/50], Step [2100/5013], Loss: 0.7193\n",
      "Epoch [22/50], Step [2200/5013], Loss: 0.7162\n",
      "Epoch [22/50], Step [2300/5013], Loss: 0.7038\n",
      "Epoch [22/50], Step [2400/5013], Loss: 0.7212\n",
      "Epoch [22/50], Step [2500/5013], Loss: 0.7104\n",
      "Epoch [22/50], Step [2600/5013], Loss: 0.7302\n",
      "Epoch [22/50], Step [2700/5013], Loss: 0.7102\n",
      "Epoch [22/50], Step [2800/5013], Loss: 0.6981\n",
      "Epoch [22/50], Step [2900/5013], Loss: 0.7193\n",
      "Epoch [22/50], Step [3000/5013], Loss: 0.7193\n",
      "Epoch [22/50], Step [3100/5013], Loss: 0.7204\n",
      "Epoch [22/50], Step [3200/5013], Loss: 0.7252\n",
      "Epoch [22/50], Step [3300/5013], Loss: 0.7372\n",
      "Epoch [22/50], Step [3400/5013], Loss: 0.7283\n",
      "Epoch [22/50], Step [3500/5013], Loss: 0.7030\n",
      "Epoch [22/50], Step [3600/5013], Loss: 0.7134\n",
      "Epoch [22/50], Step [3700/5013], Loss: 0.7092\n",
      "Epoch [22/50], Step [3800/5013], Loss: 0.7247\n",
      "Epoch [22/50], Step [3900/5013], Loss: 0.6647\n",
      "Epoch [22/50], Step [4000/5013], Loss: 0.7178\n",
      "Epoch [22/50], Step [4100/5013], Loss: 0.6977\n",
      "Epoch [22/50], Step [4200/5013], Loss: 0.7109\n",
      "Epoch [22/50], Step [4300/5013], Loss: 0.7137\n",
      "Epoch [22/50], Step [4400/5013], Loss: 0.7040\n",
      "Epoch [22/50], Step [4500/5013], Loss: 0.7001\n",
      "Epoch [22/50], Step [4600/5013], Loss: 0.7045\n",
      "Epoch [22/50], Step [4700/5013], Loss: 0.7045\n",
      "Epoch [22/50], Step [4800/5013], Loss: 0.7310\n",
      "Epoch [22/50], Step [4900/5013], Loss: 0.7261\n",
      "Epoch [22/50], Step [5000/5013], Loss: 0.7107\n",
      "Epoch [22/50] Average Loss: 0.7082\n",
      "Epoch [23/50], Step [100/5013], Loss: 0.7035\n",
      "Epoch [23/50], Step [200/5013], Loss: 0.7068\n",
      "Epoch [23/50], Step [300/5013], Loss: 0.7116\n",
      "Epoch [23/50], Step [400/5013], Loss: 0.6937\n",
      "Epoch [23/50], Step [500/5013], Loss: 0.7063\n",
      "Epoch [23/50], Step [600/5013], Loss: 0.7132\n",
      "Epoch [23/50], Step [700/5013], Loss: 0.6938\n",
      "Epoch [23/50], Step [800/5013], Loss: 0.7060\n",
      "Epoch [23/50], Step [900/5013], Loss: 0.7175\n",
      "Epoch [23/50], Step [1000/5013], Loss: 0.7057\n",
      "Epoch [23/50], Step [1100/5013], Loss: 0.7066\n",
      "Epoch [23/50], Step [1200/5013], Loss: 0.7048\n",
      "Epoch [23/50], Step [1300/5013], Loss: 0.7016\n",
      "Epoch [23/50], Step [1400/5013], Loss: 0.7095\n",
      "Epoch [23/50], Step [1500/5013], Loss: 0.6885\n",
      "Epoch [23/50], Step [1600/5013], Loss: 0.6913\n",
      "Epoch [23/50], Step [1700/5013], Loss: 0.7137\n",
      "Epoch [23/50], Step [1800/5013], Loss: 0.7280\n",
      "Epoch [23/50], Step [1900/5013], Loss: 0.6897\n",
      "Epoch [23/50], Step [2000/5013], Loss: 0.6939\n",
      "Epoch [23/50], Step [2100/5013], Loss: 0.7046\n",
      "Epoch [23/50], Step [2200/5013], Loss: 0.7027\n",
      "Epoch [23/50], Step [2300/5013], Loss: 0.7467\n",
      "Epoch [23/50], Step [2400/5013], Loss: 0.6843\n",
      "Epoch [23/50], Step [2500/5013], Loss: 0.7198\n",
      "Epoch [23/50], Step [2600/5013], Loss: 0.6950\n",
      "Epoch [23/50], Step [2700/5013], Loss: 0.6996\n",
      "Epoch [23/50], Step [2800/5013], Loss: 0.7199\n",
      "Epoch [23/50], Step [2900/5013], Loss: 0.7103\n",
      "Epoch [23/50], Step [3000/5013], Loss: 0.7431\n",
      "Epoch [23/50], Step [3100/5013], Loss: 0.6891\n",
      "Epoch [23/50], Step [3200/5013], Loss: 0.6891\n",
      "Epoch [23/50], Step [3300/5013], Loss: 0.7294\n",
      "Epoch [23/50], Step [3400/5013], Loss: 0.7170\n",
      "Epoch [23/50], Step [3500/5013], Loss: 0.8479\n",
      "Epoch [23/50], Step [3600/5013], Loss: 0.7235\n",
      "Epoch [23/50], Step [3700/5013], Loss: 0.7044\n",
      "Epoch [23/50], Step [3800/5013], Loss: 0.7080\n",
      "Epoch [23/50], Step [3900/5013], Loss: 0.7099\n",
      "Epoch [23/50], Step [4000/5013], Loss: 0.7000\n",
      "Epoch [23/50], Step [4100/5013], Loss: 0.7000\n",
      "Epoch [23/50], Step [4200/5013], Loss: 0.7053\n",
      "Epoch [23/50], Step [4300/5013], Loss: 0.7013\n",
      "Epoch [23/50], Step [4400/5013], Loss: 0.7260\n",
      "Epoch [23/50], Step [4500/5013], Loss: 0.7002\n",
      "Epoch [23/50], Step [4600/5013], Loss: 0.7112\n",
      "Epoch [23/50], Step [4700/5013], Loss: 0.7185\n",
      "Epoch [23/50], Step [4800/5013], Loss: 0.6980\n",
      "Epoch [23/50], Step [4900/5013], Loss: 0.7182\n",
      "Epoch [23/50], Step [5000/5013], Loss: 0.7031\n",
      "Epoch [23/50] Average Loss: 0.7102\n",
      "Epoch [24/50], Step [100/5013], Loss: 0.7123\n",
      "Epoch [24/50], Step [200/5013], Loss: 0.6872\n",
      "Epoch [24/50], Step [300/5013], Loss: 0.6890\n",
      "Epoch [24/50], Step [400/5013], Loss: 0.6944\n",
      "Epoch [24/50], Step [500/5013], Loss: 0.6952\n",
      "Epoch [24/50], Step [600/5013], Loss: 0.7005\n",
      "Epoch [24/50], Step [700/5013], Loss: 0.6938\n",
      "Epoch [24/50], Step [800/5013], Loss: 0.7096\n",
      "Epoch [24/50], Step [900/5013], Loss: 0.7011\n",
      "Epoch [24/50], Step [1000/5013], Loss: 0.6749\n",
      "Epoch [24/50], Step [1100/5013], Loss: 0.6868\n",
      "Epoch [24/50], Step [1200/5013], Loss: 0.6961\n",
      "Epoch [24/50], Step [1300/5013], Loss: 0.7031\n",
      "Epoch [24/50], Step [1400/5013], Loss: 0.7043\n",
      "Epoch [24/50], Step [1500/5013], Loss: 0.6787\n",
      "Epoch [24/50], Step [1600/5013], Loss: 0.7271\n",
      "Epoch [24/50], Step [1700/5013], Loss: 0.7039\n",
      "Epoch [24/50], Step [1800/5013], Loss: 0.7053\n",
      "Epoch [24/50], Step [1900/5013], Loss: 0.6843\n",
      "Epoch [24/50], Step [2000/5013], Loss: 0.7130\n",
      "Epoch [24/50], Step [2100/5013], Loss: 0.7219\n",
      "Epoch [24/50], Step [2200/5013], Loss: 0.7044\n",
      "Epoch [24/50], Step [2300/5013], Loss: 0.7117\n",
      "Epoch [24/50], Step [2400/5013], Loss: 0.7135\n",
      "Epoch [24/50], Step [2500/5013], Loss: 0.7210\n",
      "Epoch [24/50], Step [2600/5013], Loss: 0.6960\n",
      "Epoch [24/50], Step [2700/5013], Loss: 0.7133\n",
      "Epoch [24/50], Step [2800/5013], Loss: 0.7239\n",
      "Epoch [24/50], Step [2900/5013], Loss: 0.6941\n",
      "Epoch [24/50], Step [3000/5013], Loss: 0.6886\n",
      "Epoch [24/50], Step [3100/5013], Loss: 0.7373\n",
      "Epoch [24/50], Step [3200/5013], Loss: 0.7185\n",
      "Epoch [24/50], Step [3300/5013], Loss: 0.7166\n",
      "Epoch [24/50], Step [3400/5013], Loss: 0.7273\n",
      "Epoch [24/50], Step [3500/5013], Loss: 0.6962\n",
      "Epoch [24/50], Step [3600/5013], Loss: 0.7147\n",
      "Epoch [24/50], Step [3700/5013], Loss: 0.7163\n",
      "Epoch [24/50], Step [3800/5013], Loss: 0.6855\n",
      "Epoch [24/50], Step [3900/5013], Loss: 0.7147\n",
      "Epoch [24/50], Step [4000/5013], Loss: 0.7139\n",
      "Epoch [24/50], Step [4100/5013], Loss: 0.7010\n",
      "Epoch [24/50], Step [4200/5013], Loss: 0.7115\n",
      "Epoch [24/50], Step [4300/5013], Loss: 0.6864\n",
      "Epoch [24/50], Step [4400/5013], Loss: 0.7189\n",
      "Epoch [24/50], Step [4500/5013], Loss: 0.7170\n",
      "Epoch [24/50], Step [4600/5013], Loss: 0.6860\n",
      "Epoch [24/50], Step [4700/5013], Loss: 0.7220\n",
      "Epoch [24/50], Step [4800/5013], Loss: 0.7083\n",
      "Epoch [24/50], Step [4900/5013], Loss: 0.7013\n",
      "Epoch [24/50], Step [5000/5013], Loss: 0.7212\n",
      "Epoch [24/50] Average Loss: 0.7053\n",
      "Epoch [25/50], Step [100/5013], Loss: 0.7060\n",
      "Epoch [25/50], Step [200/5013], Loss: 0.6823\n",
      "Epoch [25/50], Step [300/5013], Loss: 0.6844\n",
      "Epoch [25/50], Step [400/5013], Loss: 0.6931\n",
      "Epoch [25/50], Step [500/5013], Loss: 0.7049\n",
      "Epoch [25/50], Step [600/5013], Loss: 0.6887\n",
      "Epoch [25/50], Step [700/5013], Loss: 0.6767\n",
      "Epoch [25/50], Step [800/5013], Loss: 0.6936\n",
      "Epoch [25/50], Step [900/5013], Loss: 0.7035\n",
      "Epoch [25/50], Step [1000/5013], Loss: 0.8028\n",
      "Epoch [25/50], Step [1100/5013], Loss: 0.6997\n",
      "Epoch [25/50], Step [1200/5013], Loss: 0.7004\n",
      "Epoch [25/50], Step [1300/5013], Loss: 0.7146\n",
      "Epoch [25/50], Step [1400/5013], Loss: 0.7039\n",
      "Epoch [25/50], Step [1500/5013], Loss: 0.7094\n",
      "Epoch [25/50], Step [1600/5013], Loss: 0.6930\n",
      "Epoch [25/50], Step [1700/5013], Loss: 0.6734\n",
      "Epoch [25/50], Step [1800/5013], Loss: 0.7171\n",
      "Epoch [25/50], Step [1900/5013], Loss: 0.7094\n",
      "Epoch [25/50], Step [2000/5013], Loss: 0.7001\n",
      "Epoch [25/50], Step [2100/5013], Loss: 0.7028\n",
      "Epoch [25/50], Step [2200/5013], Loss: 0.6998\n",
      "Epoch [25/50], Step [2300/5013], Loss: 0.6978\n",
      "Epoch [25/50], Step [2400/5013], Loss: 0.7083\n",
      "Epoch [25/50], Step [2500/5013], Loss: 0.7163\n",
      "Epoch [25/50], Step [2600/5013], Loss: 0.7094\n",
      "Epoch [25/50], Step [2700/5013], Loss: 0.6839\n",
      "Epoch [25/50], Step [2800/5013], Loss: 0.7022\n",
      "Epoch [25/50], Step [2900/5013], Loss: 0.6934\n",
      "Epoch [25/50], Step [3000/5013], Loss: 0.6897\n",
      "Epoch [25/50], Step [3100/5013], Loss: 0.7086\n",
      "Epoch [25/50], Step [3200/5013], Loss: 0.7103\n",
      "Epoch [25/50], Step [3300/5013], Loss: 0.7364\n",
      "Epoch [25/50], Step [3400/5013], Loss: 0.7007\n",
      "Epoch [25/50], Step [3500/5013], Loss: 0.6685\n",
      "Epoch [25/50], Step [3600/5013], Loss: 0.7129\n",
      "Epoch [25/50], Step [3700/5013], Loss: 0.7046\n",
      "Epoch [25/50], Step [3800/5013], Loss: 0.6949\n",
      "Epoch [25/50], Step [3900/5013], Loss: 0.7051\n",
      "Epoch [25/50], Step [4000/5013], Loss: 0.6895\n",
      "Epoch [25/50], Step [4100/5013], Loss: 0.7162\n",
      "Epoch [25/50], Step [4200/5013], Loss: 0.7138\n",
      "Epoch [25/50], Step [4300/5013], Loss: 0.7078\n",
      "Epoch [25/50], Step [4400/5013], Loss: 0.7091\n",
      "Epoch [25/50], Step [4500/5013], Loss: 0.7200\n",
      "Epoch [25/50], Step [4600/5013], Loss: 0.7130\n",
      "Epoch [25/50], Step [4700/5013], Loss: 0.7083\n",
      "Epoch [25/50], Step [4800/5013], Loss: 0.6882\n",
      "Epoch [25/50], Step [4900/5013], Loss: 0.7029\n",
      "Epoch [25/50], Step [5000/5013], Loss: 0.7108\n",
      "Epoch [25/50] Average Loss: 0.7037\n",
      "Epoch [26/50], Step [100/5013], Loss: 0.7189\n",
      "Epoch [26/50], Step [200/5013], Loss: 0.7100\n",
      "Epoch [26/50], Step [300/5013], Loss: 0.6895\n",
      "Epoch [26/50], Step [400/5013], Loss: 0.6883\n",
      "Epoch [26/50], Step [500/5013], Loss: 0.6905\n",
      "Epoch [26/50], Step [600/5013], Loss: 0.7073\n",
      "Epoch [26/50], Step [700/5013], Loss: 0.7199\n",
      "Epoch [26/50], Step [800/5013], Loss: 0.7058\n",
      "Epoch [26/50], Step [900/5013], Loss: 0.7187\n",
      "Epoch [26/50], Step [1000/5013], Loss: 0.7036\n",
      "Epoch [26/50], Step [1100/5013], Loss: 0.6988\n",
      "Epoch [26/50], Step [1200/5013], Loss: 0.6828\n",
      "Epoch [26/50], Step [1300/5013], Loss: 0.6720\n",
      "Epoch [26/50], Step [1400/5013], Loss: 0.7217\n",
      "Epoch [26/50], Step [1500/5013], Loss: 0.6909\n",
      "Epoch [26/50], Step [1600/5013], Loss: 0.7361\n",
      "Epoch [26/50], Step [1700/5013], Loss: 0.6956\n",
      "Epoch [26/50], Step [1800/5013], Loss: 0.6807\n",
      "Epoch [26/50], Step [1900/5013], Loss: 0.6929\n",
      "Epoch [26/50], Step [2000/5013], Loss: 0.7067\n",
      "Epoch [26/50], Step [2100/5013], Loss: 0.6777\n",
      "Epoch [26/50], Step [2200/5013], Loss: 0.6916\n",
      "Epoch [26/50], Step [2300/5013], Loss: 0.7066\n",
      "Epoch [26/50], Step [2400/5013], Loss: 0.7227\n",
      "Epoch [26/50], Step [2500/5013], Loss: 0.6796\n",
      "Epoch [26/50], Step [2600/5013], Loss: 0.7036\n",
      "Epoch [26/50], Step [2700/5013], Loss: 0.6997\n",
      "Epoch [26/50], Step [2800/5013], Loss: 0.7262\n",
      "Epoch [26/50], Step [2900/5013], Loss: 0.6962\n",
      "Epoch [26/50], Step [3000/5013], Loss: 0.7091\n",
      "Epoch [26/50], Step [3100/5013], Loss: 0.6796\n",
      "Epoch [26/50], Step [3200/5013], Loss: 0.7152\n",
      "Epoch [26/50], Step [3300/5013], Loss: 0.6930\n",
      "Epoch [26/50], Step [3400/5013], Loss: 0.7086\n",
      "Epoch [26/50], Step [3500/5013], Loss: 0.6944\n",
      "Epoch [26/50], Step [3600/5013], Loss: 0.7186\n",
      "Epoch [26/50], Step [3700/5013], Loss: 0.7078\n",
      "Epoch [26/50], Step [3800/5013], Loss: 0.7033\n",
      "Epoch [26/50], Step [3900/5013], Loss: 0.6820\n",
      "Epoch [26/50], Step [4000/5013], Loss: 0.7278\n",
      "Epoch [26/50], Step [4100/5013], Loss: 0.7044\n",
      "Epoch [26/50], Step [4200/5013], Loss: 0.7089\n",
      "Epoch [26/50], Step [4300/5013], Loss: 0.7080\n",
      "Epoch [26/50], Step [4400/5013], Loss: 0.7125\n",
      "Epoch [26/50], Step [4500/5013], Loss: 0.6920\n",
      "Epoch [26/50], Step [4600/5013], Loss: 0.7050\n",
      "Epoch [26/50], Step [4700/5013], Loss: 0.6946\n",
      "Epoch [26/50], Step [4800/5013], Loss: 0.6985\n",
      "Epoch [26/50], Step [4900/5013], Loss: 0.7120\n",
      "Epoch [26/50], Step [5000/5013], Loss: 0.6941\n",
      "Epoch [26/50] Average Loss: 0.7022\n",
      "Epoch [27/50], Step [100/5013], Loss: 0.6915\n",
      "Epoch [27/50], Step [200/5013], Loss: 0.7059\n",
      "Epoch [27/50], Step [300/5013], Loss: 0.6973\n",
      "Epoch [27/50], Step [400/5013], Loss: 0.7732\n",
      "Epoch [27/50], Step [500/5013], Loss: 0.6915\n",
      "Epoch [27/50], Step [600/5013], Loss: 0.6781\n",
      "Epoch [27/50], Step [700/5013], Loss: 0.7063\n",
      "Epoch [27/50], Step [800/5013], Loss: 0.6723\n",
      "Epoch [27/50], Step [900/5013], Loss: 0.6911\n",
      "Epoch [27/50], Step [1000/5013], Loss: 0.6985\n",
      "Epoch [27/50], Step [1100/5013], Loss: 0.6775\n",
      "Epoch [27/50], Step [1200/5013], Loss: 0.7037\n",
      "Epoch [27/50], Step [1300/5013], Loss: 0.6844\n",
      "Epoch [27/50], Step [1400/5013], Loss: 0.6957\n",
      "Epoch [27/50], Step [1500/5013], Loss: 0.7101\n",
      "Epoch [27/50], Step [1600/5013], Loss: 0.6992\n",
      "Epoch [27/50], Step [1700/5013], Loss: 0.6965\n",
      "Epoch [27/50], Step [1800/5013], Loss: 0.6796\n",
      "Epoch [27/50], Step [1900/5013], Loss: 0.7011\n",
      "Epoch [27/50], Step [2000/5013], Loss: 0.6906\n",
      "Epoch [27/50], Step [2100/5013], Loss: 0.6907\n",
      "Epoch [27/50], Step [2200/5013], Loss: 0.6906\n",
      "Epoch [27/50], Step [2300/5013], Loss: 0.6862\n",
      "Epoch [27/50], Step [2400/5013], Loss: 0.6818\n",
      "Epoch [27/50], Step [2500/5013], Loss: 0.7110\n",
      "Epoch [27/50], Step [2600/5013], Loss: 0.7012\n",
      "Epoch [27/50], Step [2700/5013], Loss: 0.7038\n",
      "Epoch [27/50], Step [2800/5013], Loss: 0.6904\n",
      "Epoch [27/50], Step [2900/5013], Loss: 0.7061\n",
      "Epoch [27/50], Step [3000/5013], Loss: 0.7302\n",
      "Epoch [27/50], Step [3100/5013], Loss: 0.7001\n",
      "Epoch [27/50], Step [3200/5013], Loss: 0.7080\n",
      "Epoch [27/50], Step [3300/5013], Loss: 0.7014\n",
      "Epoch [27/50], Step [3400/5013], Loss: 0.7290\n",
      "Epoch [27/50], Step [3500/5013], Loss: 0.6960\n",
      "Epoch [27/50], Step [3600/5013], Loss: 0.6937\n",
      "Epoch [27/50], Step [3700/5013], Loss: 0.6818\n",
      "Epoch [27/50], Step [3800/5013], Loss: 0.7147\n",
      "Epoch [27/50], Step [3900/5013], Loss: 0.6974\n",
      "Epoch [27/50], Step [4000/5013], Loss: 0.6892\n",
      "Epoch [27/50], Step [4100/5013], Loss: 0.7024\n",
      "Epoch [27/50], Step [4200/5013], Loss: 0.7045\n",
      "Epoch [27/50], Step [4300/5013], Loss: 0.7181\n",
      "Epoch [27/50], Step [4400/5013], Loss: 0.6937\n",
      "Epoch [27/50], Step [4500/5013], Loss: 0.7691\n",
      "Epoch [27/50], Step [4600/5013], Loss: 0.6968\n",
      "Epoch [27/50], Step [4700/5013], Loss: 0.7168\n",
      "Epoch [27/50], Step [4800/5013], Loss: 0.7064\n",
      "Epoch [27/50], Step [4900/5013], Loss: 0.6994\n",
      "Epoch [27/50], Step [5000/5013], Loss: 0.6954\n",
      "Epoch [27/50] Average Loss: 0.7010\n",
      "Epoch [28/50], Step [100/5013], Loss: 0.7017\n",
      "Epoch [28/50], Step [200/5013], Loss: 0.6985\n",
      "Epoch [28/50], Step [300/5013], Loss: 0.7196\n",
      "Epoch [28/50], Step [400/5013], Loss: 0.6808\n",
      "Epoch [28/50], Step [500/5013], Loss: 0.6796\n",
      "Epoch [28/50], Step [600/5013], Loss: 0.6917\n",
      "Epoch [28/50], Step [700/5013], Loss: 0.6675\n",
      "Epoch [28/50], Step [800/5013], Loss: 0.6937\n",
      "Epoch [28/50], Step [900/5013], Loss: 0.6860\n",
      "Epoch [28/50], Step [1000/5013], Loss: 0.6936\n",
      "Epoch [28/50], Step [1100/5013], Loss: 0.6931\n",
      "Epoch [28/50], Step [1200/5013], Loss: 0.6983\n",
      "Epoch [28/50], Step [1300/5013], Loss: 0.6806\n",
      "Epoch [28/50], Step [1400/5013], Loss: 0.7088\n",
      "Epoch [28/50], Step [1500/5013], Loss: 0.6975\n",
      "Epoch [28/50], Step [1600/5013], Loss: 0.6834\n",
      "Epoch [28/50], Step [1700/5013], Loss: 0.6934\n",
      "Epoch [28/50], Step [1800/5013], Loss: 0.6915\n",
      "Epoch [28/50], Step [1900/5013], Loss: 0.6768\n",
      "Epoch [28/50], Step [2000/5013], Loss: 0.7137\n",
      "Epoch [28/50], Step [2100/5013], Loss: 0.6971\n",
      "Epoch [28/50], Step [2200/5013], Loss: 0.6836\n",
      "Epoch [28/50], Step [2300/5013], Loss: 0.6943\n",
      "Epoch [28/50], Step [2400/5013], Loss: 0.7045\n",
      "Epoch [28/50], Step [2500/5013], Loss: 0.6880\n",
      "Epoch [28/50], Step [2600/5013], Loss: 0.6892\n",
      "Epoch [28/50], Step [2700/5013], Loss: 0.6890\n",
      "Epoch [28/50], Step [2800/5013], Loss: 0.7132\n",
      "Epoch [28/50], Step [2900/5013], Loss: 0.7104\n",
      "Epoch [28/50], Step [3000/5013], Loss: 0.6971\n",
      "Epoch [28/50], Step [3100/5013], Loss: 0.6884\n",
      "Epoch [28/50], Step [3200/5013], Loss: 0.6601\n",
      "Epoch [28/50], Step [3300/5013], Loss: 0.7497\n",
      "Epoch [28/50], Step [3400/5013], Loss: 0.6734\n",
      "Epoch [28/50], Step [3500/5013], Loss: 0.6945\n",
      "Epoch [28/50], Step [3600/5013], Loss: 0.6869\n",
      "Epoch [28/50], Step [3700/5013], Loss: 0.7047\n",
      "Epoch [28/50], Step [3800/5013], Loss: 0.7003\n",
      "Epoch [28/50], Step [3900/5013], Loss: 0.7074\n",
      "Epoch [28/50], Step [4000/5013], Loss: 0.7055\n",
      "Epoch [28/50], Step [4100/5013], Loss: 0.7088\n",
      "Epoch [28/50], Step [4200/5013], Loss: 0.6930\n",
      "Epoch [28/50], Step [4300/5013], Loss: 0.7128\n",
      "Epoch [28/50], Step [4400/5013], Loss: 0.6986\n",
      "Epoch [28/50], Step [4500/5013], Loss: 0.7095\n",
      "Epoch [28/50], Step [4600/5013], Loss: 0.7076\n",
      "Epoch [28/50], Step [4700/5013], Loss: 0.7271\n",
      "Epoch [28/50], Step [4800/5013], Loss: 0.6856\n",
      "Epoch [28/50], Step [4900/5013], Loss: 0.7200\n",
      "Epoch [28/50], Step [5000/5013], Loss: 0.7155\n",
      "Epoch [28/50] Average Loss: 0.6972\n",
      "Epoch [29/50], Step [100/5013], Loss: 0.6969\n",
      "Epoch [29/50], Step [200/5013], Loss: 0.6689\n",
      "Epoch [29/50], Step [300/5013], Loss: 0.6679\n",
      "Epoch [29/50], Step [400/5013], Loss: 0.6708\n",
      "Epoch [29/50], Step [500/5013], Loss: 0.6659\n",
      "Epoch [29/50], Step [600/5013], Loss: 0.6847\n",
      "Epoch [29/50], Step [700/5013], Loss: 0.6755\n",
      "Epoch [29/50], Step [800/5013], Loss: 0.6767\n",
      "Epoch [29/50], Step [900/5013], Loss: 0.7064\n",
      "Epoch [29/50], Step [1000/5013], Loss: 0.6853\n",
      "Epoch [29/50], Step [1100/5013], Loss: 0.6907\n",
      "Epoch [29/50], Step [1200/5013], Loss: 0.6778\n",
      "Epoch [29/50], Step [1300/5013], Loss: 0.7064\n",
      "Epoch [29/50], Step [1400/5013], Loss: 0.7063\n",
      "Epoch [29/50], Step [1500/5013], Loss: 0.6844\n",
      "Epoch [29/50], Step [1600/5013], Loss: 0.6567\n",
      "Epoch [29/50], Step [1700/5013], Loss: 0.6941\n",
      "Epoch [29/50], Step [1800/5013], Loss: 0.6927\n",
      "Epoch [29/50], Step [1900/5013], Loss: 0.6870\n",
      "Epoch [29/50], Step [2000/5013], Loss: 0.7029\n",
      "Epoch [29/50], Step [2100/5013], Loss: 0.6739\n",
      "Epoch [29/50], Step [2200/5013], Loss: 0.7125\n",
      "Epoch [29/50], Step [2300/5013], Loss: 0.7056\n",
      "Epoch [29/50], Step [2400/5013], Loss: 0.6754\n",
      "Epoch [29/50], Step [2500/5013], Loss: 0.6667\n",
      "Epoch [29/50], Step [2600/5013], Loss: 0.7004\n",
      "Epoch [29/50], Step [2700/5013], Loss: 0.6942\n",
      "Epoch [29/50], Step [2800/5013], Loss: 0.7069\n",
      "Epoch [29/50], Step [2900/5013], Loss: 0.7030\n",
      "Epoch [29/50], Step [3000/5013], Loss: 0.6986\n",
      "Epoch [29/50], Step [3100/5013], Loss: 0.7099\n",
      "Epoch [29/50], Step [3200/5013], Loss: 0.7131\n",
      "Epoch [29/50], Step [3300/5013], Loss: 0.7235\n",
      "Epoch [29/50], Step [3400/5013], Loss: 0.6928\n",
      "Epoch [29/50], Step [3500/5013], Loss: 0.7041\n",
      "Epoch [29/50], Step [3600/5013], Loss: 0.6948\n",
      "Epoch [29/50], Step [3700/5013], Loss: 0.6933\n",
      "Epoch [29/50], Step [3800/5013], Loss: 0.6937\n",
      "Epoch [29/50], Step [3900/5013], Loss: 0.7011\n",
      "Epoch [29/50], Step [4000/5013], Loss: 0.7093\n",
      "Epoch [29/50], Step [4100/5013], Loss: 0.7027\n",
      "Epoch [29/50], Step [4200/5013], Loss: 0.7151\n",
      "Epoch [29/50], Step [4300/5013], Loss: 0.6944\n",
      "Epoch [29/50], Step [4400/5013], Loss: 0.7080\n",
      "Epoch [29/50], Step [4500/5013], Loss: 0.6912\n",
      "Epoch [29/50], Step [4600/5013], Loss: 0.7214\n",
      "Epoch [29/50], Step [4700/5013], Loss: 0.7053\n",
      "Epoch [29/50], Step [4800/5013], Loss: 0.7033\n",
      "Epoch [29/50], Step [4900/5013], Loss: 0.7007\n",
      "Epoch [29/50], Step [5000/5013], Loss: 0.7193\n",
      "Epoch [29/50] Average Loss: 0.6945\n",
      "Epoch [30/50], Step [100/5013], Loss: 0.6485\n",
      "Epoch [30/50], Step [200/5013], Loss: 0.7010\n",
      "Epoch [30/50], Step [300/5013], Loss: 0.6670\n",
      "Epoch [30/50], Step [400/5013], Loss: 0.6741\n",
      "Epoch [30/50], Step [500/5013], Loss: 0.7287\n",
      "Epoch [30/50], Step [600/5013], Loss: 0.6826\n",
      "Epoch [30/50], Step [700/5013], Loss: 0.6759\n",
      "Epoch [30/50], Step [800/5013], Loss: 0.6675\n",
      "Epoch [30/50], Step [900/5013], Loss: 0.6826\n",
      "Epoch [30/50], Step [1000/5013], Loss: 0.6881\n",
      "Epoch [30/50], Step [1100/5013], Loss: 0.7038\n",
      "Epoch [30/50], Step [1200/5013], Loss: 0.6814\n",
      "Epoch [30/50], Step [1300/5013], Loss: 0.6899\n",
      "Epoch [30/50], Step [1400/5013], Loss: 0.7032\n",
      "Epoch [30/50], Step [1500/5013], Loss: 0.7049\n",
      "Epoch [30/50], Step [1600/5013], Loss: 0.6973\n",
      "Epoch [30/50], Step [1700/5013], Loss: 0.6944\n",
      "Epoch [30/50], Step [1800/5013], Loss: 0.7033\n",
      "Epoch [30/50], Step [1900/5013], Loss: 0.7101\n",
      "Epoch [30/50], Step [2000/5013], Loss: 0.6878\n",
      "Epoch [30/50], Step [2100/5013], Loss: 0.6876\n",
      "Epoch [30/50], Step [2200/5013], Loss: 0.6931\n",
      "Epoch [30/50], Step [2300/5013], Loss: 0.7126\n",
      "Epoch [30/50], Step [2400/5013], Loss: 0.7116\n",
      "Epoch [30/50], Step [2500/5013], Loss: 0.6686\n",
      "Epoch [30/50], Step [2600/5013], Loss: 0.6808\n",
      "Epoch [30/50], Step [2700/5013], Loss: 0.7017\n",
      "Epoch [30/50], Step [2800/5013], Loss: 0.7095\n",
      "Epoch [30/50], Step [2900/5013], Loss: 0.6887\n",
      "Epoch [30/50], Step [3000/5013], Loss: 0.6742\n",
      "Epoch [30/50], Step [3100/5013], Loss: 0.6872\n",
      "Epoch [30/50], Step [3200/5013], Loss: 0.6794\n",
      "Epoch [30/50], Step [3300/5013], Loss: 0.6667\n",
      "Epoch [30/50], Step [3400/5013], Loss: 0.6995\n",
      "Epoch [30/50], Step [3500/5013], Loss: 0.6691\n",
      "Epoch [30/50], Step [3600/5013], Loss: 0.7155\n",
      "Epoch [30/50], Step [3700/5013], Loss: 0.6696\n",
      "Epoch [30/50], Step [3800/5013], Loss: 0.6903\n",
      "Epoch [30/50], Step [3900/5013], Loss: 0.6912\n",
      "Epoch [30/50], Step [4000/5013], Loss: 0.6959\n",
      "Epoch [30/50], Step [4100/5013], Loss: 0.6911\n",
      "Epoch [30/50], Step [4200/5013], Loss: 0.6719\n",
      "Epoch [30/50], Step [4300/5013], Loss: 0.6984\n",
      "Epoch [30/50], Step [4400/5013], Loss: 0.6835\n",
      "Epoch [30/50], Step [4500/5013], Loss: 0.6971\n",
      "Epoch [30/50], Step [4600/5013], Loss: 0.6767\n",
      "Epoch [30/50], Step [4700/5013], Loss: 0.6922\n",
      "Epoch [30/50], Step [4800/5013], Loss: 0.6849\n",
      "Epoch [30/50], Step [4900/5013], Loss: 0.7223\n",
      "Epoch [30/50], Step [5000/5013], Loss: 0.6725\n",
      "Epoch [30/50] Average Loss: 0.6895\n",
      "Epoch [31/50], Step [100/5013], Loss: 0.6895\n",
      "Epoch [31/50], Step [200/5013], Loss: 0.6946\n",
      "Epoch [31/50], Step [300/5013], Loss: 0.7285\n",
      "Epoch [31/50], Step [400/5013], Loss: 0.6921\n",
      "Epoch [31/50], Step [500/5013], Loss: 0.7033\n",
      "Epoch [31/50], Step [600/5013], Loss: 0.7950\n",
      "Epoch [31/50], Step [700/5013], Loss: 0.6856\n",
      "Epoch [31/50], Step [800/5013], Loss: 0.7025\n",
      "Epoch [31/50], Step [900/5013], Loss: 0.6751\n",
      "Epoch [31/50], Step [1000/5013], Loss: 0.6759\n",
      "Epoch [31/50], Step [1100/5013], Loss: 0.6692\n",
      "Epoch [31/50], Step [1200/5013], Loss: 0.6837\n",
      "Epoch [31/50], Step [1300/5013], Loss: 0.6900\n",
      "Epoch [31/50], Step [1400/5013], Loss: 0.6817\n",
      "Epoch [31/50], Step [1500/5013], Loss: 0.7016\n",
      "Epoch [31/50], Step [1600/5013], Loss: 0.7004\n",
      "Epoch [31/50], Step [1700/5013], Loss: 0.6812\n",
      "Epoch [31/50], Step [1800/5013], Loss: 0.6878\n",
      "Epoch [31/50], Step [1900/5013], Loss: 0.7007\n",
      "Epoch [31/50], Step [2000/5013], Loss: 0.7108\n",
      "Epoch [31/50], Step [2100/5013], Loss: 0.6535\n",
      "Epoch [31/50], Step [2200/5013], Loss: 0.6823\n",
      "Epoch [31/50], Step [2300/5013], Loss: 0.6806\n",
      "Epoch [31/50], Step [2400/5013], Loss: 0.6956\n",
      "Epoch [31/50], Step [2500/5013], Loss: 0.6594\n",
      "Epoch [31/50], Step [2600/5013], Loss: 0.6625\n",
      "Epoch [31/50], Step [2700/5013], Loss: 0.6897\n",
      "Epoch [31/50], Step [2800/5013], Loss: 0.6823\n",
      "Epoch [31/50], Step [2900/5013], Loss: 0.6843\n",
      "Epoch [31/50], Step [3000/5013], Loss: 0.6962\n",
      "Epoch [31/50], Step [3100/5013], Loss: 0.6912\n",
      "Epoch [31/50], Step [3200/5013], Loss: 0.7035\n",
      "Epoch [31/50], Step [3300/5013], Loss: 0.6949\n",
      "Epoch [31/50], Step [3400/5013], Loss: 0.7153\n",
      "Epoch [31/50], Step [3500/5013], Loss: 0.6862\n",
      "Epoch [31/50], Step [3600/5013], Loss: 0.6988\n",
      "Epoch [31/50], Step [3700/5013], Loss: 0.6903\n",
      "Epoch [31/50], Step [3800/5013], Loss: 0.7053\n",
      "Epoch [31/50], Step [3900/5013], Loss: 0.7016\n",
      "Epoch [31/50], Step [4000/5013], Loss: 0.6817\n",
      "Epoch [31/50], Step [4100/5013], Loss: 0.7078\n",
      "Epoch [31/50], Step [4200/5013], Loss: 0.6979\n",
      "Epoch [31/50], Step [4300/5013], Loss: 0.6957\n",
      "Epoch [31/50], Step [4400/5013], Loss: 0.7178\n",
      "Epoch [31/50], Step [4500/5013], Loss: 0.7046\n",
      "Epoch [31/50], Step [4600/5013], Loss: 0.6906\n",
      "Epoch [31/50], Step [4700/5013], Loss: 0.6929\n",
      "Epoch [31/50], Step [4800/5013], Loss: 0.6977\n",
      "Epoch [31/50], Step [4900/5013], Loss: 0.7159\n",
      "Epoch [31/50], Step [5000/5013], Loss: 0.7103\n",
      "Epoch [31/50] Average Loss: 0.6948\n",
      "Epoch [32/50], Step [100/5013], Loss: 0.6795\n",
      "Epoch [32/50], Step [200/5013], Loss: 0.6770\n",
      "Epoch [32/50], Step [300/5013], Loss: 0.6772\n",
      "Epoch [32/50], Step [400/5013], Loss: 0.6631\n",
      "Epoch [32/50], Step [500/5013], Loss: 0.7342\n",
      "Epoch [32/50], Step [600/5013], Loss: 0.6722\n",
      "Epoch [32/50], Step [700/5013], Loss: 0.6589\n",
      "Epoch [32/50], Step [800/5013], Loss: 0.6954\n",
      "Epoch [32/50], Step [900/5013], Loss: 0.6852\n",
      "Epoch [32/50], Step [1000/5013], Loss: 0.6843\n",
      "Epoch [32/50], Step [1100/5013], Loss: 0.6889\n",
      "Epoch [32/50], Step [1200/5013], Loss: 0.6856\n",
      "Epoch [32/50], Step [1300/5013], Loss: 0.6709\n",
      "Epoch [32/50], Step [1400/5013], Loss: 0.6907\n",
      "Epoch [32/50], Step [1500/5013], Loss: 0.6796\n",
      "Epoch [32/50], Step [1600/5013], Loss: 0.6824\n",
      "Epoch [32/50], Step [1700/5013], Loss: 0.6903\n",
      "Epoch [32/50], Step [1800/5013], Loss: 0.6956\n",
      "Epoch [32/50], Step [1900/5013], Loss: 0.6763\n",
      "Epoch [32/50], Step [2000/5013], Loss: 0.6916\n",
      "Epoch [32/50], Step [2100/5013], Loss: 0.6803\n",
      "Epoch [32/50], Step [2200/5013], Loss: 0.6778\n",
      "Epoch [32/50], Step [2300/5013], Loss: 0.7068\n",
      "Epoch [32/50], Step [2400/5013], Loss: 0.6913\n",
      "Epoch [32/50], Step [2500/5013], Loss: 0.7019\n",
      "Epoch [32/50], Step [2600/5013], Loss: 0.6967\n",
      "Epoch [32/50], Step [2700/5013], Loss: 0.6762\n",
      "Epoch [32/50], Step [2800/5013], Loss: 0.6747\n",
      "Epoch [32/50], Step [2900/5013], Loss: 0.6740\n",
      "Epoch [32/50], Step [3000/5013], Loss: 0.6853\n",
      "Epoch [32/50], Step [3100/5013], Loss: 0.7045\n",
      "Epoch [32/50], Step [3200/5013], Loss: 0.7176\n",
      "Epoch [32/50], Step [3300/5013], Loss: 0.7176\n",
      "Epoch [32/50], Step [3400/5013], Loss: 0.6895\n",
      "Epoch [32/50], Step [3500/5013], Loss: 0.6682\n",
      "Epoch [32/50], Step [3600/5013], Loss: 0.6918\n",
      "Epoch [32/50], Step [3700/5013], Loss: 0.6765\n",
      "Epoch [32/50], Step [3800/5013], Loss: 0.7039\n",
      "Epoch [32/50], Step [3900/5013], Loss: 0.7018\n",
      "Epoch [32/50], Step [4000/5013], Loss: 0.6854\n",
      "Epoch [32/50], Step [4100/5013], Loss: 0.7096\n",
      "Epoch [32/50], Step [4200/5013], Loss: 0.6862\n",
      "Epoch [32/50], Step [4300/5013], Loss: 0.6962\n",
      "Epoch [32/50], Step [4400/5013], Loss: 0.6954\n",
      "Epoch [32/50], Step [4500/5013], Loss: 0.6973\n",
      "Epoch [32/50], Step [4600/5013], Loss: 0.6946\n",
      "Epoch [32/50], Step [4700/5013], Loss: 0.7175\n",
      "Epoch [32/50], Step [4800/5013], Loss: 0.6848\n",
      "Epoch [32/50], Step [4900/5013], Loss: 0.7019\n",
      "Epoch [32/50], Step [5000/5013], Loss: 0.6830\n",
      "Epoch [32/50] Average Loss: 0.6893\n",
      "Epoch [33/50], Step [100/5013], Loss: 0.6885\n",
      "Epoch [33/50], Step [200/5013], Loss: 0.6802\n",
      "Epoch [33/50], Step [300/5013], Loss: 0.6821\n",
      "Epoch [33/50], Step [400/5013], Loss: 0.6935\n",
      "Epoch [33/50], Step [500/5013], Loss: 0.6957\n",
      "Epoch [33/50], Step [600/5013], Loss: 0.6935\n",
      "Epoch [33/50], Step [700/5013], Loss: 0.6892\n",
      "Epoch [33/50], Step [800/5013], Loss: 0.6400\n",
      "Epoch [33/50], Step [900/5013], Loss: 0.6968\n",
      "Epoch [33/50], Step [1000/5013], Loss: 0.6990\n",
      "Epoch [33/50], Step [1100/5013], Loss: 0.6570\n",
      "Epoch [33/50], Step [1200/5013], Loss: 0.6635\n",
      "Epoch [33/50], Step [1300/5013], Loss: 0.6798\n",
      "Epoch [33/50], Step [1400/5013], Loss: 0.6683\n",
      "Epoch [33/50], Step [1500/5013], Loss: 0.6651\n",
      "Epoch [33/50], Step [1600/5013], Loss: 0.6593\n",
      "Epoch [33/50], Step [1700/5013], Loss: 0.6667\n",
      "Epoch [33/50], Step [1800/5013], Loss: 0.6841\n",
      "Epoch [33/50], Step [1900/5013], Loss: 0.6822\n",
      "Epoch [33/50], Step [2000/5013], Loss: 0.6785\n",
      "Epoch [33/50], Step [2100/5013], Loss: 0.6818\n",
      "Epoch [33/50], Step [2200/5013], Loss: 0.6899\n",
      "Epoch [33/50], Step [2300/5013], Loss: 0.6929\n",
      "Epoch [33/50], Step [2400/5013], Loss: 0.6682\n",
      "Epoch [33/50], Step [2500/5013], Loss: 0.6810\n",
      "Epoch [33/50], Step [2600/5013], Loss: 0.6862\n",
      "Epoch [33/50], Step [2700/5013], Loss: 0.6876\n",
      "Epoch [33/50], Step [2800/5013], Loss: 0.6976\n",
      "Epoch [33/50], Step [2900/5013], Loss: 0.6861\n",
      "Epoch [33/50], Step [3000/5013], Loss: 0.6663\n",
      "Epoch [33/50], Step [3100/5013], Loss: 0.6728\n",
      "Epoch [33/50], Step [3200/5013], Loss: 0.6776\n",
      "Epoch [33/50], Step [3300/5013], Loss: 0.6762\n",
      "Epoch [33/50], Step [3400/5013], Loss: 0.7032\n",
      "Epoch [33/50], Step [3500/5013], Loss: 0.6939\n",
      "Epoch [33/50], Step [3600/5013], Loss: 0.6694\n",
      "Epoch [33/50], Step [3700/5013], Loss: 0.7065\n",
      "Epoch [33/50], Step [3800/5013], Loss: 0.7394\n",
      "Epoch [33/50], Step [3900/5013], Loss: 0.6857\n",
      "Epoch [33/50], Step [4000/5013], Loss: 0.6898\n",
      "Epoch [33/50], Step [4100/5013], Loss: 0.6903\n",
      "Epoch [33/50], Step [4200/5013], Loss: 0.6767\n",
      "Epoch [33/50], Step [4300/5013], Loss: 0.7098\n",
      "Epoch [33/50], Step [4400/5013], Loss: 0.6926\n",
      "Epoch [33/50], Step [4500/5013], Loss: 0.6850\n",
      "Epoch [33/50], Step [4600/5013], Loss: 0.6958\n",
      "Epoch [33/50], Step [4700/5013], Loss: 0.6942\n",
      "Epoch [33/50], Step [4800/5013], Loss: 0.6784\n",
      "Epoch [33/50], Step [4900/5013], Loss: 0.7897\n",
      "Epoch [33/50], Step [5000/5013], Loss: 0.7113\n",
      "Epoch [33/50] Average Loss: 0.6869\n",
      "Epoch [34/50], Step [100/5013], Loss: 0.6797\n",
      "Epoch [34/50], Step [200/5013], Loss: 0.6917\n",
      "Epoch [34/50], Step [300/5013], Loss: 0.6800\n",
      "Epoch [34/50], Step [400/5013], Loss: 0.7090\n",
      "Epoch [34/50], Step [500/5013], Loss: 0.6863\n",
      "Epoch [34/50], Step [600/5013], Loss: 0.6967\n",
      "Epoch [34/50], Step [700/5013], Loss: 0.7114\n",
      "Epoch [34/50], Step [800/5013], Loss: 0.6955\n",
      "Epoch [34/50], Step [900/5013], Loss: 0.6830\n",
      "Epoch [34/50], Step [1000/5013], Loss: 0.7123\n",
      "Epoch [34/50], Step [1100/5013], Loss: 0.6804\n",
      "Epoch [34/50], Step [1200/5013], Loss: 0.6649\n",
      "Epoch [34/50], Step [1300/5013], Loss: 0.6584\n",
      "Epoch [34/50], Step [1400/5013], Loss: 0.6829\n",
      "Epoch [34/50], Step [1500/5013], Loss: 0.6828\n",
      "Epoch [34/50], Step [1600/5013], Loss: 0.6889\n",
      "Epoch [34/50], Step [1700/5013], Loss: 0.7022\n",
      "Epoch [34/50], Step [1800/5013], Loss: 0.6827\n",
      "Epoch [34/50], Step [1900/5013], Loss: 0.7004\n",
      "Epoch [34/50], Step [2000/5013], Loss: 0.6899\n",
      "Epoch [34/50], Step [2100/5013], Loss: 0.6774\n",
      "Epoch [34/50], Step [2200/5013], Loss: 0.6638\n",
      "Epoch [34/50], Step [2300/5013], Loss: 0.6807\n",
      "Epoch [34/50], Step [2400/5013], Loss: 0.6893\n",
      "Epoch [34/50], Step [2500/5013], Loss: 0.6816\n",
      "Epoch [34/50], Step [2600/5013], Loss: 0.6849\n",
      "Epoch [34/50], Step [2700/5013], Loss: 0.6793\n",
      "Epoch [34/50], Step [2800/5013], Loss: 0.6780\n",
      "Epoch [34/50], Step [2900/5013], Loss: 0.6545\n",
      "Epoch [34/50], Step [3000/5013], Loss: 0.6966\n",
      "Epoch [34/50], Step [3100/5013], Loss: 0.6754\n",
      "Epoch [34/50], Step [3200/5013], Loss: 0.6942\n",
      "Epoch [34/50], Step [3300/5013], Loss: 0.8030\n",
      "Epoch [34/50], Step [3400/5013], Loss: 0.6960\n",
      "Epoch [34/50], Step [3500/5013], Loss: 0.6950\n",
      "Epoch [34/50], Step [3600/5013], Loss: 0.6897\n",
      "Epoch [34/50], Step [3700/5013], Loss: 0.6724\n",
      "Epoch [34/50], Step [3800/5013], Loss: 0.6716\n",
      "Epoch [34/50], Step [3900/5013], Loss: 0.6699\n",
      "Epoch [34/50], Step [4000/5013], Loss: 0.6898\n",
      "Epoch [34/50], Step [4100/5013], Loss: 0.6661\n",
      "Epoch [34/50], Step [4200/5013], Loss: 0.6877\n",
      "Epoch [34/50], Step [4300/5013], Loss: 0.6740\n",
      "Epoch [34/50], Step [4400/5013], Loss: 0.6850\n",
      "Epoch [34/50], Step [4500/5013], Loss: 0.6776\n",
      "Epoch [34/50], Step [4600/5013], Loss: 0.6981\n",
      "Epoch [34/50], Step [4700/5013], Loss: 0.6805\n",
      "Epoch [34/50], Step [4800/5013], Loss: 0.6915\n",
      "Epoch [34/50], Step [4900/5013], Loss: 0.6720\n",
      "Epoch [34/50], Step [5000/5013], Loss: 0.7066\n",
      "Epoch [34/50] Average Loss: 0.6871\n",
      "Epoch [35/50], Step [100/5013], Loss: 0.6884\n",
      "Epoch [35/50], Step [200/5013], Loss: 0.6739\n",
      "Epoch [35/50], Step [300/5013], Loss: 0.6627\n",
      "Epoch [35/50], Step [400/5013], Loss: 0.6773\n",
      "Epoch [35/50], Step [500/5013], Loss: 0.6817\n",
      "Epoch [35/50], Step [600/5013], Loss: 0.6774\n",
      "Epoch [35/50], Step [700/5013], Loss: 0.6971\n",
      "Epoch [35/50], Step [800/5013], Loss: 0.7102\n",
      "Epoch [35/50], Step [900/5013], Loss: 0.6907\n",
      "Epoch [35/50], Step [1000/5013], Loss: 0.6740\n",
      "Epoch [35/50], Step [1100/5013], Loss: 0.6995\n",
      "Epoch [35/50], Step [1200/5013], Loss: 0.6763\n",
      "Epoch [35/50], Step [1300/5013], Loss: 0.6720\n",
      "Epoch [35/50], Step [1400/5013], Loss: 0.6672\n",
      "Epoch [35/50], Step [1500/5013], Loss: 0.6819\n",
      "Epoch [35/50], Step [1600/5013], Loss: 0.6751\n",
      "Epoch [35/50], Step [1700/5013], Loss: 0.6801\n",
      "Epoch [35/50], Step [1800/5013], Loss: 0.6900\n",
      "Epoch [35/50], Step [1900/5013], Loss: 0.6876\n",
      "Epoch [35/50], Step [2000/5013], Loss: 0.6356\n",
      "Epoch [35/50], Step [2100/5013], Loss: 0.6768\n",
      "Epoch [35/50], Step [2200/5013], Loss: 0.6842\n",
      "Epoch [35/50], Step [2300/5013], Loss: 0.7056\n",
      "Epoch [35/50], Step [2400/5013], Loss: 0.6915\n",
      "Epoch [35/50], Step [2500/5013], Loss: 0.6743\n",
      "Epoch [35/50], Step [2600/5013], Loss: 0.6839\n",
      "Epoch [35/50], Step [2700/5013], Loss: 0.6812\n",
      "Epoch [35/50], Step [2800/5013], Loss: 0.6815\n",
      "Epoch [35/50], Step [2900/5013], Loss: 0.6808\n",
      "Epoch [35/50], Step [3000/5013], Loss: 0.6614\n",
      "Epoch [35/50], Step [3100/5013], Loss: 0.7168\n",
      "Epoch [35/50], Step [3200/5013], Loss: 0.7087\n",
      "Epoch [35/50], Step [3300/5013], Loss: 0.6921\n",
      "Epoch [35/50], Step [3400/5013], Loss: 0.6795\n",
      "Epoch [35/50], Step [3500/5013], Loss: 0.7018\n",
      "Epoch [35/50], Step [3600/5013], Loss: 0.7039\n",
      "Epoch [35/50], Step [3700/5013], Loss: 0.6753\n",
      "Epoch [35/50], Step [3800/5013], Loss: 0.6939\n",
      "Epoch [35/50], Step [3900/5013], Loss: 0.6934\n",
      "Epoch [35/50], Step [4000/5013], Loss: 0.7010\n",
      "Epoch [35/50], Step [4100/5013], Loss: 0.6980\n",
      "Epoch [35/50], Step [4200/5013], Loss: 0.6717\n",
      "Epoch [35/50], Step [4300/5013], Loss: 0.7093\n",
      "Epoch [35/50], Step [4400/5013], Loss: 0.6771\n",
      "Epoch [35/50], Step [4500/5013], Loss: 0.7012\n",
      "Epoch [35/50], Step [4600/5013], Loss: 0.6904\n",
      "Epoch [35/50], Step [4700/5013], Loss: 0.6916\n",
      "Epoch [35/50], Step [4800/5013], Loss: 0.7066\n",
      "Epoch [35/50], Step [4900/5013], Loss: 0.6660\n",
      "Epoch [35/50], Step [5000/5013], Loss: 0.6780\n",
      "Epoch [35/50] Average Loss: 0.6856\n",
      "Epoch [36/50], Step [100/5013], Loss: 0.6757\n",
      "Epoch [36/50], Step [200/5013], Loss: 0.6871\n",
      "Epoch [36/50], Step [300/5013], Loss: 0.6874\n",
      "Epoch [36/50], Step [400/5013], Loss: 0.6843\n",
      "Epoch [36/50], Step [500/5013], Loss: 0.6971\n",
      "Epoch [36/50], Step [600/5013], Loss: 0.6903\n",
      "Epoch [36/50], Step [700/5013], Loss: 0.6887\n",
      "Epoch [36/50], Step [800/5013], Loss: 0.6780\n",
      "Epoch [36/50], Step [900/5013], Loss: 0.6950\n",
      "Epoch [36/50], Step [1000/5013], Loss: 0.6508\n",
      "Epoch [36/50], Step [1100/5013], Loss: 0.6802\n",
      "Epoch [36/50], Step [1200/5013], Loss: 0.6834\n",
      "Epoch [36/50], Step [1300/5013], Loss: 0.6859\n",
      "Epoch [36/50], Step [1400/5013], Loss: 0.6991\n",
      "Epoch [36/50], Step [1500/5013], Loss: 0.6745\n",
      "Epoch [36/50], Step [1600/5013], Loss: 0.6675\n",
      "Epoch [36/50], Step [1700/5013], Loss: 0.6928\n",
      "Epoch [36/50], Step [1800/5013], Loss: 0.6846\n",
      "Epoch [36/50], Step [1900/5013], Loss: 0.6918\n",
      "Epoch [36/50], Step [2000/5013], Loss: 0.6550\n",
      "Epoch [36/50], Step [2100/5013], Loss: 0.7035\n",
      "Epoch [36/50], Step [2200/5013], Loss: 0.6815\n",
      "Epoch [36/50], Step [2300/5013], Loss: 0.6855\n",
      "Epoch [36/50], Step [2400/5013], Loss: 0.6971\n",
      "Epoch [36/50], Step [2500/5013], Loss: 0.6987\n",
      "Epoch [36/50], Step [2600/5013], Loss: 0.6691\n",
      "Epoch [36/50], Step [2700/5013], Loss: 0.6753\n",
      "Epoch [36/50], Step [2800/5013], Loss: 0.6896\n",
      "Epoch [36/50], Step [2900/5013], Loss: 0.6637\n",
      "Epoch [36/50], Step [3000/5013], Loss: 0.6692\n",
      "Epoch [36/50], Step [3100/5013], Loss: 0.6642\n",
      "Epoch [36/50], Step [3200/5013], Loss: 0.6725\n",
      "Epoch [36/50], Step [3300/5013], Loss: 0.6982\n",
      "Epoch [36/50], Step [3400/5013], Loss: 0.6752\n",
      "Epoch [36/50], Step [3500/5013], Loss: 0.6893\n",
      "Epoch [36/50], Step [3600/5013], Loss: 0.7027\n",
      "Epoch [36/50], Step [3700/5013], Loss: 0.6936\n",
      "Epoch [36/50], Step [3800/5013], Loss: 0.6872\n",
      "Epoch [36/50], Step [3900/5013], Loss: 0.6952\n",
      "Epoch [36/50], Step [4000/5013], Loss: 0.7883\n",
      "Epoch [36/50], Step [4100/5013], Loss: 0.7036\n",
      "Epoch [36/50], Step [4200/5013], Loss: 0.6707\n",
      "Epoch [36/50], Step [4300/5013], Loss: 0.6532\n",
      "Epoch [36/50], Step [4400/5013], Loss: 0.6921\n",
      "Epoch [36/50], Step [4500/5013], Loss: 0.6861\n",
      "Epoch [36/50], Step [4600/5013], Loss: 0.6750\n",
      "Epoch [36/50], Step [4700/5013], Loss: 0.6923\n",
      "Epoch [36/50], Step [4800/5013], Loss: 0.7014\n",
      "Epoch [36/50], Step [4900/5013], Loss: 0.6989\n",
      "Epoch [36/50], Step [5000/5013], Loss: 0.6921\n",
      "Epoch [36/50] Average Loss: 0.6863\n",
      "Epoch [37/50], Step [100/5013], Loss: 0.6669\n",
      "Epoch [37/50], Step [200/5013], Loss: 0.6439\n",
      "Epoch [37/50], Step [300/5013], Loss: 0.6653\n",
      "Epoch [37/50], Step [400/5013], Loss: 0.6820\n",
      "Epoch [37/50], Step [500/5013], Loss: 0.6967\n",
      "Epoch [37/50], Step [600/5013], Loss: 0.6934\n",
      "Epoch [37/50], Step [700/5013], Loss: 0.6663\n",
      "Epoch [37/50], Step [800/5013], Loss: 0.6826\n",
      "Epoch [37/50], Step [900/5013], Loss: 0.6548\n",
      "Epoch [37/50], Step [1000/5013], Loss: 0.6723\n",
      "Epoch [37/50], Step [1100/5013], Loss: 0.6834\n",
      "Epoch [37/50], Step [1200/5013], Loss: 0.6716\n",
      "Epoch [37/50], Step [1300/5013], Loss: 0.6751\n",
      "Epoch [37/50], Step [1400/5013], Loss: 0.6699\n",
      "Epoch [37/50], Step [1500/5013], Loss: 0.6821\n",
      "Epoch [37/50], Step [1600/5013], Loss: 0.6541\n",
      "Epoch [37/50], Step [1700/5013], Loss: 0.7149\n",
      "Epoch [37/50], Step [1800/5013], Loss: 0.6900\n",
      "Epoch [37/50], Step [1900/5013], Loss: 0.6628\n",
      "Epoch [37/50], Step [2000/5013], Loss: 0.6734\n",
      "Epoch [37/50], Step [2100/5013], Loss: 0.6648\n",
      "Epoch [37/50], Step [2200/5013], Loss: 0.6911\n",
      "Epoch [37/50], Step [2300/5013], Loss: 0.6861\n",
      "Epoch [37/50], Step [2400/5013], Loss: 0.6727\n",
      "Epoch [37/50], Step [2500/5013], Loss: 0.6729\n",
      "Epoch [37/50], Step [2600/5013], Loss: 0.6849\n",
      "Epoch [37/50], Step [2700/5013], Loss: 0.6927\n",
      "Epoch [37/50], Step [2800/5013], Loss: 0.6632\n",
      "Epoch [37/50], Step [2900/5013], Loss: 0.6667\n",
      "Epoch [37/50], Step [3000/5013], Loss: 0.6882\n",
      "Epoch [37/50], Step [3100/5013], Loss: 0.6740\n",
      "Epoch [37/50], Step [3200/5013], Loss: 0.6948\n",
      "Epoch [37/50], Step [3300/5013], Loss: 0.6735\n",
      "Epoch [37/50], Step [3400/5013], Loss: 0.6711\n",
      "Epoch [37/50], Step [3500/5013], Loss: 0.6828\n",
      "Epoch [37/50], Step [3600/5013], Loss: 0.6908\n",
      "Epoch [37/50], Step [3700/5013], Loss: 0.6776\n",
      "Epoch [37/50], Step [3800/5013], Loss: 0.6749\n",
      "Epoch [37/50], Step [3900/5013], Loss: 0.8473\n",
      "Epoch [37/50], Step [4000/5013], Loss: 0.6842\n",
      "Epoch [37/50], Step [4100/5013], Loss: 0.6832\n",
      "Epoch [37/50], Step [4200/5013], Loss: 0.6745\n",
      "Epoch [37/50], Step [4300/5013], Loss: 0.6920\n",
      "Epoch [37/50], Step [4400/5013], Loss: 0.6697\n",
      "Epoch [37/50], Step [4500/5013], Loss: 0.7005\n",
      "Epoch [37/50], Step [4600/5013], Loss: 0.6746\n",
      "Epoch [37/50], Step [4700/5013], Loss: 0.6840\n",
      "Epoch [37/50], Step [4800/5013], Loss: 0.7048\n",
      "Epoch [37/50], Step [4900/5013], Loss: 0.7031\n",
      "Epoch [37/50], Step [5000/5013], Loss: 0.6875\n",
      "Epoch [37/50] Average Loss: 0.6826\n",
      "Epoch [38/50], Step [100/5013], Loss: 0.6620\n",
      "Epoch [38/50], Step [200/5013], Loss: 0.6730\n",
      "Epoch [38/50], Step [300/5013], Loss: 0.6597\n",
      "Epoch [38/50], Step [400/5013], Loss: 0.6676\n",
      "Epoch [38/50], Step [500/5013], Loss: 0.6714\n",
      "Epoch [38/50], Step [600/5013], Loss: 0.6837\n",
      "Epoch [38/50], Step [700/5013], Loss: 0.6976\n",
      "Epoch [38/50], Step [800/5013], Loss: 0.6830\n",
      "Epoch [38/50], Step [900/5013], Loss: 0.6705\n",
      "Epoch [38/50], Step [1000/5013], Loss: 0.6487\n",
      "Epoch [38/50], Step [1100/5013], Loss: 0.7047\n",
      "Epoch [38/50], Step [1200/5013], Loss: 0.6627\n",
      "Epoch [38/50], Step [1300/5013], Loss: 0.6737\n",
      "Epoch [38/50], Step [1400/5013], Loss: 0.6751\n",
      "Epoch [38/50], Step [1500/5013], Loss: 0.6745\n",
      "Epoch [38/50], Step [1600/5013], Loss: 0.6555\n",
      "Epoch [38/50], Step [1700/5013], Loss: 0.6754\n",
      "Epoch [38/50], Step [1800/5013], Loss: 0.6715\n",
      "Epoch [38/50], Step [1900/5013], Loss: 0.6766\n",
      "Epoch [38/50], Step [2000/5013], Loss: 0.6588\n",
      "Epoch [38/50], Step [2100/5013], Loss: 0.6767\n",
      "Epoch [38/50], Step [2200/5013], Loss: 0.6717\n",
      "Epoch [38/50], Step [2300/5013], Loss: 0.6763\n",
      "Epoch [38/50], Step [2400/5013], Loss: 0.6851\n",
      "Epoch [38/50], Step [2500/5013], Loss: 0.7127\n",
      "Epoch [38/50], Step [2600/5013], Loss: 0.6837\n",
      "Epoch [38/50], Step [2700/5013], Loss: 0.6981\n",
      "Epoch [38/50], Step [2800/5013], Loss: 0.6964\n",
      "Epoch [38/50], Step [2900/5013], Loss: 0.6720\n",
      "Epoch [38/50], Step [3000/5013], Loss: 0.6747\n",
      "Epoch [38/50], Step [3100/5013], Loss: 0.6952\n",
      "Epoch [38/50], Step [3200/5013], Loss: 0.6841\n",
      "Epoch [38/50], Step [3300/5013], Loss: 0.6692\n",
      "Epoch [38/50], Step [3400/5013], Loss: 0.6527\n",
      "Epoch [38/50], Step [3500/5013], Loss: 0.6797\n",
      "Epoch [38/50], Step [3600/5013], Loss: 0.6877\n",
      "Epoch [38/50], Step [3700/5013], Loss: 0.6743\n",
      "Epoch [38/50], Step [3800/5013], Loss: 0.6742\n",
      "Epoch [38/50], Step [3900/5013], Loss: 0.6999\n",
      "Epoch [38/50], Step [4000/5013], Loss: 0.6715\n",
      "Epoch [38/50], Step [4100/5013], Loss: 0.6979\n",
      "Epoch [38/50], Step [4200/5013], Loss: 0.6605\n",
      "Epoch [38/50], Step [4300/5013], Loss: 0.7038\n",
      "Epoch [38/50], Step [4400/5013], Loss: 0.6939\n",
      "Epoch [38/50], Step [4500/5013], Loss: 0.6852\n",
      "Epoch [38/50], Step [4600/5013], Loss: 0.7200\n",
      "Epoch [38/50], Step [4700/5013], Loss: 0.6938\n",
      "Epoch [38/50], Step [4800/5013], Loss: 0.6662\n",
      "Epoch [38/50], Step [4900/5013], Loss: 0.7010\n",
      "Epoch [38/50], Step [5000/5013], Loss: 0.7033\n",
      "Epoch [38/50] Average Loss: 0.6801\n",
      "Epoch [39/50], Step [100/5013], Loss: 0.6781\n",
      "Epoch [39/50], Step [200/5013], Loss: 0.7233\n",
      "Epoch [39/50], Step [300/5013], Loss: 0.7058\n",
      "Epoch [39/50], Step [400/5013], Loss: 0.6838\n",
      "Epoch [39/50], Step [500/5013], Loss: 0.6670\n",
      "Epoch [39/50], Step [600/5013], Loss: 0.6769\n",
      "Epoch [39/50], Step [700/5013], Loss: 0.6724\n",
      "Epoch [39/50], Step [800/5013], Loss: 0.6961\n",
      "Epoch [39/50], Step [900/5013], Loss: 0.6598\n",
      "Epoch [39/50], Step [1000/5013], Loss: 0.6793\n",
      "Epoch [39/50], Step [1100/5013], Loss: 0.6615\n",
      "Epoch [39/50], Step [1200/5013], Loss: 0.6871\n",
      "Epoch [39/50], Step [1300/5013], Loss: 0.6538\n",
      "Epoch [39/50], Step [1400/5013], Loss: 0.6587\n",
      "Epoch [39/50], Step [1500/5013], Loss: 0.6991\n",
      "Epoch [39/50], Step [1600/5013], Loss: 0.6780\n",
      "Epoch [39/50], Step [1700/5013], Loss: 0.6674\n",
      "Epoch [39/50], Step [1800/5013], Loss: 0.6751\n",
      "Epoch [39/50], Step [1900/5013], Loss: 0.6954\n",
      "Epoch [39/50], Step [2000/5013], Loss: 0.6841\n",
      "Epoch [39/50], Step [2100/5013], Loss: 0.6537\n",
      "Epoch [39/50], Step [2200/5013], Loss: 0.6463\n",
      "Epoch [39/50], Step [2300/5013], Loss: 0.6874\n",
      "Epoch [39/50], Step [2400/5013], Loss: 0.6923\n",
      "Epoch [39/50], Step [2500/5013], Loss: 0.7075\n",
      "Epoch [39/50], Step [2600/5013], Loss: 0.7046\n",
      "Epoch [39/50], Step [2700/5013], Loss: 0.6623\n",
      "Epoch [39/50], Step [2800/5013], Loss: 0.6761\n",
      "Epoch [39/50], Step [2900/5013], Loss: 0.6590\n",
      "Epoch [39/50], Step [3000/5013], Loss: 0.6905\n",
      "Epoch [39/50], Step [3100/5013], Loss: 0.6742\n",
      "Epoch [39/50], Step [3200/5013], Loss: 0.6996\n",
      "Epoch [39/50], Step [3300/5013], Loss: 0.6749\n",
      "Epoch [39/50], Step [3400/5013], Loss: 0.7010\n",
      "Epoch [39/50], Step [3500/5013], Loss: 0.6802\n",
      "Epoch [39/50], Step [3600/5013], Loss: 0.6880\n",
      "Epoch [39/50], Step [3700/5013], Loss: 0.6766\n",
      "Epoch [39/50], Step [3800/5013], Loss: 0.6861\n",
      "Epoch [39/50], Step [3900/5013], Loss: 0.6754\n",
      "Epoch [39/50], Step [4000/5013], Loss: 0.6704\n",
      "Epoch [39/50], Step [4100/5013], Loss: 0.6828\n",
      "Epoch [39/50], Step [4200/5013], Loss: 0.6834\n",
      "Epoch [39/50], Step [4300/5013], Loss: 0.6891\n",
      "Epoch [39/50], Step [4400/5013], Loss: 0.6773\n",
      "Epoch [39/50], Step [4500/5013], Loss: 0.6690\n",
      "Epoch [39/50], Step [4600/5013], Loss: 0.6939\n",
      "Epoch [39/50], Step [4700/5013], Loss: 0.6857\n",
      "Epoch [39/50], Step [4800/5013], Loss: 0.6930\n",
      "Epoch [39/50], Step [4900/5013], Loss: 0.6664\n",
      "Epoch [39/50], Step [5000/5013], Loss: 0.6707\n",
      "Epoch [39/50] Average Loss: 0.6805\n",
      "Epoch [40/50], Step [100/5013], Loss: 0.6724\n",
      "Epoch [40/50], Step [200/5013], Loss: 0.6765\n",
      "Epoch [40/50], Step [300/5013], Loss: 0.6771\n",
      "Epoch [40/50], Step [400/5013], Loss: 0.6898\n",
      "Epoch [40/50], Step [500/5013], Loss: 0.6898\n",
      "Epoch [40/50], Step [600/5013], Loss: 0.6632\n",
      "Epoch [40/50], Step [700/5013], Loss: 0.6685\n",
      "Epoch [40/50], Step [800/5013], Loss: 0.6780\n",
      "Epoch [40/50], Step [900/5013], Loss: 0.6446\n",
      "Epoch [40/50], Step [1000/5013], Loss: 0.6939\n",
      "Epoch [40/50], Step [1100/5013], Loss: 0.6838\n",
      "Epoch [40/50], Step [1200/5013], Loss: 0.7052\n",
      "Epoch [40/50], Step [1300/5013], Loss: 0.6868\n",
      "Epoch [40/50], Step [1400/5013], Loss: 0.6918\n",
      "Epoch [40/50], Step [1500/5013], Loss: 0.6700\n",
      "Epoch [40/50], Step [1600/5013], Loss: 0.6582\n",
      "Epoch [40/50], Step [1700/5013], Loss: 0.6927\n",
      "Epoch [40/50], Step [1800/5013], Loss: 0.6672\n",
      "Epoch [40/50], Step [1900/5013], Loss: 0.6581\n",
      "Epoch [40/50], Step [2000/5013], Loss: 0.6808\n",
      "Epoch [40/50], Step [2100/5013], Loss: 0.6638\n",
      "Epoch [40/50], Step [2200/5013], Loss: 0.6968\n",
      "Epoch [40/50], Step [2300/5013], Loss: 0.6920\n",
      "Epoch [40/50], Step [2400/5013], Loss: 0.6889\n",
      "Epoch [40/50], Step [2500/5013], Loss: 0.6765\n",
      "Epoch [40/50], Step [2600/5013], Loss: 0.7008\n",
      "Epoch [40/50], Step [2700/5013], Loss: 0.6863\n",
      "Epoch [40/50], Step [2800/5013], Loss: 0.6701\n",
      "Epoch [40/50], Step [2900/5013], Loss: 0.6858\n",
      "Epoch [40/50], Step [3000/5013], Loss: 0.6890\n",
      "Epoch [40/50], Step [3100/5013], Loss: 0.7064\n",
      "Epoch [40/50], Step [3200/5013], Loss: 0.6720\n",
      "Epoch [40/50], Step [3300/5013], Loss: 0.6872\n",
      "Epoch [40/50], Step [3400/5013], Loss: 0.6935\n",
      "Epoch [40/50], Step [3500/5013], Loss: 0.6577\n",
      "Epoch [40/50], Step [3600/5013], Loss: 0.6723\n",
      "Epoch [40/50], Step [3700/5013], Loss: 0.6748\n",
      "Epoch [40/50], Step [3800/5013], Loss: 0.6714\n",
      "Epoch [40/50], Step [3900/5013], Loss: 0.7160\n",
      "Epoch [40/50], Step [4000/5013], Loss: 0.6676\n",
      "Epoch [40/50], Step [4100/5013], Loss: 0.6796\n",
      "Epoch [40/50], Step [4200/5013], Loss: 0.6722\n",
      "Epoch [40/50], Step [4300/5013], Loss: 0.6622\n",
      "Epoch [40/50], Step [4400/5013], Loss: 0.6767\n",
      "Epoch [40/50], Step [4500/5013], Loss: 0.6633\n",
      "Epoch [40/50], Step [4600/5013], Loss: 0.6694\n",
      "Epoch [40/50], Step [4700/5013], Loss: 0.6928\n",
      "Epoch [40/50], Step [4800/5013], Loss: 0.6959\n",
      "Epoch [40/50], Step [4900/5013], Loss: 0.6465\n",
      "Epoch [40/50], Step [5000/5013], Loss: 0.6859\n",
      "Epoch [40/50] Average Loss: 0.6794\n",
      "Epoch [41/50], Step [100/5013], Loss: 0.6644\n",
      "Epoch [41/50], Step [200/5013], Loss: 0.6660\n",
      "Epoch [41/50], Step [300/5013], Loss: 0.6613\n",
      "Epoch [41/50], Step [400/5013], Loss: 0.6860\n",
      "Epoch [41/50], Step [500/5013], Loss: 0.6773\n",
      "Epoch [41/50], Step [600/5013], Loss: 0.7242\n",
      "Epoch [41/50], Step [700/5013], Loss: 0.6950\n",
      "Epoch [41/50], Step [800/5013], Loss: 0.6642\n",
      "Epoch [41/50], Step [900/5013], Loss: 0.6580\n",
      "Epoch [41/50], Step [1000/5013], Loss: 0.6756\n",
      "Epoch [41/50], Step [1100/5013], Loss: 0.6773\n",
      "Epoch [41/50], Step [1200/5013], Loss: 0.6849\n",
      "Epoch [41/50], Step [1300/5013], Loss: 0.6671\n",
      "Epoch [41/50], Step [1400/5013], Loss: 0.6694\n",
      "Epoch [41/50], Step [1500/5013], Loss: 0.6704\n",
      "Epoch [41/50], Step [1600/5013], Loss: 0.6622\n",
      "Epoch [41/50], Step [1700/5013], Loss: 0.6740\n",
      "Epoch [41/50], Step [1800/5013], Loss: 0.6507\n",
      "Epoch [41/50], Step [1900/5013], Loss: 0.7449\n",
      "Epoch [41/50], Step [2000/5013], Loss: 0.6806\n",
      "Epoch [41/50], Step [2100/5013], Loss: 0.7025\n",
      "Epoch [41/50], Step [2200/5013], Loss: 0.6738\n",
      "Epoch [41/50], Step [2300/5013], Loss: 0.6782\n",
      "Epoch [41/50], Step [2400/5013], Loss: 0.6717\n",
      "Epoch [41/50], Step [2500/5013], Loss: 0.6688\n",
      "Epoch [41/50], Step [2600/5013], Loss: 0.6731\n",
      "Epoch [41/50], Step [2700/5013], Loss: 0.6660\n",
      "Epoch [41/50], Step [2800/5013], Loss: 0.6891\n",
      "Epoch [41/50], Step [2900/5013], Loss: 0.6932\n",
      "Epoch [41/50], Step [3000/5013], Loss: 0.6742\n",
      "Epoch [41/50], Step [3100/5013], Loss: 0.6818\n",
      "Epoch [41/50], Step [3200/5013], Loss: 0.6612\n",
      "Epoch [41/50], Step [3300/5013], Loss: 0.6760\n",
      "Epoch [41/50], Step [3400/5013], Loss: 0.6758\n",
      "Epoch [41/50], Step [3500/5013], Loss: 0.6853\n",
      "Epoch [41/50], Step [3600/5013], Loss: 0.6674\n",
      "Epoch [41/50], Step [3700/5013], Loss: 0.6913\n",
      "Epoch [41/50], Step [3800/5013], Loss: 0.6928\n",
      "Epoch [41/50], Step [3900/5013], Loss: 0.6639\n",
      "Epoch [41/50], Step [4000/5013], Loss: 0.7717\n",
      "Epoch [41/50], Step [4100/5013], Loss: 0.6862\n",
      "Epoch [41/50], Step [4200/5013], Loss: 0.6787\n",
      "Epoch [41/50], Step [4300/5013], Loss: 0.6765\n",
      "Epoch [41/50], Step [4400/5013], Loss: 0.6926\n",
      "Epoch [41/50], Step [4500/5013], Loss: 0.6804\n",
      "Epoch [41/50], Step [4600/5013], Loss: 0.6761\n",
      "Epoch [41/50], Step [4700/5013], Loss: 0.6970\n",
      "Epoch [41/50], Step [4800/5013], Loss: 0.6953\n",
      "Epoch [41/50], Step [4900/5013], Loss: 0.6807\n",
      "Epoch [41/50], Step [5000/5013], Loss: 0.6560\n",
      "Epoch [41/50] Average Loss: 0.6807\n",
      "Epoch [42/50], Step [100/5013], Loss: 0.6803\n",
      "Epoch [42/50], Step [200/5013], Loss: 0.6625\n",
      "Epoch [42/50], Step [300/5013], Loss: 0.6905\n",
      "Epoch [42/50], Step [400/5013], Loss: 0.6541\n",
      "Epoch [42/50], Step [500/5013], Loss: 0.6535\n",
      "Epoch [42/50], Step [600/5013], Loss: 0.6739\n",
      "Epoch [42/50], Step [700/5013], Loss: 0.6795\n",
      "Epoch [42/50], Step [800/5013], Loss: 0.6773\n",
      "Epoch [42/50], Step [900/5013], Loss: 0.6828\n",
      "Epoch [42/50], Step [1000/5013], Loss: 0.6850\n",
      "Epoch [42/50], Step [1100/5013], Loss: 0.6782\n",
      "Epoch [42/50], Step [1200/5013], Loss: 0.6878\n",
      "Epoch [42/50], Step [1300/5013], Loss: 0.6750\n",
      "Epoch [42/50], Step [1400/5013], Loss: 0.7126\n",
      "Epoch [42/50], Step [1500/5013], Loss: 0.6632\n",
      "Epoch [42/50], Step [1600/5013], Loss: 0.6727\n",
      "Epoch [42/50], Step [1700/5013], Loss: 0.6782\n",
      "Epoch [42/50], Step [1800/5013], Loss: 0.7007\n",
      "Epoch [42/50], Step [1900/5013], Loss: 0.6864\n",
      "Epoch [42/50], Step [2000/5013], Loss: 0.6531\n",
      "Epoch [42/50], Step [2100/5013], Loss: 0.6855\n",
      "Epoch [42/50], Step [2200/5013], Loss: 0.6762\n",
      "Epoch [42/50], Step [2300/5013], Loss: 0.6643\n",
      "Epoch [42/50], Step [2400/5013], Loss: 0.6891\n",
      "Epoch [42/50], Step [2500/5013], Loss: 0.6801\n",
      "Epoch [42/50], Step [2600/5013], Loss: 0.6620\n",
      "Epoch [42/50], Step [2700/5013], Loss: 0.6845\n",
      "Epoch [42/50], Step [2800/5013], Loss: 0.6731\n",
      "Epoch [42/50], Step [2900/5013], Loss: 0.6793\n",
      "Epoch [42/50], Step [3000/5013], Loss: 0.6499\n",
      "Epoch [42/50], Step [3100/5013], Loss: 0.6392\n",
      "Epoch [42/50], Step [3200/5013], Loss: 0.6627\n",
      "Epoch [42/50], Step [3300/5013], Loss: 0.6673\n",
      "Epoch [42/50], Step [3400/5013], Loss: 0.6468\n",
      "Epoch [42/50], Step [3500/5013], Loss: 0.6554\n",
      "Epoch [42/50], Step [3600/5013], Loss: 0.6761\n",
      "Epoch [42/50], Step [3700/5013], Loss: 0.6880\n",
      "Epoch [42/50], Step [3800/5013], Loss: 0.6820\n",
      "Epoch [42/50], Step [3900/5013], Loss: 0.6925\n",
      "Epoch [42/50], Step [4000/5013], Loss: 0.6783\n",
      "Epoch [42/50], Step [4100/5013], Loss: 0.6719\n",
      "Epoch [42/50], Step [4200/5013], Loss: 0.6885\n",
      "Epoch [42/50], Step [4300/5013], Loss: 0.6601\n",
      "Epoch [42/50], Step [4400/5013], Loss: 0.6992\n",
      "Epoch [42/50], Step [4500/5013], Loss: 0.6682\n",
      "Epoch [42/50], Step [4600/5013], Loss: 0.6721\n",
      "Epoch [42/50], Step [4700/5013], Loss: 0.6869\n",
      "Epoch [42/50], Step [4800/5013], Loss: 0.6756\n",
      "Epoch [42/50], Step [4900/5013], Loss: 0.6889\n",
      "Epoch [42/50], Step [5000/5013], Loss: 0.6934\n",
      "Epoch [42/50] Average Loss: 0.6757\n",
      "Epoch [43/50], Step [100/5013], Loss: 0.6774\n",
      "Epoch [43/50], Step [200/5013], Loss: 0.6749\n",
      "Epoch [43/50], Step [300/5013], Loss: 0.6780\n",
      "Epoch [43/50], Step [400/5013], Loss: 0.6637\n",
      "Epoch [43/50], Step [500/5013], Loss: 0.6797\n",
      "Epoch [43/50], Step [600/5013], Loss: 0.6846\n",
      "Epoch [43/50], Step [700/5013], Loss: 0.6869\n",
      "Epoch [43/50], Step [800/5013], Loss: 0.6722\n",
      "Epoch [43/50], Step [900/5013], Loss: 0.6801\n",
      "Epoch [43/50], Step [1000/5013], Loss: 0.6676\n",
      "Epoch [43/50], Step [1100/5013], Loss: 0.6510\n",
      "Epoch [43/50], Step [1200/5013], Loss: 0.6757\n",
      "Epoch [43/50], Step [1300/5013], Loss: 0.6536\n",
      "Epoch [43/50], Step [1400/5013], Loss: 0.6514\n",
      "Epoch [43/50], Step [1500/5013], Loss: 0.6736\n",
      "Epoch [43/50], Step [1600/5013], Loss: 0.6601\n",
      "Epoch [43/50], Step [1700/5013], Loss: 0.6788\n",
      "Epoch [43/50], Step [1800/5013], Loss: 0.6864\n",
      "Epoch [43/50], Step [1900/5013], Loss: 0.6752\n",
      "Epoch [43/50], Step [2000/5013], Loss: 0.6724\n",
      "Epoch [43/50], Step [2100/5013], Loss: 0.6767\n",
      "Epoch [43/50], Step [2200/5013], Loss: 0.6532\n",
      "Epoch [43/50], Step [2300/5013], Loss: 0.6850\n",
      "Epoch [43/50], Step [2400/5013], Loss: 0.6848\n",
      "Epoch [43/50], Step [2500/5013], Loss: 0.6775\n",
      "Epoch [43/50], Step [2600/5013], Loss: 0.6674\n",
      "Epoch [43/50], Step [2700/5013], Loss: 0.6779\n",
      "Epoch [43/50], Step [2800/5013], Loss: 0.6599\n",
      "Epoch [43/50], Step [2900/5013], Loss: 0.6909\n",
      "Epoch [43/50], Step [3000/5013], Loss: 0.6967\n",
      "Epoch [43/50], Step [3100/5013], Loss: 0.6643\n",
      "Epoch [43/50], Step [3200/5013], Loss: 0.6930\n",
      "Epoch [43/50], Step [3300/5013], Loss: 0.6465\n",
      "Epoch [43/50], Step [3400/5013], Loss: 0.6566\n",
      "Epoch [43/50], Step [3500/5013], Loss: 0.6932\n",
      "Epoch [43/50], Step [3600/5013], Loss: 0.6618\n",
      "Epoch [43/50], Step [3700/5013], Loss: 0.6638\n",
      "Epoch [43/50], Step [3800/5013], Loss: 0.6772\n",
      "Epoch [43/50], Step [3900/5013], Loss: 0.6711\n",
      "Epoch [43/50], Step [4000/5013], Loss: 0.6641\n",
      "Epoch [43/50], Step [4100/5013], Loss: 0.7665\n",
      "Epoch [43/50], Step [4200/5013], Loss: 0.6566\n",
      "Epoch [43/50], Step [4300/5013], Loss: 0.6771\n",
      "Epoch [43/50], Step [4400/5013], Loss: 0.6745\n",
      "Epoch [43/50], Step [4500/5013], Loss: 0.6677\n",
      "Epoch [43/50], Step [4600/5013], Loss: 0.6706\n",
      "Epoch [43/50], Step [4700/5013], Loss: 0.7066\n",
      "Epoch [43/50], Step [4800/5013], Loss: 0.6634\n",
      "Epoch [43/50], Step [4900/5013], Loss: 0.6536\n",
      "Epoch [43/50], Step [5000/5013], Loss: 0.6639\n",
      "Epoch [43/50] Average Loss: 0.6741\n",
      "Epoch [44/50], Step [100/5013], Loss: 0.6822\n",
      "Epoch [44/50], Step [200/5013], Loss: 0.6712\n",
      "Epoch [44/50], Step [300/5013], Loss: 0.6824\n",
      "Epoch [44/50], Step [400/5013], Loss: 0.6393\n",
      "Epoch [44/50], Step [500/5013], Loss: 0.6692\n",
      "Epoch [44/50], Step [600/5013], Loss: 0.6718\n",
      "Epoch [44/50], Step [700/5013], Loss: 0.6662\n",
      "Epoch [44/50], Step [800/5013], Loss: 0.6657\n",
      "Epoch [44/50], Step [900/5013], Loss: 0.6670\n",
      "Epoch [44/50], Step [1000/5013], Loss: 0.6614\n",
      "Epoch [44/50], Step [1100/5013], Loss: 0.6423\n",
      "Epoch [44/50], Step [1200/5013], Loss: 0.6657\n",
      "Epoch [44/50], Step [1300/5013], Loss: 0.6764\n",
      "Epoch [44/50], Step [1400/5013], Loss: 0.6557\n",
      "Epoch [44/50], Step [1500/5013], Loss: 0.6703\n",
      "Epoch [44/50], Step [1600/5013], Loss: 0.6798\n",
      "Epoch [44/50], Step [1700/5013], Loss: 0.6728\n",
      "Epoch [44/50], Step [1800/5013], Loss: 0.6881\n",
      "Epoch [44/50], Step [1900/5013], Loss: 0.6855\n",
      "Epoch [44/50], Step [2000/5013], Loss: 0.6789\n",
      "Epoch [44/50], Step [2100/5013], Loss: 0.6649\n",
      "Epoch [44/50], Step [2200/5013], Loss: 0.6780\n",
      "Epoch [44/50], Step [2300/5013], Loss: 0.6871\n",
      "Epoch [44/50], Step [2400/5013], Loss: 0.6582\n",
      "Epoch [44/50], Step [2500/5013], Loss: 0.6656\n",
      "Epoch [44/50], Step [2600/5013], Loss: 0.6657\n",
      "Epoch [44/50], Step [2700/5013], Loss: 0.6899\n",
      "Epoch [44/50], Step [2800/5013], Loss: 0.6703\n",
      "Epoch [44/50], Step [2900/5013], Loss: 0.6583\n",
      "Epoch [44/50], Step [3000/5013], Loss: 0.6634\n",
      "Epoch [44/50], Step [3100/5013], Loss: 0.6670\n",
      "Epoch [44/50], Step [3200/5013], Loss: 0.6908\n",
      "Epoch [44/50], Step [3300/5013], Loss: 0.6760\n",
      "Epoch [44/50], Step [3400/5013], Loss: 0.6570\n",
      "Epoch [44/50], Step [3500/5013], Loss: 0.6786\n",
      "Epoch [44/50], Step [3600/5013], Loss: 0.6910\n",
      "Epoch [44/50], Step [3700/5013], Loss: 0.6711\n",
      "Epoch [44/50], Step [3800/5013], Loss: 0.6543\n",
      "Epoch [44/50], Step [3900/5013], Loss: 0.6631\n",
      "Epoch [44/50], Step [4000/5013], Loss: 0.6843\n",
      "Epoch [44/50], Step [4100/5013], Loss: 0.6803\n",
      "Epoch [44/50], Step [4200/5013], Loss: 0.6627\n",
      "Epoch [44/50], Step [4300/5013], Loss: 0.6819\n",
      "Epoch [44/50], Step [4400/5013], Loss: 0.6677\n",
      "Epoch [44/50], Step [4500/5013], Loss: 0.6519\n",
      "Epoch [44/50], Step [4600/5013], Loss: 0.6670\n",
      "Epoch [44/50], Step [4700/5013], Loss: 0.6789\n",
      "Epoch [44/50], Step [4800/5013], Loss: 0.6614\n",
      "Epoch [44/50], Step [4900/5013], Loss: 0.6801\n",
      "Epoch [44/50], Step [5000/5013], Loss: 0.6948\n",
      "Epoch [44/50] Average Loss: 0.6708\n",
      "Epoch [45/50], Step [100/5013], Loss: 0.6437\n",
      "Epoch [45/50], Step [200/5013], Loss: 0.6800\n",
      "Epoch [45/50], Step [300/5013], Loss: 0.6792\n",
      "Epoch [45/50], Step [400/5013], Loss: 0.6737\n",
      "Epoch [45/50], Step [500/5013], Loss: 0.6460\n",
      "Epoch [45/50], Step [600/5013], Loss: 0.6653\n",
      "Epoch [45/50], Step [700/5013], Loss: 0.6563\n",
      "Epoch [45/50], Step [800/5013], Loss: 0.6677\n",
      "Epoch [45/50], Step [900/5013], Loss: 0.6682\n",
      "Epoch [45/50], Step [1000/5013], Loss: 0.6805\n",
      "Epoch [45/50], Step [1100/5013], Loss: 0.6745\n",
      "Epoch [45/50], Step [1200/5013], Loss: 0.6603\n",
      "Epoch [45/50], Step [1300/5013], Loss: 0.6782\n",
      "Epoch [45/50], Step [1400/5013], Loss: 0.6710\n",
      "Epoch [45/50], Step [1500/5013], Loss: 0.6593\n",
      "Epoch [45/50], Step [1600/5013], Loss: 0.6832\n",
      "Epoch [45/50], Step [1700/5013], Loss: 0.6599\n",
      "Epoch [45/50], Step [1800/5013], Loss: 0.6720\n",
      "Epoch [45/50], Step [1900/5013], Loss: 0.6657\n",
      "Epoch [45/50], Step [2000/5013], Loss: 0.6786\n",
      "Epoch [45/50], Step [2100/5013], Loss: 0.6775\n",
      "Epoch [45/50], Step [2200/5013], Loss: 0.6677\n",
      "Epoch [45/50], Step [2300/5013], Loss: 0.6669\n",
      "Epoch [45/50], Step [2400/5013], Loss: 0.6683\n",
      "Epoch [45/50], Step [2500/5013], Loss: 0.6834\n",
      "Epoch [45/50], Step [2600/5013], Loss: 0.6735\n",
      "Epoch [45/50], Step [2700/5013], Loss: 0.6831\n",
      "Epoch [45/50], Step [2800/5013], Loss: 0.6655\n",
      "Epoch [45/50], Step [2900/5013], Loss: 0.6553\n",
      "Epoch [45/50], Step [3000/5013], Loss: 0.6844\n",
      "Epoch [45/50], Step [3100/5013], Loss: 0.6694\n",
      "Epoch [45/50], Step [3200/5013], Loss: 0.6428\n",
      "Epoch [45/50], Step [3300/5013], Loss: 0.6832\n",
      "Epoch [45/50], Step [3400/5013], Loss: 0.6784\n",
      "Epoch [45/50], Step [3500/5013], Loss: 0.6865\n",
      "Epoch [45/50], Step [3600/5013], Loss: 0.6776\n",
      "Epoch [45/50], Step [3700/5013], Loss: 0.6674\n",
      "Epoch [45/50], Step [3800/5013], Loss: 0.6747\n",
      "Epoch [45/50], Step [3900/5013], Loss: 0.6879\n",
      "Epoch [45/50], Step [4000/5013], Loss: 0.6571\n",
      "Epoch [45/50], Step [4100/5013], Loss: 0.6658\n",
      "Epoch [45/50], Step [4200/5013], Loss: 0.6722\n",
      "Epoch [45/50], Step [4300/5013], Loss: 0.6603\n",
      "Epoch [45/50], Step [4400/5013], Loss: 0.6754\n",
      "Epoch [45/50], Step [4500/5013], Loss: 0.6751\n",
      "Epoch [45/50], Step [4600/5013], Loss: 0.6588\n",
      "Epoch [45/50], Step [4700/5013], Loss: 0.6600\n",
      "Epoch [45/50], Step [4800/5013], Loss: 0.6786\n",
      "Epoch [45/50], Step [4900/5013], Loss: 0.6924\n",
      "Epoch [45/50], Step [5000/5013], Loss: 0.6577\n",
      "Epoch [45/50] Average Loss: 0.6702\n",
      "Epoch [46/50], Step [100/5013], Loss: 0.6676\n",
      "Epoch [46/50], Step [200/5013], Loss: 0.6597\n",
      "Epoch [46/50], Step [300/5013], Loss: 0.6628\n",
      "Epoch [46/50], Step [400/5013], Loss: 0.6758\n",
      "Epoch [46/50], Step [500/5013], Loss: 0.6419\n",
      "Epoch [46/50], Step [600/5013], Loss: 0.6603\n",
      "Epoch [46/50], Step [700/5013], Loss: 0.6600\n",
      "Epoch [46/50], Step [800/5013], Loss: 0.6723\n",
      "Epoch [46/50], Step [900/5013], Loss: 0.6813\n",
      "Epoch [46/50], Step [1000/5013], Loss: 0.6737\n",
      "Epoch [46/50], Step [1100/5013], Loss: 0.6886\n",
      "Epoch [46/50], Step [1200/5013], Loss: 0.6536\n",
      "Epoch [46/50], Step [1300/5013], Loss: 0.6805\n",
      "Epoch [46/50], Step [1400/5013], Loss: 0.6576\n",
      "Epoch [46/50], Step [1500/5013], Loss: 0.6834\n",
      "Epoch [46/50], Step [1600/5013], Loss: 0.6544\n",
      "Epoch [46/50], Step [1700/5013], Loss: 0.6647\n",
      "Epoch [46/50], Step [1800/5013], Loss: 0.6854\n",
      "Epoch [46/50], Step [1900/5013], Loss: 0.6531\n",
      "Epoch [46/50], Step [2000/5013], Loss: 0.6567\n",
      "Epoch [46/50], Step [2100/5013], Loss: 0.6470\n",
      "Epoch [46/50], Step [2200/5013], Loss: 0.6669\n",
      "Epoch [46/50], Step [2300/5013], Loss: 0.6441\n",
      "Epoch [46/50], Step [2400/5013], Loss: 0.6512\n",
      "Epoch [46/50], Step [2500/5013], Loss: 0.6694\n",
      "Epoch [46/50], Step [2600/5013], Loss: 0.6777\n",
      "Epoch [46/50], Step [2700/5013], Loss: 0.6702\n",
      "Epoch [46/50], Step [2800/5013], Loss: 0.6544\n",
      "Epoch [46/50], Step [2900/5013], Loss: 0.6788\n",
      "Epoch [46/50], Step [3000/5013], Loss: 0.6800\n",
      "Epoch [46/50], Step [3100/5013], Loss: 0.6793\n",
      "Epoch [46/50], Step [3200/5013], Loss: 0.6931\n",
      "Epoch [46/50], Step [3300/5013], Loss: 0.6653\n",
      "Epoch [46/50], Step [3400/5013], Loss: 0.7007\n",
      "Epoch [46/50], Step [3500/5013], Loss: 0.6669\n",
      "Epoch [46/50], Step [3600/5013], Loss: 0.6343\n",
      "Epoch [46/50], Step [3700/5013], Loss: 0.6760\n",
      "Epoch [46/50], Step [3800/5013], Loss: 0.6818\n",
      "Epoch [46/50], Step [3900/5013], Loss: 0.6762\n",
      "Epoch [46/50], Step [4000/5013], Loss: 0.6813\n",
      "Epoch [46/50], Step [4100/5013], Loss: 0.6720\n",
      "Epoch [46/50], Step [4200/5013], Loss: 0.6763\n",
      "Epoch [46/50], Step [4300/5013], Loss: 0.6963\n",
      "Epoch [46/50], Step [4400/5013], Loss: 0.6882\n",
      "Epoch [46/50], Step [4500/5013], Loss: 0.7022\n",
      "Epoch [46/50], Step [4600/5013], Loss: 0.6581\n",
      "Epoch [46/50], Step [4700/5013], Loss: 0.6637\n",
      "Epoch [46/50], Step [4800/5013], Loss: 0.6901\n",
      "Epoch [46/50], Step [4900/5013], Loss: 0.7086\n",
      "Epoch [46/50], Step [5000/5013], Loss: 0.6526\n",
      "Epoch [46/50] Average Loss: 0.6708\n",
      "Epoch [47/50], Step [100/5013], Loss: 0.6672\n",
      "Epoch [47/50], Step [200/5013], Loss: 0.6908\n",
      "Epoch [47/50], Step [300/5013], Loss: 0.6593\n",
      "Epoch [47/50], Step [400/5013], Loss: 0.6865\n",
      "Epoch [47/50], Step [500/5013], Loss: 0.6510\n",
      "Epoch [47/50], Step [600/5013], Loss: 0.6730\n",
      "Epoch [47/50], Step [700/5013], Loss: 0.6544\n",
      "Epoch [47/50], Step [800/5013], Loss: 0.6792\n",
      "Epoch [47/50], Step [900/5013], Loss: 0.6711\n",
      "Epoch [47/50], Step [1000/5013], Loss: 0.6836\n",
      "Epoch [47/50], Step [1100/5013], Loss: 0.6582\n",
      "Epoch [47/50], Step [1200/5013], Loss: 0.6899\n",
      "Epoch [47/50], Step [1300/5013], Loss: 0.6678\n",
      "Epoch [47/50], Step [1400/5013], Loss: 0.6806\n",
      "Epoch [47/50], Step [1500/5013], Loss: 0.6752\n",
      "Epoch [47/50], Step [1600/5013], Loss: 0.6852\n",
      "Epoch [47/50], Step [1700/5013], Loss: 0.6495\n",
      "Epoch [47/50], Step [1800/5013], Loss: 0.6704\n",
      "Epoch [47/50], Step [1900/5013], Loss: 0.6882\n",
      "Epoch [47/50], Step [2000/5013], Loss: 0.7032\n",
      "Epoch [47/50], Step [2100/5013], Loss: 0.6669\n",
      "Epoch [47/50], Step [2200/5013], Loss: 0.6724\n",
      "Epoch [47/50], Step [2300/5013], Loss: 0.6569\n",
      "Epoch [47/50], Step [2400/5013], Loss: 0.6521\n",
      "Epoch [47/50], Step [2500/5013], Loss: 0.6559\n",
      "Epoch [47/50], Step [2600/5013], Loss: 0.6746\n",
      "Epoch [47/50], Step [2700/5013], Loss: 0.6503\n",
      "Epoch [47/50], Step [2800/5013], Loss: 0.6850\n",
      "Epoch [47/50], Step [2900/5013], Loss: 0.6584\n",
      "Epoch [47/50], Step [3000/5013], Loss: 0.6687\n",
      "Epoch [47/50], Step [3100/5013], Loss: 0.7106\n",
      "Epoch [47/50], Step [3200/5013], Loss: 0.6690\n",
      "Epoch [47/50], Step [3300/5013], Loss: 0.6554\n",
      "Epoch [47/50], Step [3400/5013], Loss: 0.6760\n",
      "Epoch [47/50], Step [3500/5013], Loss: 0.6443\n",
      "Epoch [47/50], Step [3600/5013], Loss: 0.6702\n",
      "Epoch [47/50], Step [3700/5013], Loss: 0.6747\n",
      "Epoch [47/50], Step [3800/5013], Loss: 0.6799\n",
      "Epoch [47/50], Step [3900/5013], Loss: 0.6822\n",
      "Epoch [47/50], Step [4000/5013], Loss: 0.6602\n",
      "Epoch [47/50], Step [4100/5013], Loss: 0.6741\n",
      "Epoch [47/50], Step [4200/5013], Loss: 0.6588\n",
      "Epoch [47/50], Step [4300/5013], Loss: 0.6830\n",
      "Epoch [47/50], Step [4400/5013], Loss: 0.6777\n",
      "Epoch [47/50], Step [4500/5013], Loss: 0.6768\n",
      "Epoch [47/50], Step [4600/5013], Loss: 0.6766\n",
      "Epoch [47/50], Step [4700/5013], Loss: 0.6605\n",
      "Epoch [47/50], Step [4800/5013], Loss: 0.6585\n",
      "Epoch [47/50], Step [4900/5013], Loss: 0.6704\n",
      "Epoch [47/50], Step [5000/5013], Loss: 0.7038\n",
      "Epoch [47/50] Average Loss: 0.6717\n",
      "Epoch [48/50], Step [100/5013], Loss: 0.6551\n",
      "Epoch [48/50], Step [200/5013], Loss: 0.6623\n",
      "Epoch [48/50], Step [300/5013], Loss: 0.6426\n",
      "Epoch [48/50], Step [400/5013], Loss: 0.6642\n",
      "Epoch [48/50], Step [500/5013], Loss: 0.6723\n",
      "Epoch [48/50], Step [600/5013], Loss: 0.6695\n",
      "Epoch [48/50], Step [700/5013], Loss: 0.6778\n",
      "Epoch [48/50], Step [800/5013], Loss: 0.6690\n",
      "Epoch [48/50], Step [900/5013], Loss: 0.6538\n",
      "Epoch [48/50], Step [1000/5013], Loss: 0.6733\n",
      "Epoch [48/50], Step [1100/5013], Loss: 0.6589\n",
      "Epoch [48/50], Step [1200/5013], Loss: 0.6623\n",
      "Epoch [48/50], Step [1300/5013], Loss: 0.6641\n",
      "Epoch [48/50], Step [1400/5013], Loss: 0.6652\n",
      "Epoch [48/50], Step [1500/5013], Loss: 0.6777\n",
      "Epoch [48/50], Step [1600/5013], Loss: 0.6617\n",
      "Epoch [48/50], Step [1700/5013], Loss: 0.6608\n",
      "Epoch [48/50], Step [1800/5013], Loss: 0.6518\n",
      "Epoch [48/50], Step [1900/5013], Loss: 0.6698\n",
      "Epoch [48/50], Step [2000/5013], Loss: 0.6663\n",
      "Epoch [48/50], Step [2100/5013], Loss: 0.6912\n",
      "Epoch [48/50], Step [2200/5013], Loss: 0.6674\n",
      "Epoch [48/50], Step [2300/5013], Loss: 0.6737\n",
      "Epoch [48/50], Step [2400/5013], Loss: 0.6672\n",
      "Epoch [48/50], Step [2500/5013], Loss: 0.6504\n",
      "Epoch [48/50], Step [2600/5013], Loss: 0.6764\n",
      "Epoch [48/50], Step [2700/5013], Loss: 0.6832\n",
      "Epoch [48/50], Step [2800/5013], Loss: 0.6496\n",
      "Epoch [48/50], Step [2900/5013], Loss: 0.6651\n",
      "Epoch [48/50], Step [3000/5013], Loss: 0.6706\n",
      "Epoch [48/50], Step [3100/5013], Loss: 0.6582\n",
      "Epoch [48/50], Step [3200/5013], Loss: 0.6819\n",
      "Epoch [48/50], Step [3300/5013], Loss: 0.6751\n",
      "Epoch [48/50], Step [3400/5013], Loss: 0.6650\n",
      "Epoch [48/50], Step [3500/5013], Loss: 0.6804\n",
      "Epoch [48/50], Step [3600/5013], Loss: 0.6587\n",
      "Epoch [48/50], Step [3700/5013], Loss: 0.6770\n",
      "Epoch [48/50], Step [3800/5013], Loss: 0.6906\n",
      "Epoch [48/50], Step [3900/5013], Loss: 0.6688\n",
      "Epoch [48/50], Step [4000/5013], Loss: 0.6558\n",
      "Epoch [48/50], Step [4100/5013], Loss: 0.6514\n",
      "Epoch [48/50], Step [4200/5013], Loss: 0.6728\n",
      "Epoch [48/50], Step [4300/5013], Loss: 0.6806\n",
      "Epoch [48/50], Step [4400/5013], Loss: 0.6658\n",
      "Epoch [48/50], Step [4500/5013], Loss: 0.6714\n",
      "Epoch [48/50], Step [4600/5013], Loss: 0.6582\n",
      "Epoch [48/50], Step [4700/5013], Loss: 0.6615\n",
      "Epoch [48/50], Step [4800/5013], Loss: 0.6566\n",
      "Epoch [48/50], Step [4900/5013], Loss: 0.6839\n",
      "Epoch [48/50], Step [5000/5013], Loss: 0.6692\n",
      "Epoch [48/50] Average Loss: 0.6672\n",
      "Epoch [49/50], Step [100/5013], Loss: 0.6665\n",
      "Epoch [49/50], Step [200/5013], Loss: 0.6606\n",
      "Epoch [49/50], Step [300/5013], Loss: 0.6753\n",
      "Epoch [49/50], Step [400/5013], Loss: 0.6789\n",
      "Epoch [49/50], Step [500/5013], Loss: 0.6527\n",
      "Epoch [49/50], Step [600/5013], Loss: 0.6590\n",
      "Epoch [49/50], Step [700/5013], Loss: 0.6656\n",
      "Epoch [49/50], Step [800/5013], Loss: 0.6473\n",
      "Epoch [49/50], Step [900/5013], Loss: 0.6468\n",
      "Epoch [49/50], Step [1000/5013], Loss: 0.6466\n",
      "Epoch [49/50], Step [1100/5013], Loss: 0.6630\n",
      "Epoch [49/50], Step [1200/5013], Loss: 0.6884\n",
      "Epoch [49/50], Step [1300/5013], Loss: 0.6647\n",
      "Epoch [49/50], Step [1400/5013], Loss: 0.6922\n",
      "Epoch [49/50], Step [1500/5013], Loss: 0.6682\n",
      "Epoch [49/50], Step [1600/5013], Loss: 0.6693\n",
      "Epoch [49/50], Step [1700/5013], Loss: 0.6547\n",
      "Epoch [49/50], Step [1800/5013], Loss: 0.6588\n",
      "Epoch [49/50], Step [1900/5013], Loss: 0.6600\n",
      "Epoch [49/50], Step [2000/5013], Loss: 0.6487\n",
      "Epoch [49/50], Step [2100/5013], Loss: 0.6624\n",
      "Epoch [49/50], Step [2200/5013], Loss: 0.6820\n",
      "Epoch [49/50], Step [2300/5013], Loss: 0.6733\n",
      "Epoch [49/50], Step [2400/5013], Loss: 0.6720\n",
      "Epoch [49/50], Step [2500/5013], Loss: 0.6724\n",
      "Epoch [49/50], Step [2600/5013], Loss: 0.6579\n",
      "Epoch [49/50], Step [2700/5013], Loss: 0.6778\n",
      "Epoch [49/50], Step [2800/5013], Loss: 0.6468\n",
      "Epoch [49/50], Step [2900/5013], Loss: 0.6723\n",
      "Epoch [49/50], Step [3000/5013], Loss: 0.6632\n",
      "Epoch [49/50], Step [3100/5013], Loss: 0.6734\n",
      "Epoch [49/50], Step [3200/5013], Loss: 0.6735\n",
      "Epoch [49/50], Step [3300/5013], Loss: 0.6976\n",
      "Epoch [49/50], Step [3400/5013], Loss: 0.6823\n",
      "Epoch [49/50], Step [3500/5013], Loss: 0.6981\n",
      "Epoch [49/50], Step [3600/5013], Loss: 0.6971\n",
      "Epoch [49/50], Step [3700/5013], Loss: 0.6617\n",
      "Epoch [49/50], Step [3800/5013], Loss: 0.6776\n",
      "Epoch [49/50], Step [3900/5013], Loss: 0.6817\n",
      "Epoch [49/50], Step [4000/5013], Loss: 0.6729\n",
      "Epoch [49/50], Step [4100/5013], Loss: 0.6876\n",
      "Epoch [49/50], Step [4200/5013], Loss: 0.6525\n",
      "Epoch [49/50], Step [4300/5013], Loss: 0.6524\n",
      "Epoch [49/50], Step [4400/5013], Loss: 0.6657\n",
      "Epoch [49/50], Step [4500/5013], Loss: 0.6620\n",
      "Epoch [49/50], Step [4600/5013], Loss: 0.6645\n",
      "Epoch [49/50], Step [4700/5013], Loss: 0.6694\n",
      "Epoch [49/50], Step [4800/5013], Loss: 0.6872\n",
      "Epoch [49/50], Step [4900/5013], Loss: 0.6424\n",
      "Epoch [49/50], Step [5000/5013], Loss: 0.6632\n",
      "Epoch [49/50] Average Loss: 0.6684\n",
      "Epoch [50/50], Step [100/5013], Loss: 0.6717\n",
      "Epoch [50/50], Step [200/5013], Loss: 0.6816\n",
      "Epoch [50/50], Step [300/5013], Loss: 0.6724\n",
      "Epoch [50/50], Step [400/5013], Loss: 0.6502\n",
      "Epoch [50/50], Step [500/5013], Loss: 0.6655\n",
      "Epoch [50/50], Step [600/5013], Loss: 0.6558\n",
      "Epoch [50/50], Step [700/5013], Loss: 0.6461\n",
      "Epoch [50/50], Step [800/5013], Loss: 0.6694\n",
      "Epoch [50/50], Step [900/5013], Loss: 0.6455\n",
      "Epoch [50/50], Step [1000/5013], Loss: 0.6655\n",
      "Epoch [50/50], Step [1100/5013], Loss: 0.6700\n",
      "Epoch [50/50], Step [1200/5013], Loss: 0.6606\n",
      "Epoch [50/50], Step [1300/5013], Loss: 0.6309\n",
      "Epoch [50/50], Step [1400/5013], Loss: 0.6544\n",
      "Epoch [50/50], Step [1500/5013], Loss: 0.6515\n",
      "Epoch [50/50], Step [1600/5013], Loss: 0.6659\n",
      "Epoch [50/50], Step [1700/5013], Loss: 0.6391\n",
      "Epoch [50/50], Step [1800/5013], Loss: 0.6650\n",
      "Epoch [50/50], Step [1900/5013], Loss: 0.6551\n",
      "Epoch [50/50], Step [2000/5013], Loss: 0.6818\n",
      "Epoch [50/50], Step [2100/5013], Loss: 0.6507\n",
      "Epoch [50/50], Step [2200/5013], Loss: 0.6509\n",
      "Epoch [50/50], Step [2300/5013], Loss: 0.6569\n",
      "Epoch [50/50], Step [2400/5013], Loss: 0.6810\n",
      "Epoch [50/50], Step [2500/5013], Loss: 0.6578\n",
      "Epoch [50/50], Step [2600/5013], Loss: 0.6320\n",
      "Epoch [50/50], Step [2700/5013], Loss: 0.7024\n",
      "Epoch [50/50], Step [2800/5013], Loss: 0.6619\n",
      "Epoch [50/50], Step [2900/5013], Loss: 0.6739\n",
      "Epoch [50/50], Step [3000/5013], Loss: 0.6723\n",
      "Epoch [50/50], Step [3100/5013], Loss: 0.6551\n",
      "Epoch [50/50], Step [3200/5013], Loss: 0.6799\n",
      "Epoch [50/50], Step [3300/5013], Loss: 0.6748\n",
      "Epoch [50/50], Step [3400/5013], Loss: 0.6801\n",
      "Epoch [50/50], Step [3500/5013], Loss: 0.6811\n",
      "Epoch [50/50], Step [3600/5013], Loss: 0.6973\n",
      "Epoch [50/50], Step [3700/5013], Loss: 0.6865\n",
      "Epoch [50/50], Step [3800/5013], Loss: 0.6873\n",
      "Epoch [50/50], Step [3900/5013], Loss: 0.6763\n",
      "Epoch [50/50], Step [4000/5013], Loss: 0.7018\n",
      "Epoch [50/50], Step [4100/5013], Loss: 0.6743\n",
      "Epoch [50/50], Step [4200/5013], Loss: 0.6648\n",
      "Epoch [50/50], Step [4300/5013], Loss: 0.6603\n",
      "Epoch [50/50], Step [4400/5013], Loss: 0.6929\n",
      "Epoch [50/50], Step [4500/5013], Loss: 0.6871\n",
      "Epoch [50/50], Step [4600/5013], Loss: 0.6734\n",
      "Epoch [50/50], Step [4700/5013], Loss: 0.6842\n",
      "Epoch [50/50], Step [4800/5013], Loss: 0.6678\n",
      "Epoch [50/50], Step [4900/5013], Loss: 0.6728\n",
      "Epoch [50/50], Step [5000/5013], Loss: 0.6571\n",
      "Epoch [50/50] Average Loss: 0.6678\n",
      "Training complete\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "gc.collect()\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=50, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4938260d-d795-4254-9e05-d888c65f9311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss: 0.8454, Accuracy: 0.6982\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.8454493901509785, 0.6982001151525772)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform evaluation\n",
    "gc.collect()\n",
    "evaluate_model(model, test_loader, criterion, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bbc9a25-2878-4bfe-909c-6ea66c63fbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.0\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print((end-start)//60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "897f2344-ea73-4a97-8153-7bbda44b6d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), r\"C:\\Users\\nadav.k\\Documents\\DS\\DL_classification\\saved_models\\sentinal1_best_manuel_params_fulldata_new_labels.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd67e095-5ab6-4260-b20c-ae8c38698ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
