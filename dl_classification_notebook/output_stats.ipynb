{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c83fb373-efeb-4a73-acde-e219f6485661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nbimporter\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from two_sats import SatelliteDataset, ConvNet\n",
    "from sentinal_1 import Sentinel1Dataset, Sentinel1ConvNet\n",
    "from sentinal_2 import Sentinel2Dataset, Sentinel2ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "327a4d62-55aa-4471-8b02-06292c3974f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .h5 file into memory once\n",
    "\n",
    "h5_file_path_test = r\"C:\\Users\\nadav.k\\Documents\\DS\\DL_classification\\classification_data\\testing.h5\"\n",
    "\n",
    "# Open the H5 files\n",
    "h5_test = h5py.File(h5_file_path_test, 'r')\n",
    "test_sen1_data = h5_test['sen1']\n",
    "test_sen2_data = h5_test['sen2']\n",
    "test_labels = h5_test['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d362be5-1521-42c0-9d8d-a4bc7e1fe428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_dataloader(dataset_type, batch_size=64, shuffle=False):\n",
    "    \"\"\"\n",
    "    Returns a DataLoader for the specified dataset type.\n",
    "\n",
    "    Args:\n",
    "        dataset_type (str): Type of the dataset ('SatelliteDataset', 'Sentinel1Dataset', 'Sentinel2Dataset').\n",
    "        batch_size (int): Batch size for the DataLoader.\n",
    "        shuffle (bool): Whether to shuffle the dataset.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: DataLoader for the specified dataset.\n",
    "    \"\"\"\n",
    "    if dataset_type == \"SatelliteDataset\":\n",
    "        dataset = SatelliteDataset(sen1_data=test_sen1_data, sen2_data=test_sen2_data, labels=test_labels)\n",
    "    elif dataset_type == \"Sentinel1Dataset\":\n",
    "        dataset = Sentinel1Dataset(sen1_data=test_sen1_data, labels=test_labels)\n",
    "    elif dataset_type == \"Sentinel2Dataset\":\n",
    "        dataset = Sentinel2Dataset(sen2_data=test_sen2_data, labels=test_labels)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset type: {dataset_type}\")\n",
    "\n",
    "    # Create and return the DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    print(f\"{dataset_type} loaded with {len(dataset)} samples.\")\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9c5518e-8270-472c-9f3f-847d156d7343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General function to load a model\n",
    "def load_model(model_class, path, num_classes=17, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Load a saved model from the specified path.\n",
    "\n",
    "    Args:\n",
    "        model_class: The class of the model to instantiate.\n",
    "        path: Full path to the saved model (e.g., './models/sentinel2_classification_model.pth').\n",
    "        num_classes: Number of classes for the model.\n",
    "        device: Device to load the model ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        The loaded model.\n",
    "    \"\"\"\n",
    "    model = model_class(num_classes=num_classes)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e38a519-e352-4f0c-af79-eaa57e0d57ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_performance_general(model, test_loader, num_classes=17, satellite_type=\"both\", device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    General evaluation function for Sentinel-1, Sentinel-2, or both.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model to evaluate.\n",
    "        test_loader: DataLoader for the test dataset.\n",
    "        num_classes: Number of classes.\n",
    "        satellite_type: 'sentinel1', 'sentinel2', or 'both'.\n",
    "        device: Device for computation ('cuda' or 'cpu').\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "\n",
    "            # Get predicted labels\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            # Append to lists\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    true_labels = np.array(true_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    cm = confusion_matrix(true_labels, predicted_labels, labels=range(num_classes))\n",
    "\n",
    "    # Display confusion matrix\n",
    "    print(f\"Confusion Matrix for {satellite_type.capitalize()}:\\n\", cm)\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(num_classes)).plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Confusion Matrix - {satellite_type.capitalize()}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "        # Correct vs incorrect predictions\n",
    "    correct_per_label = np.diag(cm)\n",
    "    total_per_label = np.sum(cm, axis=1)\n",
    "    incorrect_per_label = total_per_label - correct_per_label\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.arange(len(correct_per_label))\n",
    "    bar_width = 0.4\n",
    "\n",
    "    # Plot bars\n",
    "    correct_bars = plt.bar(x - bar_width / 2, correct_per_label, width=bar_width, label=\"Correct\", color=\"g\")\n",
    "    incorrect_bars = plt.bar(x + bar_width / 2, incorrect_per_label, width=bar_width, label=\"Incorrect\", color=\"r\")\n",
    "\n",
    "    # Add labels above bars\n",
    "    for bar in correct_bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, yval + 1, f\"{int(yval)}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    for bar in incorrect_bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, yval + 1, f\"{int(yval)}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    # Formatting\n",
    "    plt.xticks(ticks=x, labels=range(num_classes))\n",
    "    plt.title(f\"Correct vs Incorrect Predictions - {satellite_type.capitalize()}\")\n",
    "    plt.xlabel(\"Labels\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.grid(axis=\"y\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4f8d1799-cde8-41e4-928c-afc790893c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from H5 file...\n",
      "Test data loaded. Samples: 79894\n",
      "Creating DataLoader...\n",
      "SatelliteDataset loaded with 79894 samples.\n",
      "Loading the trained model...\n",
      "Model loaded from C:\\Users\\nadav.k\\Documents\\DS\\DL_classification\\saved_models\\two_stas_best_manual_fulldata.pth\n",
      "Analyzing model performance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nadav.k\\AppData\\Local\\Temp\\ipykernel_9176\\2026791148.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path, map_location=device))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Step 4: Analyze model performance\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyzing model performance...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m \u001b[43manalyze_model_performance_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m17\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43msatellite_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSatelliteDataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust this if needed\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     29\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Step 5: Clean up resources\u001b[39;00m\n\u001b[0;32m     32\u001b[0m h5_test\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[45], line 19\u001b[0m, in \u001b[0;36manalyze_model_performance_general\u001b[1;34m(model, test_loader, num_classes, satellite_type, device)\u001b[0m\n\u001b[0;32m     16\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m     20\u001b[0m         data, labels \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     21\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(data)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Define paths to saved model and test H5 file\n",
    "saved_model_path = r\"C:\\Users\\nadav.k\\Documents\\DS\\DL_classification\\saved_models\\two_stas_best_manual_fulldata.pth\"\n",
    "h5_file_path_test = r\"C:\\Users\\nadav.k\\Documents\\DS\\DL_classification\\classification_data\\testing.h5\"\n",
    "\n",
    "# Step 1: Load the H5 test file\n",
    "print(\"Loading test data from H5 file...\")\n",
    "h5_test = h5py.File(h5_file_path_test, 'r')\n",
    "test_sen1_data = h5_test['sen1']\n",
    "test_sen2_data = h5_test['sen2']\n",
    "test_labels = h5_test['label']\n",
    "print(f\"Test data loaded. Samples: {len(test_labels)}\")\n",
    "\n",
    "# Step 2: Get the DataLoader\n",
    "print(\"Creating DataLoader...\")\n",
    "test_loader = get_dataloader(dataset_type=\"SatelliteDataset\", batch_size=64, shuffle=False)\n",
    "\n",
    "# Step 3: Load the trained model\n",
    "print(\"Loading the trained model...\")\n",
    "model = load_model(ConvNet, path=saved_model_path, num_classes=17, device=\"cuda\")\n",
    "\n",
    "# Step 4: Analyze model performance\n",
    "print(\"Analyzing model performance...\")\n",
    "analyze_model_performance_general(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    num_classes=17,\n",
    "    satellite_type=\"SatelliteDataset\",  # Adjust this if needed\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "# Step 5: Clean up resources\n",
    "h5_test.close()\n",
    "print(\"Evaluation complete. Test H5 file closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3eb1684e-e81b-4b92-a0ae-e13404bc8555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_performance_general_with_groups(model, test_loader, num_classes=17, satellite_type=\"both\", device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    General evaluation function for Sentinel-1, Sentinel-2, or both, with grouped bar chart for label categories.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model to evaluate.\n",
    "        test_loader: DataLoader for the test dataset.\n",
    "        num_classes: Number of classes.\n",
    "        satellite_type: 'sentinel1', 'sentinel2', or 'both'.\n",
    "        device: Device for computation ('cuda' or 'cpu').\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "\n",
    "            # Get predicted labels\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            # Append to lists\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    true_labels = np.array(true_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    cm = confusion_matrix(true_labels, predicted_labels, labels=range(num_classes))\n",
    "\n",
    "    # Display confusion matrix\n",
    "    print(f\"Confusion Matrix for {satellite_type.capitalize()}:\\n\", cm)\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(num_classes)).plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Confusion Matrix - {satellite_type.capitalize()}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Correct vs incorrect predictions\n",
    "    correct_per_label = np.diag(cm)\n",
    "    total_per_label = np.sum(cm, axis=1)\n",
    "    incorrect_per_label = total_per_label - correct_per_label\n",
    "\n",
    "    # Standard bar chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.arange(len(correct_per_label))\n",
    "    bar_width = 0.4\n",
    "\n",
    "    correct_bars = plt.bar(x - bar_width / 2, correct_per_label, width=bar_width, label=\"Correct\", color=\"g\")\n",
    "    incorrect_bars = plt.bar(x + bar_width / 2, incorrect_per_label, width=bar_width, label=\"Incorrect\", color=\"r\")\n",
    "\n",
    "    for bar in correct_bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, yval + 1, f\"{int(yval)}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    for bar in incorrect_bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, yval + 1, f\"{int(yval)}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    plt.xticks(ticks=x, labels=range(num_classes))\n",
    "    plt.title(f\"Correct vs Incorrect Predictions - {satellite_type.capitalize()}\")\n",
    "    plt.xlabel(\"Labels\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.grid(axis=\"y\")\n",
    "    plt.show()\n",
    "\n",
    "    # Grouped categories\n",
    "    label_groups = {\n",
    "        \"Compact Urban\": [0, 1, 2],\n",
    "        \"Light Urban\": [3, 4, 5, 8],\n",
    "        \"Lightweight Building\": [6],\n",
    "        \"Industrial\": [7, 9],\n",
    "        \"Forested\": [10, 11],\n",
    "        \"Low Plants\": [12, 13],\n",
    "        \"Bare Rock/Paved\": [14],\n",
    "        \"Bare Soil/Sand\": [15],\n",
    "        \"Water\": [16]\n",
    "    }\n",
    "\n",
    "    grouped_correct = []\n",
    "    grouped_incorrect = []\n",
    "    group_labels = []\n",
    "\n",
    "    for group_name, indices in label_groups.items():\n",
    "        grouped_correct.append(np.sum([correct_per_label[i] for i in indices]))\n",
    "        grouped_incorrect.append(np.sum([incorrect_per_label[i] for i in indices]))\n",
    "        group_labels.append(group_name)\n",
    "\n",
    "    # Grouped bar chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x_grouped = np.arange(len(grouped_correct))\n",
    "    correct_bars_grouped = plt.bar(x_grouped - bar_width / 2, grouped_correct, width=bar_width, label=\"Correct\", color=\"g\")\n",
    "    incorrect_bars_grouped = plt.bar(x_grouped + bar_width / 2, grouped_incorrect, width=bar_width, label=\"Incorrect\", color=\"r\")\n",
    "\n",
    "    for bar in correct_bars_grouped:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, yval + 1, f\"{int(yval)}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    for bar in incorrect_bars_grouped:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, yval + 1, f\"{int(yval)}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    plt.xticks(ticks=x_grouped, labels=group_labels, rotation=45, ha=\"right\")\n",
    "    plt.title(f\"Grouped Correct vs Incorrect Predictions - {satellite_type.capitalize()}\")\n",
    "    plt.xlabel(\"Groups\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.grid(axis=\"y\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9481e6e2-f24f-4acc-b273-625ad8181d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentinel1Dataset loaded with 79894 samples.\n"
     ]
    }
   ],
   "source": [
    "test_loader = get_dataloader(dataset_type=\"Sentinel1Dataset\", batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45941277-92ee-4255-bcb7-0d2c08ea3b5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m analyze_model_performance_general_with_groups(\n\u001b[1;32m----> 2\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m,\n\u001b[0;32m      3\u001b[0m     test_loader\u001b[38;5;241m=\u001b[39mtest_loader,\n\u001b[0;32m      4\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m17\u001b[39m,\n\u001b[0;32m      5\u001b[0m     satellite_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentinel2\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 'sentinel1', 'sentinel2', or 'both'\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "analyze_model_performance_general_with_groups(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    num_classes=17,\n",
    "    satellite_type=\"sentinel2\",  # 'sentinel1', 'sentinel2', or 'both'\n",
    "    device=\"cuda\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaecbc3-6d96-4bd2-a51f-b190774153c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
