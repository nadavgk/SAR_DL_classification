{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f63d136-1087-4062-ac67-c6d16a0c86b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import gc\n",
    "import time\n",
    "\n",
    "gc.collect()  # Clear unused memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d51ab8f-dc5e-4986-8d2d-02fbc3dd3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .h5 file into memory once\n",
    "h5_file_path_train = r\"C:\\Users\\nadav.k\\Documents\\DS\\DL_classification\\classification_data\\training.h5\"\n",
    "h5_file_path_test = r\"C:\\Users\\nadav.k\\Documents\\DS\\DL_classification\\classification_data\\testing.h5\"\n",
    "\n",
    "# Open the H5 files\n",
    "h5_train = h5py.File(h5_file_path_train, 'r')\n",
    "h5_test = h5py.File(h5_file_path_test, 'r')\n",
    "\n",
    "# Extract datasets\n",
    "train_sen1_data = h5_train['sen1']\n",
    "train_sen2_data = h5_train['sen2']\n",
    "train_labels = h5_train['label']\n",
    "\n",
    "test_sen1_data = h5_test['sen1']\n",
    "test_sen2_data = h5_test['sen2']\n",
    "test_labels = h5_test['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e75ed6a3-57f9-4c19-b278-1313172836d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, sen1_data, sen2_data, labels):\n",
    "        self.sen1_data = sen1_data\n",
    "        self.sen2_data = sen2_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sen1_image = self.sen1_data[idx]\n",
    "        sen2_image = self.sen2_data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        sen1_image = torch.tensor(sen1_image, dtype=torch.float32).permute(2, 0, 1)\n",
    "        sen2_image = torch.tensor(sen2_image, dtype=torch.float32).permute(2, 0, 1)\n",
    "\n",
    "        # Convert one-hot encoded label to class index\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        label = torch.argmax(label).long()\n",
    "\n",
    "        return sen1_image, sen2_image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bee5a203-dac1-416f-93ef-ff41c54035a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = SatelliteDataset(train_sen1_data, train_sen2_data, train_labels)\n",
    "test_dataset = SatelliteDataset(test_sen1_data, test_sen2_data, test_labels)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88f6d3c0-49d2-44c3-848a-83e678941033",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=17):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        # Sentinel-1 branch\n",
    "        self.sen1_conv1 = nn.Conv2d(8, 32, kernel_size=3, padding=1)\n",
    "        self.sen1_dropout1 = nn.Dropout(p=0.25)\n",
    "        self.sen1_conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.sen1_dropout2 = nn.Dropout(p=0.25)\n",
    "        self.sen1_pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Sentinel-2 branch\n",
    "        self.sen2_conv1 = nn.Conv2d(10, 32, kernel_size=3, padding=1)\n",
    "        self.sen2_dropout1 = nn.Dropout(p=0.25)\n",
    "        self.sen2_conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.sen2_dropout2 = nn.Dropout(p=0.25)\n",
    "        self.sen2_pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully connected layers after concatenation\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16 * 2, 128)\n",
    "        self.fc1_dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc2_dropout = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, sen1, sen2):\n",
    "        # Sentinel-1 forward pass\n",
    "        x1 = F.relu(self.sen1_conv1(sen1))\n",
    "        x1 = self.sen1_dropout1(x1)\n",
    "        x1 = self.sen1_pool(F.relu(self.sen1_conv2(x1)))\n",
    "        x1 = self.sen1_dropout2(x1)\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "\n",
    "        # Sentinel-2 forward pass\n",
    "        x2 = F.relu(self.sen2_conv1(sen2))\n",
    "        x2 = self.sen2_dropout1(x2)\n",
    "        x2 = self.sen2_pool(F.relu(self.sen2_conv2(x2)))\n",
    "        x2 = self.sen2_dropout2(x2)\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "\n",
    "        # Concatenate both branches\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc1_dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc2_dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b586d227-c66a-4e53-864c-bd64efa1ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with visualization and memory clearing\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    train_losses = []  # List to store training loss for visualization\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i, (sen1, sen2, labels) in enumerate(train_loader):\n",
    "            sen1, sen2, labels = sen1.to(device), sen2.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sen1, sen2)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Clear memory for each batch (optional but not usually necessary here)\n",
    "            del outputs, loss\n",
    "\n",
    "            if i % 100 == 99:  # Print every 100 mini-batches\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Average loss for the epoch\n",
    "        epoch_loss /= len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}] Average Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        # Clear unused memory after each epoch\n",
    "        torch.cuda.empty_cache()  # Clear GPU memory\n",
    "        gc.collect()  # Trigger garbage collection for CPU memory\n",
    "\n",
    "    print('Training complete')\n",
    "\n",
    "    # Visualization of training loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o', label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a65f9d1e-b689-47ed-8ce4-362bd97e369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    val_losses = []  # Optional, to track across batches if needed\n",
    "    val_accuracies = []  # Optional, to track across batches if needed\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sen1, sen2, labels in test_loader:\n",
    "            sen1, sen2, labels = sen1.to(device), sen2.to(device), labels.to(device)\n",
    "            outputs = model(sen1, sen2)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    print(f'Average Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Visualization (optional)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(['Loss', 'Accuracy'], [avg_loss, accuracy])\n",
    "    plt.title('Evaluation Results')\n",
    "    plt.ylabel('Value')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff47e8bb-8c75-4f5c-ad46-e2a79869ba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = ConvNet(num_classes=17)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec9e4c5-902e-4bc0-91cb-750b3363eaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [100/5013], Loss: 2.3171\n",
      "Epoch [1/50], Step [200/5013], Loss: 1.8771\n",
      "Epoch [1/50], Step [300/5013], Loss: 1.6959\n",
      "Epoch [1/50], Step [400/5013], Loss: 1.5908\n",
      "Epoch [1/50], Step [500/5013], Loss: 1.5773\n",
      "Epoch [1/50], Step [600/5013], Loss: 1.5129\n",
      "Epoch [1/50], Step [700/5013], Loss: 1.4830\n",
      "Epoch [1/50], Step [800/5013], Loss: 1.4802\n",
      "Epoch [1/50], Step [900/5013], Loss: 1.4110\n",
      "Epoch [1/50], Step [1000/5013], Loss: 1.3876\n",
      "Epoch [1/50], Step [1100/5013], Loss: 1.4246\n",
      "Epoch [1/50], Step [1200/5013], Loss: 1.3696\n",
      "Epoch [1/50], Step [1300/5013], Loss: 1.3649\n",
      "Epoch [1/50], Step [1400/5013], Loss: 1.3669\n",
      "Epoch [1/50], Step [1500/5013], Loss: 1.3047\n",
      "Epoch [1/50], Step [1600/5013], Loss: 1.3114\n",
      "Epoch [1/50], Step [1700/5013], Loss: 1.2754\n",
      "Epoch [1/50], Step [1800/5013], Loss: 1.3279\n",
      "Epoch [1/50], Step [1900/5013], Loss: 1.2967\n",
      "Epoch [1/50], Step [2000/5013], Loss: 1.2588\n",
      "Epoch [1/50], Step [2100/5013], Loss: 1.2765\n",
      "Epoch [1/50], Step [2200/5013], Loss: 1.2794\n",
      "Epoch [1/50], Step [2300/5013], Loss: 1.2461\n",
      "Epoch [1/50], Step [2400/5013], Loss: 1.2224\n",
      "Epoch [1/50], Step [2500/5013], Loss: 1.2351\n",
      "Epoch [1/50], Step [2600/5013], Loss: 1.2193\n",
      "Epoch [1/50], Step [2700/5013], Loss: 1.2353\n",
      "Epoch [1/50], Step [2800/5013], Loss: 1.1853\n",
      "Epoch [1/50], Step [2900/5013], Loss: 1.2038\n",
      "Epoch [1/50], Step [3000/5013], Loss: 1.1811\n",
      "Epoch [1/50], Step [3100/5013], Loss: 1.1817\n",
      "Epoch [1/50], Step [3200/5013], Loss: 1.1694\n",
      "Epoch [1/50], Step [3300/5013], Loss: 1.1854\n",
      "Epoch [1/50], Step [3400/5013], Loss: 1.1845\n",
      "Epoch [1/50], Step [3500/5013], Loss: 1.1521\n",
      "Epoch [1/50], Step [3600/5013], Loss: 1.1522\n",
      "Epoch [1/50], Step [3700/5013], Loss: 1.1391\n",
      "Epoch [1/50], Step [3800/5013], Loss: 1.1206\n",
      "Epoch [1/50], Step [3900/5013], Loss: 1.1175\n",
      "Epoch [1/50], Step [4000/5013], Loss: 1.1740\n",
      "Epoch [1/50], Step [4100/5013], Loss: 1.1328\n",
      "Epoch [1/50], Step [4200/5013], Loss: 1.0811\n",
      "Epoch [1/50], Step [4300/5013], Loss: 1.1136\n",
      "Epoch [1/50], Step [4400/5013], Loss: 1.1216\n",
      "Epoch [1/50], Step [4500/5013], Loss: 1.1027\n",
      "Epoch [1/50], Step [4600/5013], Loss: 1.0736\n",
      "Epoch [1/50], Step [4700/5013], Loss: 1.0690\n",
      "Epoch [1/50], Step [4800/5013], Loss: 1.0634\n",
      "Epoch [1/50], Step [4900/5013], Loss: 1.0851\n",
      "Epoch [1/50], Step [5000/5013], Loss: 1.0969\n",
      "Epoch [1/50] Average Loss: 1.2880\n",
      "Epoch [2/50], Step [100/5013], Loss: 1.0838\n",
      "Epoch [2/50], Step [200/5013], Loss: 1.0758\n",
      "Epoch [2/50], Step [300/5013], Loss: 1.0595\n",
      "Epoch [2/50], Step [400/5013], Loss: 1.0407\n",
      "Epoch [2/50], Step [500/5013], Loss: 1.0373\n",
      "Epoch [2/50], Step [600/5013], Loss: 1.0475\n",
      "Epoch [2/50], Step [700/5013], Loss: 1.0736\n",
      "Epoch [2/50], Step [800/5013], Loss: 1.0249\n",
      "Epoch [2/50], Step [900/5013], Loss: 1.0495\n",
      "Epoch [2/50], Step [1000/5013], Loss: 1.0711\n",
      "Epoch [2/50], Step [1100/5013], Loss: 1.0416\n",
      "Epoch [2/50], Step [1200/5013], Loss: 1.0587\n",
      "Epoch [2/50], Step [1300/5013], Loss: 1.0336\n",
      "Epoch [2/50], Step [1400/5013], Loss: 1.0200\n",
      "Epoch [2/50], Step [1500/5013], Loss: 0.9980\n",
      "Epoch [2/50], Step [1600/5013], Loss: 1.0210\n",
      "Epoch [2/50], Step [1700/5013], Loss: 1.0372\n",
      "Epoch [2/50], Step [1800/5013], Loss: 0.9893\n",
      "Epoch [2/50], Step [1900/5013], Loss: 1.0267\n",
      "Epoch [2/50], Step [2000/5013], Loss: 1.0062\n",
      "Epoch [2/50], Step [2100/5013], Loss: 1.0126\n",
      "Epoch [2/50], Step [2200/5013], Loss: 1.0541\n",
      "Epoch [2/50], Step [2300/5013], Loss: 1.0407\n",
      "Epoch [2/50], Step [2400/5013], Loss: 0.9906\n",
      "Epoch [2/50], Step [2500/5013], Loss: 0.9929\n",
      "Epoch [2/50], Step [2600/5013], Loss: 0.9949\n",
      "Epoch [2/50], Step [2700/5013], Loss: 1.0116\n",
      "Epoch [2/50], Step [2800/5013], Loss: 0.9855\n",
      "Epoch [2/50], Step [2900/5013], Loss: 0.9746\n",
      "Epoch [2/50], Step [3000/5013], Loss: 0.9870\n",
      "Epoch [2/50], Step [3100/5013], Loss: 0.9987\n",
      "Epoch [2/50], Step [3200/5013], Loss: 1.0206\n",
      "Epoch [2/50], Step [3300/5013], Loss: 1.0083\n",
      "Epoch [2/50], Step [3400/5013], Loss: 1.0062\n",
      "Epoch [2/50], Step [3500/5013], Loss: 0.9505\n",
      "Epoch [2/50], Step [3600/5013], Loss: 0.9272\n",
      "Epoch [2/50], Step [3700/5013], Loss: 0.9432\n",
      "Epoch [2/50], Step [3800/5013], Loss: 0.9571\n",
      "Epoch [2/50], Step [3900/5013], Loss: 0.9848\n",
      "Epoch [2/50], Step [4000/5013], Loss: 0.9554\n",
      "Epoch [2/50], Step [4100/5013], Loss: 0.9487\n",
      "Epoch [2/50], Step [4200/5013], Loss: 0.9751\n",
      "Epoch [2/50], Step [4300/5013], Loss: 0.9760\n",
      "Epoch [2/50], Step [4400/5013], Loss: 0.9412\n",
      "Epoch [2/50], Step [4500/5013], Loss: 0.9456\n",
      "Epoch [2/50], Step [4600/5013], Loss: 0.9621\n",
      "Epoch [2/50], Step [4700/5013], Loss: 0.9668\n",
      "Epoch [2/50], Step [4800/5013], Loss: 0.9410\n",
      "Epoch [2/50], Step [4900/5013], Loss: 0.9415\n",
      "Epoch [2/50], Step [5000/5013], Loss: 0.9274\n",
      "Epoch [2/50] Average Loss: 1.0022\n",
      "Epoch [3/50], Step [100/5013], Loss: 0.9480\n",
      "Epoch [3/50], Step [200/5013], Loss: 0.9500\n",
      "Epoch [3/50], Step [300/5013], Loss: 0.9307\n",
      "Epoch [3/50], Step [400/5013], Loss: 0.9078\n",
      "Epoch [3/50], Step [500/5013], Loss: 0.9082\n",
      "Epoch [3/50], Step [600/5013], Loss: 0.9262\n",
      "Epoch [3/50], Step [700/5013], Loss: 0.9297\n",
      "Epoch [3/50], Step [800/5013], Loss: 0.9172\n",
      "Epoch [3/50], Step [900/5013], Loss: 0.8964\n",
      "Epoch [3/50], Step [1000/5013], Loss: 0.9273\n",
      "Epoch [3/50], Step [1100/5013], Loss: 0.8916\n",
      "Epoch [3/50], Step [1200/5013], Loss: 0.8918\n",
      "Epoch [3/50], Step [1300/5013], Loss: 0.8987\n",
      "Epoch [3/50], Step [1400/5013], Loss: 0.8837\n",
      "Epoch [3/50], Step [1500/5013], Loss: 0.8969\n",
      "Epoch [3/50], Step [1600/5013], Loss: 0.9292\n",
      "Epoch [3/50], Step [1700/5013], Loss: 0.9079\n",
      "Epoch [3/50], Step [1800/5013], Loss: 0.9064\n",
      "Epoch [3/50], Step [1900/5013], Loss: 0.9184\n",
      "Epoch [3/50], Step [2000/5013], Loss: 0.8964\n",
      "Epoch [3/50], Step [2100/5013], Loss: 0.9133\n",
      "Epoch [3/50], Step [2200/5013], Loss: 0.9356\n",
      "Epoch [3/50], Step [2300/5013], Loss: 0.8793\n",
      "Epoch [3/50], Step [2400/5013], Loss: 0.9349\n",
      "Epoch [3/50], Step [2500/5013], Loss: 0.8830\n",
      "Epoch [3/50], Step [2600/5013], Loss: 0.9430\n",
      "Epoch [3/50], Step [2700/5013], Loss: 0.9229\n",
      "Epoch [3/50], Step [2800/5013], Loss: 0.9191\n",
      "Epoch [3/50], Step [2900/5013], Loss: 0.8960\n",
      "Epoch [3/50], Step [3000/5013], Loss: 0.9042\n",
      "Epoch [3/50], Step [3100/5013], Loss: 0.9437\n",
      "Epoch [3/50], Step [3200/5013], Loss: 0.9166\n",
      "Epoch [3/50], Step [3300/5013], Loss: 0.9197\n",
      "Epoch [3/50], Step [3400/5013], Loss: 0.8757\n",
      "Epoch [3/50], Step [3500/5013], Loss: 0.9018\n",
      "Epoch [3/50], Step [3600/5013], Loss: 0.8587\n",
      "Epoch [3/50], Step [3700/5013], Loss: 0.8869\n",
      "Epoch [3/50], Step [3800/5013], Loss: 0.8724\n",
      "Epoch [3/50], Step [3900/5013], Loss: 0.8643\n",
      "Epoch [3/50], Step [4000/5013], Loss: 0.8827\n",
      "Epoch [3/50], Step [4100/5013], Loss: 0.8864\n",
      "Epoch [3/50], Step [4200/5013], Loss: 0.9049\n",
      "Epoch [3/50], Step [4300/5013], Loss: 0.8695\n",
      "Epoch [3/50], Step [4400/5013], Loss: 0.8938\n",
      "Epoch [3/50], Step [4500/5013], Loss: 0.8779\n",
      "Epoch [3/50], Step [4600/5013], Loss: 0.8849\n",
      "Epoch [3/50], Step [4700/5013], Loss: 0.8680\n",
      "Epoch [3/50], Step [4800/5013], Loss: 0.8883\n",
      "Epoch [3/50], Step [4900/5013], Loss: 0.8762\n",
      "Epoch [3/50], Step [5000/5013], Loss: 0.9095\n",
      "Epoch [3/50] Average Loss: 0.9031\n",
      "Epoch [4/50], Step [100/5013], Loss: 0.8725\n",
      "Epoch [4/50], Step [200/5013], Loss: 0.9013\n",
      "Epoch [4/50], Step [300/5013], Loss: 0.8661\n",
      "Epoch [4/50], Step [400/5013], Loss: 0.8652\n",
      "Epoch [4/50], Step [500/5013], Loss: 0.8414\n",
      "Epoch [4/50], Step [600/5013], Loss: 0.8632\n",
      "Epoch [4/50], Step [700/5013], Loss: 0.8674\n",
      "Epoch [4/50], Step [800/5013], Loss: 0.8278\n",
      "Epoch [4/50], Step [900/5013], Loss: 0.8991\n",
      "Epoch [4/50], Step [1000/5013], Loss: 0.8343\n",
      "Epoch [4/50], Step [1100/5013], Loss: 0.8624\n",
      "Epoch [4/50], Step [1200/5013], Loss: 0.8497\n",
      "Epoch [4/50], Step [1300/5013], Loss: 0.8421\n",
      "Epoch [4/50], Step [1400/5013], Loss: 0.8300\n",
      "Epoch [4/50], Step [1500/5013], Loss: 0.8591\n",
      "Epoch [4/50], Step [1600/5013], Loss: 0.8252\n",
      "Epoch [4/50], Step [1700/5013], Loss: 0.8203\n",
      "Epoch [4/50], Step [1800/5013], Loss: 0.8589\n",
      "Epoch [4/50], Step [1900/5013], Loss: 0.8631\n",
      "Epoch [4/50], Step [2000/5013], Loss: 0.8436\n",
      "Epoch [4/50], Step [2100/5013], Loss: 0.8486\n",
      "Epoch [4/50], Step [2200/5013], Loss: 0.8451\n",
      "Epoch [4/50], Step [2300/5013], Loss: 0.8191\n",
      "Epoch [4/50], Step [2400/5013], Loss: 0.8387\n",
      "Epoch [4/50], Step [2500/5013], Loss: 0.8445\n",
      "Epoch [4/50], Step [2600/5013], Loss: 0.8188\n",
      "Epoch [4/50], Step [2700/5013], Loss: 0.8695\n",
      "Epoch [4/50], Step [2800/5013], Loss: 0.8404\n",
      "Epoch [4/50], Step [2900/5013], Loss: 0.8189\n",
      "Epoch [4/50], Step [3000/5013], Loss: 0.8214\n",
      "Epoch [4/50], Step [3100/5013], Loss: 0.8260\n",
      "Epoch [4/50], Step [3200/5013], Loss: 0.8518\n",
      "Epoch [4/50], Step [3300/5013], Loss: 0.8586\n",
      "Epoch [4/50], Step [3400/5013], Loss: 0.8522\n",
      "Epoch [4/50], Step [3500/5013], Loss: 0.8310\n",
      "Epoch [4/50], Step [3600/5013], Loss: 0.8732\n",
      "Epoch [4/50], Step [3700/5013], Loss: 0.8343\n",
      "Epoch [4/50], Step [3800/5013], Loss: 0.8471\n",
      "Epoch [4/50], Step [3900/5013], Loss: 0.8228\n",
      "Epoch [4/50], Step [4000/5013], Loss: 0.8232\n",
      "Epoch [4/50], Step [4100/5013], Loss: 0.8443\n",
      "Epoch [4/50], Step [4200/5013], Loss: 0.8004\n",
      "Epoch [4/50], Step [4300/5013], Loss: 0.8184\n",
      "Epoch [4/50], Step [4400/5013], Loss: 0.8440\n",
      "Epoch [4/50], Step [4500/5013], Loss: 0.8345\n",
      "Epoch [4/50], Step [4600/5013], Loss: 0.8273\n",
      "Epoch [4/50], Step [4700/5013], Loss: 0.8119\n",
      "Epoch [4/50], Step [4800/5013], Loss: 0.8246\n",
      "Epoch [4/50], Step [4900/5013], Loss: 0.8018\n",
      "Epoch [4/50], Step [5000/5013], Loss: 0.8304\n",
      "Epoch [4/50] Average Loss: 0.8422\n",
      "Epoch [5/50], Step [100/5013], Loss: 0.8059\n",
      "Epoch [5/50], Step [200/5013], Loss: 0.7920\n",
      "Epoch [5/50], Step [300/5013], Loss: 0.7860\n",
      "Epoch [5/50], Step [400/5013], Loss: 0.8015\n",
      "Epoch [5/50], Step [500/5013], Loss: 0.8281\n",
      "Epoch [5/50], Step [600/5013], Loss: 0.8004\n",
      "Epoch [5/50], Step [700/5013], Loss: 0.8146\n",
      "Epoch [5/50], Step [800/5013], Loss: 0.7893\n",
      "Epoch [5/50], Step [900/5013], Loss: 0.8015\n",
      "Epoch [5/50], Step [1000/5013], Loss: 0.7977\n",
      "Epoch [5/50], Step [1100/5013], Loss: 0.8133\n",
      "Epoch [5/50], Step [1200/5013], Loss: 0.7840\n",
      "Epoch [5/50], Step [1300/5013], Loss: 0.7835\n",
      "Epoch [5/50], Step [1400/5013], Loss: 0.8244\n",
      "Epoch [5/50], Step [1500/5013], Loss: 0.8523\n",
      "Epoch [5/50], Step [1600/5013], Loss: 0.8493\n",
      "Epoch [5/50], Step [1700/5013], Loss: 0.8053\n",
      "Epoch [5/50], Step [1800/5013], Loss: 0.7890\n",
      "Epoch [5/50], Step [1900/5013], Loss: 0.8218\n",
      "Epoch [5/50], Step [2000/5013], Loss: 0.7790\n",
      "Epoch [5/50], Step [2100/5013], Loss: 0.7796\n",
      "Epoch [5/50], Step [2200/5013], Loss: 0.7700\n",
      "Epoch [5/50], Step [2300/5013], Loss: 0.7894\n",
      "Epoch [5/50], Step [2400/5013], Loss: 0.7852\n",
      "Epoch [5/50], Step [2500/5013], Loss: 0.7600\n",
      "Epoch [5/50], Step [2600/5013], Loss: 0.7653\n",
      "Epoch [5/50], Step [2700/5013], Loss: 0.8021\n",
      "Epoch [5/50], Step [2800/5013], Loss: 0.7883\n",
      "Epoch [5/50], Step [2900/5013], Loss: 0.8266\n",
      "Epoch [5/50], Step [3000/5013], Loss: 0.7985\n",
      "Epoch [5/50], Step [3100/5013], Loss: 0.7744\n",
      "Epoch [5/50], Step [3200/5013], Loss: 0.7596\n",
      "Epoch [5/50], Step [3300/5013], Loss: 0.7919\n",
      "Epoch [5/50], Step [3400/5013], Loss: 0.7982\n",
      "Epoch [5/50], Step [3500/5013], Loss: 0.7854\n",
      "Epoch [5/50], Step [3600/5013], Loss: 0.7867\n",
      "Epoch [5/50], Step [3700/5013], Loss: 0.7911\n",
      "Epoch [5/50], Step [3800/5013], Loss: 0.8008\n",
      "Epoch [5/50], Step [3900/5013], Loss: 0.7557\n",
      "Epoch [5/50], Step [4000/5013], Loss: 0.8032\n",
      "Epoch [5/50], Step [4100/5013], Loss: 0.7975\n",
      "Epoch [5/50], Step [4200/5013], Loss: 0.7803\n",
      "Epoch [5/50], Step [4300/5013], Loss: 0.7548\n",
      "Epoch [5/50], Step [4400/5013], Loss: 0.7994\n",
      "Epoch [5/50], Step [4500/5013], Loss: 0.7517\n",
      "Epoch [5/50], Step [4600/5013], Loss: 0.8030\n",
      "Epoch [5/50], Step [4700/5013], Loss: 0.7995\n",
      "Epoch [5/50], Step [4800/5013], Loss: 0.7919\n",
      "Epoch [5/50], Step [4900/5013], Loss: 0.7802\n",
      "Epoch [5/50], Step [5000/5013], Loss: 0.7791\n",
      "Epoch [5/50] Average Loss: 0.7934\n",
      "Epoch [6/50], Step [100/5013], Loss: 0.7407\n",
      "Epoch [6/50], Step [200/5013], Loss: 0.7874\n",
      "Epoch [6/50], Step [300/5013], Loss: 0.7454\n",
      "Epoch [6/50], Step [400/5013], Loss: 0.7572\n",
      "Epoch [6/50], Step [500/5013], Loss: 0.7931\n",
      "Epoch [6/50], Step [600/5013], Loss: 0.7275\n",
      "Epoch [6/50], Step [700/5013], Loss: 0.7923\n",
      "Epoch [6/50], Step [800/5013], Loss: 0.7373\n",
      "Epoch [6/50], Step [900/5013], Loss: 0.7920\n",
      "Epoch [6/50], Step [1000/5013], Loss: 0.7680\n",
      "Epoch [6/50], Step [1100/5013], Loss: 0.7791\n",
      "Epoch [6/50], Step [1200/5013], Loss: 0.7537\n",
      "Epoch [6/50], Step [1300/5013], Loss: 0.7644\n",
      "Epoch [6/50], Step [1400/5013], Loss: 0.7453\n",
      "Epoch [6/50], Step [1500/5013], Loss: 0.7318\n",
      "Epoch [6/50], Step [1600/5013], Loss: 0.7840\n",
      "Epoch [6/50], Step [1700/5013], Loss: 0.7589\n",
      "Epoch [6/50], Step [1800/5013], Loss: 0.7484\n",
      "Epoch [6/50], Step [1900/5013], Loss: 0.7689\n",
      "Epoch [6/50], Step [2000/5013], Loss: 0.7399\n",
      "Epoch [6/50], Step [2100/5013], Loss: 0.7902\n",
      "Epoch [6/50], Step [2200/5013], Loss: 0.7298\n",
      "Epoch [6/50], Step [2300/5013], Loss: 0.7685\n",
      "Epoch [6/50], Step [2400/5013], Loss: 0.7520\n",
      "Epoch [6/50], Step [2500/5013], Loss: 0.7869\n",
      "Epoch [6/50], Step [2600/5013], Loss: 0.7571\n",
      "Epoch [6/50], Step [2700/5013], Loss: 0.7790\n",
      "Epoch [6/50], Step [2800/5013], Loss: 0.7432\n",
      "Epoch [6/50], Step [2900/5013], Loss: 0.7705\n",
      "Epoch [6/50], Step [3000/5013], Loss: 0.7649\n",
      "Epoch [6/50], Step [3100/5013], Loss: 0.7706\n",
      "Epoch [6/50], Step [3200/5013], Loss: 0.7802\n",
      "Epoch [6/50], Step [3300/5013], Loss: 0.7563\n",
      "Epoch [6/50], Step [3400/5013], Loss: 0.7630\n",
      "Epoch [6/50], Step [3500/5013], Loss: 0.7688\n",
      "Epoch [6/50], Step [3600/5013], Loss: 0.7380\n",
      "Epoch [6/50], Step [3700/5013], Loss: 0.7637\n",
      "Epoch [6/50], Step [3800/5013], Loss: 0.7617\n",
      "Epoch [6/50], Step [3900/5013], Loss: 0.7371\n",
      "Epoch [6/50], Step [4000/5013], Loss: 0.7438\n",
      "Epoch [6/50], Step [4100/5013], Loss: 0.7319\n",
      "Epoch [6/50], Step [4200/5013], Loss: 0.7325\n",
      "Epoch [6/50], Step [4300/5013], Loss: 0.7770\n",
      "Epoch [6/50], Step [4400/5013], Loss: 0.7502\n",
      "Epoch [6/50], Step [4500/5013], Loss: 0.7946\n",
      "Epoch [6/50], Step [4600/5013], Loss: 0.7270\n",
      "Epoch [6/50], Step [4700/5013], Loss: 0.7872\n",
      "Epoch [6/50], Step [4800/5013], Loss: 0.7398\n",
      "Epoch [6/50], Step [4900/5013], Loss: 0.7540\n",
      "Epoch [6/50], Step [5000/5013], Loss: 0.7652\n",
      "Epoch [6/50] Average Loss: 0.7601\n",
      "Epoch [7/50], Step [100/5013], Loss: 0.7418\n",
      "Epoch [7/50], Step [200/5013], Loss: 0.7311\n",
      "Epoch [7/50], Step [300/5013], Loss: 0.7284\n",
      "Epoch [7/50], Step [400/5013], Loss: 0.7715\n",
      "Epoch [7/50], Step [500/5013], Loss: 0.7148\n",
      "Epoch [7/50], Step [600/5013], Loss: 0.7477\n",
      "Epoch [7/50], Step [700/5013], Loss: 0.7552\n",
      "Epoch [7/50], Step [800/5013], Loss: 0.7311\n",
      "Epoch [7/50], Step [900/5013], Loss: 0.7424\n",
      "Epoch [7/50], Step [1000/5013], Loss: 0.7221\n",
      "Epoch [7/50], Step [1100/5013], Loss: 0.7688\n",
      "Epoch [7/50], Step [1200/5013], Loss: 0.7520\n",
      "Epoch [7/50], Step [1300/5013], Loss: 0.7196\n",
      "Epoch [7/50], Step [1400/5013], Loss: 0.7341\n",
      "Epoch [7/50], Step [1500/5013], Loss: 0.7313\n",
      "Epoch [7/50], Step [1600/5013], Loss: 0.7295\n",
      "Epoch [7/50], Step [1700/5013], Loss: 0.7495\n",
      "Epoch [7/50], Step [1800/5013], Loss: 0.7495\n",
      "Epoch [7/50], Step [1900/5013], Loss: 0.7363\n",
      "Epoch [7/50], Step [2000/5013], Loss: 0.7422\n",
      "Epoch [7/50], Step [2100/5013], Loss: 0.7551\n",
      "Epoch [7/50], Step [2200/5013], Loss: 0.7075\n",
      "Epoch [7/50], Step [2300/5013], Loss: 0.7392\n",
      "Epoch [7/50], Step [2400/5013], Loss: 0.7264\n",
      "Epoch [7/50], Step [2500/5013], Loss: 0.7373\n",
      "Epoch [7/50], Step [2600/5013], Loss: 0.7461\n",
      "Epoch [7/50], Step [2700/5013], Loss: 0.7586\n",
      "Epoch [7/50], Step [2800/5013], Loss: 0.7342\n",
      "Epoch [7/50], Step [2900/5013], Loss: 0.7507\n",
      "Epoch [7/50], Step [3000/5013], Loss: 0.7369\n",
      "Epoch [7/50], Step [3100/5013], Loss: 0.7203\n",
      "Epoch [7/50], Step [3200/5013], Loss: 0.7163\n",
      "Epoch [7/50], Step [3300/5013], Loss: 0.7227\n",
      "Epoch [7/50], Step [3400/5013], Loss: 0.7633\n",
      "Epoch [7/50], Step [3500/5013], Loss: 0.7584\n",
      "Epoch [7/50], Step [3600/5013], Loss: 0.7569\n",
      "Epoch [7/50], Step [3700/5013], Loss: 0.7332\n",
      "Epoch [7/50], Step [3800/5013], Loss: 0.7427\n",
      "Epoch [7/50], Step [3900/5013], Loss: 0.7485\n",
      "Epoch [7/50], Step [4000/5013], Loss: 0.7365\n",
      "Epoch [7/50], Step [4100/5013], Loss: 0.7515\n",
      "Epoch [7/50], Step [4200/5013], Loss: 0.7227\n",
      "Epoch [7/50], Step [4300/5013], Loss: 0.7239\n",
      "Epoch [7/50], Step [4400/5013], Loss: 0.7805\n",
      "Epoch [7/50], Step [4500/5013], Loss: 0.7271\n",
      "Epoch [7/50], Step [4600/5013], Loss: 0.6918\n",
      "Epoch [7/50], Step [4700/5013], Loss: 0.7337\n",
      "Epoch [7/50], Step [4800/5013], Loss: 0.7760\n",
      "Epoch [7/50], Step [4900/5013], Loss: 0.7400\n",
      "Epoch [7/50], Step [5000/5013], Loss: 0.7401\n",
      "Epoch [7/50] Average Loss: 0.7399\n",
      "Epoch [8/50], Step [100/5013], Loss: 0.7063\n",
      "Epoch [8/50], Step [200/5013], Loss: 0.7283\n",
      "Epoch [8/50], Step [300/5013], Loss: 0.7247\n",
      "Epoch [8/50], Step [400/5013], Loss: 0.7023\n",
      "Epoch [8/50], Step [500/5013], Loss: 0.7002\n",
      "Epoch [8/50], Step [600/5013], Loss: 0.7212\n",
      "Epoch [8/50], Step [700/5013], Loss: 0.7018\n",
      "Epoch [8/50], Step [800/5013], Loss: 0.7215\n",
      "Epoch [8/50], Step [900/5013], Loss: 0.7118\n",
      "Epoch [8/50], Step [1000/5013], Loss: 0.6790\n",
      "Epoch [8/50], Step [1100/5013], Loss: 0.7369\n",
      "Epoch [8/50], Step [1200/5013], Loss: 0.6902\n",
      "Epoch [8/50], Step [1300/5013], Loss: 0.7089\n",
      "Epoch [8/50], Step [1400/5013], Loss: 0.7139\n",
      "Epoch [8/50], Step [1500/5013], Loss: 0.7175\n",
      "Epoch [8/50], Step [1600/5013], Loss: 0.7149\n",
      "Epoch [8/50], Step [1700/5013], Loss: 0.7052\n",
      "Epoch [8/50], Step [1800/5013], Loss: 0.7128\n",
      "Epoch [8/50], Step [1900/5013], Loss: 0.6968\n",
      "Epoch [8/50], Step [2000/5013], Loss: 0.6718\n",
      "Epoch [8/50], Step [2100/5013], Loss: 0.7286\n",
      "Epoch [8/50], Step [2200/5013], Loss: 0.7313\n",
      "Epoch [8/50], Step [2300/5013], Loss: 0.7371\n",
      "Epoch [8/50], Step [2400/5013], Loss: 0.7141\n",
      "Epoch [8/50], Step [2500/5013], Loss: 0.7770\n",
      "Epoch [8/50], Step [2600/5013], Loss: 0.7152\n",
      "Epoch [8/50], Step [2700/5013], Loss: 0.7136\n",
      "Epoch [8/50], Step [2800/5013], Loss: 0.6943\n",
      "Epoch [8/50], Step [2900/5013], Loss: 0.7434\n",
      "Epoch [8/50], Step [3000/5013], Loss: 0.7079\n",
      "Epoch [8/50], Step [3100/5013], Loss: 0.7005\n",
      "Epoch [8/50], Step [3200/5013], Loss: 0.7456\n",
      "Epoch [8/50], Step [3300/5013], Loss: 0.6978\n",
      "Epoch [8/50], Step [3400/5013], Loss: 0.7195\n",
      "Epoch [8/50], Step [3500/5013], Loss: 0.7396\n",
      "Epoch [8/50], Step [3600/5013], Loss: 0.7055\n",
      "Epoch [8/50], Step [3700/5013], Loss: 0.7085\n",
      "Epoch [8/50], Step [3800/5013], Loss: 0.7487\n",
      "Epoch [8/50], Step [3900/5013], Loss: 0.6817\n",
      "Epoch [8/50], Step [4000/5013], Loss: 0.6998\n",
      "Epoch [8/50], Step [4100/5013], Loss: 0.7116\n",
      "Epoch [8/50], Step [4200/5013], Loss: 0.7302\n",
      "Epoch [8/50], Step [4300/5013], Loss: 0.7284\n",
      "Epoch [8/50], Step [4400/5013], Loss: 0.7320\n",
      "Epoch [8/50], Step [4500/5013], Loss: 0.7147\n",
      "Epoch [8/50], Step [4600/5013], Loss: 0.7194\n",
      "Epoch [8/50], Step [4700/5013], Loss: 0.6898\n",
      "Epoch [8/50], Step [4800/5013], Loss: 0.7068\n",
      "Epoch [8/50], Step [4900/5013], Loss: 0.7151\n",
      "Epoch [8/50], Step [5000/5013], Loss: 0.7113\n",
      "Epoch [8/50] Average Loss: 0.7147\n",
      "Epoch [9/50], Step [100/5013], Loss: 0.6855\n",
      "Epoch [9/50], Step [200/5013], Loss: 0.7153\n",
      "Epoch [9/50], Step [300/5013], Loss: 0.6946\n",
      "Epoch [9/50], Step [400/5013], Loss: 0.6950\n",
      "Epoch [9/50], Step [500/5013], Loss: 0.6960\n",
      "Epoch [9/50], Step [600/5013], Loss: 0.6939\n",
      "Epoch [9/50], Step [700/5013], Loss: 0.6734\n",
      "Epoch [9/50], Step [800/5013], Loss: 0.6833\n",
      "Epoch [9/50], Step [900/5013], Loss: 0.7105\n",
      "Epoch [9/50], Step [1000/5013], Loss: 0.6997\n",
      "Epoch [9/50], Step [1100/5013], Loss: 0.7059\n",
      "Epoch [9/50], Step [1200/5013], Loss: 0.7064\n",
      "Epoch [9/50], Step [1300/5013], Loss: 0.6781\n",
      "Epoch [9/50], Step [1400/5013], Loss: 0.6951\n",
      "Epoch [9/50], Step [1500/5013], Loss: 0.6951\n",
      "Epoch [9/50], Step [1600/5013], Loss: 0.6753\n",
      "Epoch [9/50], Step [1700/5013], Loss: 0.7437\n",
      "Epoch [9/50], Step [1800/5013], Loss: 0.6879\n",
      "Epoch [9/50], Step [1900/5013], Loss: 0.7073\n",
      "Epoch [9/50], Step [2000/5013], Loss: 0.7007\n",
      "Epoch [9/50], Step [2100/5013], Loss: 0.7059\n",
      "Epoch [9/50], Step [2200/5013], Loss: 0.6875\n",
      "Epoch [9/50], Step [2300/5013], Loss: 0.7131\n",
      "Epoch [9/50], Step [2400/5013], Loss: 0.6951\n",
      "Epoch [9/50], Step [2500/5013], Loss: 0.7077\n",
      "Epoch [9/50], Step [2600/5013], Loss: 0.7100\n",
      "Epoch [9/50], Step [2700/5013], Loss: 0.7017\n",
      "Epoch [9/50], Step [2800/5013], Loss: 0.6813\n",
      "Epoch [9/50], Step [2900/5013], Loss: 0.7071\n",
      "Epoch [9/50], Step [3000/5013], Loss: 0.7171\n",
      "Epoch [9/50], Step [3100/5013], Loss: 0.7039\n",
      "Epoch [9/50], Step [3200/5013], Loss: 0.7051\n",
      "Epoch [9/50], Step [3300/5013], Loss: 0.7176\n",
      "Epoch [9/50], Step [3400/5013], Loss: 0.6922\n",
      "Epoch [9/50], Step [3500/5013], Loss: 0.7102\n",
      "Epoch [9/50], Step [3600/5013], Loss: 0.6897\n",
      "Epoch [9/50], Step [3700/5013], Loss: 0.7009\n",
      "Epoch [9/50], Step [3800/5013], Loss: 0.7038\n",
      "Epoch [9/50], Step [3900/5013], Loss: 0.7203\n",
      "Epoch [9/50], Step [4000/5013], Loss: 0.7163\n",
      "Epoch [9/50], Step [4100/5013], Loss: 0.7225\n",
      "Epoch [9/50], Step [4200/5013], Loss: 0.6989\n",
      "Epoch [9/50], Step [4300/5013], Loss: 0.6678\n",
      "Epoch [9/50], Step [4400/5013], Loss: 0.6942\n",
      "Epoch [9/50], Step [4500/5013], Loss: 0.6865\n",
      "Epoch [9/50], Step [4600/5013], Loss: 0.6912\n",
      "Epoch [9/50], Step [4700/5013], Loss: 0.6663\n",
      "Epoch [9/50], Step [4800/5013], Loss: 0.6985\n",
      "Epoch [9/50], Step [4900/5013], Loss: 0.6928\n",
      "Epoch [9/50], Step [5000/5013], Loss: 0.7354\n",
      "Epoch [9/50] Average Loss: 0.6994\n",
      "Epoch [10/50], Step [100/5013], Loss: 0.6985\n",
      "Epoch [10/50], Step [200/5013], Loss: 0.6848\n",
      "Epoch [10/50], Step [300/5013], Loss: 0.6967\n",
      "Epoch [10/50], Step [400/5013], Loss: 0.6779\n",
      "Epoch [10/50], Step [500/5013], Loss: 0.6951\n",
      "Epoch [10/50], Step [600/5013], Loss: 0.6909\n",
      "Epoch [10/50], Step [700/5013], Loss: 0.6778\n",
      "Epoch [10/50], Step [800/5013], Loss: 0.6993\n",
      "Epoch [10/50], Step [900/5013], Loss: 0.6682\n",
      "Epoch [10/50], Step [1000/5013], Loss: 0.6854\n",
      "Epoch [10/50], Step [1100/5013], Loss: 0.6816\n",
      "Epoch [10/50], Step [1200/5013], Loss: 0.7026\n",
      "Epoch [10/50], Step [1300/5013], Loss: 0.6647\n",
      "Epoch [10/50], Step [1400/5013], Loss: 0.6819\n",
      "Epoch [10/50], Step [1500/5013], Loss: 0.6380\n",
      "Epoch [10/50], Step [1600/5013], Loss: 0.6768\n",
      "Epoch [10/50], Step [1700/5013], Loss: 0.6903\n",
      "Epoch [10/50], Step [1800/5013], Loss: 0.7215\n",
      "Epoch [10/50], Step [1900/5013], Loss: 0.6679\n",
      "Epoch [10/50], Step [2000/5013], Loss: 0.6888\n",
      "Epoch [10/50], Step [2100/5013], Loss: 0.6865\n",
      "Epoch [10/50], Step [2200/5013], Loss: 0.6975\n",
      "Epoch [10/50], Step [2300/5013], Loss: 0.6839\n",
      "Epoch [10/50], Step [2400/5013], Loss: 0.6948\n",
      "Epoch [10/50], Step [2500/5013], Loss: 0.6932\n",
      "Epoch [10/50], Step [2600/5013], Loss: 0.6569\n",
      "Epoch [10/50], Step [2700/5013], Loss: 0.6756\n",
      "Epoch [10/50], Step [2800/5013], Loss: 0.6776\n",
      "Epoch [10/50], Step [2900/5013], Loss: 0.6845\n",
      "Epoch [10/50], Step [3000/5013], Loss: 0.6884\n",
      "Epoch [10/50], Step [3100/5013], Loss: 0.6462\n",
      "Epoch [10/50], Step [3200/5013], Loss: 0.6686\n",
      "Epoch [10/50], Step [3300/5013], Loss: 0.6917\n",
      "Epoch [10/50], Step [3400/5013], Loss: 0.7031\n",
      "Epoch [10/50], Step [3500/5013], Loss: 0.6726\n",
      "Epoch [10/50], Step [3600/5013], Loss: 0.6994\n",
      "Epoch [10/50], Step [3700/5013], Loss: 0.6730\n",
      "Epoch [10/50], Step [3800/5013], Loss: 0.6947\n",
      "Epoch [10/50], Step [3900/5013], Loss: 0.7093\n",
      "Epoch [10/50], Step [4000/5013], Loss: 0.6829\n",
      "Epoch [10/50], Step [4100/5013], Loss: 0.6923\n",
      "Epoch [10/50], Step [4200/5013], Loss: 0.6878\n",
      "Epoch [10/50], Step [4300/5013], Loss: 0.6913\n",
      "Epoch [10/50], Step [4400/5013], Loss: 0.6642\n",
      "Epoch [10/50], Step [4500/5013], Loss: 0.6863\n",
      "Epoch [10/50], Step [4600/5013], Loss: 0.6950\n",
      "Epoch [10/50], Step [4700/5013], Loss: 0.6935\n",
      "Epoch [10/50], Step [4800/5013], Loss: 0.6586\n",
      "Epoch [10/50], Step [4900/5013], Loss: 0.6541\n",
      "Epoch [10/50], Step [5000/5013], Loss: 0.7201\n",
      "Epoch [10/50] Average Loss: 0.6845\n",
      "Epoch [11/50], Step [100/5013], Loss: 0.6683\n",
      "Epoch [11/50], Step [200/5013], Loss: 0.6482\n",
      "Epoch [11/50], Step [300/5013], Loss: 0.7035\n",
      "Epoch [11/50], Step [400/5013], Loss: 0.6932\n",
      "Epoch [11/50], Step [500/5013], Loss: 0.6790\n",
      "Epoch [11/50], Step [600/5013], Loss: 0.6604\n",
      "Epoch [11/50], Step [700/5013], Loss: 0.6626\n",
      "Epoch [11/50], Step [800/5013], Loss: 0.6649\n",
      "Epoch [11/50], Step [900/5013], Loss: 0.6644\n",
      "Epoch [11/50], Step [1000/5013], Loss: 0.6764\n",
      "Epoch [11/50], Step [1100/5013], Loss: 0.6627\n",
      "Epoch [11/50], Step [1200/5013], Loss: 0.7029\n",
      "Epoch [11/50], Step [1300/5013], Loss: 0.6712\n",
      "Epoch [11/50], Step [1400/5013], Loss: 0.6645\n",
      "Epoch [11/50], Step [1500/5013], Loss: 0.6713\n",
      "Epoch [11/50], Step [1600/5013], Loss: 0.6635\n",
      "Epoch [11/50], Step [1700/5013], Loss: 0.6757\n",
      "Epoch [11/50], Step [1800/5013], Loss: 0.6775\n",
      "Epoch [11/50], Step [1900/5013], Loss: 0.6908\n",
      "Epoch [11/50], Step [2000/5013], Loss: 0.6597\n",
      "Epoch [11/50], Step [2100/5013], Loss: 0.7361\n",
      "Epoch [11/50], Step [2200/5013], Loss: 0.6800\n",
      "Epoch [11/50], Step [2300/5013], Loss: 0.6349\n",
      "Epoch [11/50], Step [2400/5013], Loss: 0.6976\n",
      "Epoch [11/50], Step [2500/5013], Loss: 0.6716\n",
      "Epoch [11/50], Step [2600/5013], Loss: 0.6494\n",
      "Epoch [11/50], Step [2700/5013], Loss: 0.6524\n",
      "Epoch [11/50], Step [2800/5013], Loss: 0.6688\n",
      "Epoch [11/50], Step [2900/5013], Loss: 0.6442\n",
      "Epoch [11/50], Step [3000/5013], Loss: 0.6911\n",
      "Epoch [11/50], Step [3100/5013], Loss: 0.6656\n",
      "Epoch [11/50], Step [3200/5013], Loss: 0.6741\n",
      "Epoch [11/50], Step [3300/5013], Loss: 0.6519\n",
      "Epoch [11/50], Step [3400/5013], Loss: 0.6827\n",
      "Epoch [11/50], Step [3500/5013], Loss: 0.6736\n",
      "Epoch [11/50], Step [3600/5013], Loss: 0.6624\n",
      "Epoch [11/50], Step [3700/5013], Loss: 0.6721\n",
      "Epoch [11/50], Step [3800/5013], Loss: 0.6589\n",
      "Epoch [11/50], Step [3900/5013], Loss: 0.6695\n",
      "Epoch [11/50], Step [4000/5013], Loss: 0.6714\n",
      "Epoch [11/50], Step [4100/5013], Loss: 0.6697\n",
      "Epoch [11/50], Step [4200/5013], Loss: 0.6577\n",
      "Epoch [11/50], Step [4300/5013], Loss: 0.6612\n",
      "Epoch [11/50], Step [4400/5013], Loss: 0.6947\n",
      "Epoch [11/50], Step [4500/5013], Loss: 0.6482\n",
      "Epoch [11/50], Step [4600/5013], Loss: 0.6573\n",
      "Epoch [11/50], Step [4700/5013], Loss: 0.6721\n",
      "Epoch [11/50], Step [4800/5013], Loss: 0.6606\n",
      "Epoch [11/50], Step [4900/5013], Loss: 0.7156\n",
      "Epoch [11/50], Step [5000/5013], Loss: 0.6863\n",
      "Epoch [11/50] Average Loss: 0.6718\n",
      "Epoch [12/50], Step [100/5013], Loss: 0.6570\n",
      "Epoch [12/50], Step [200/5013], Loss: 0.6313\n",
      "Epoch [12/50], Step [300/5013], Loss: 0.6579\n",
      "Epoch [12/50], Step [400/5013], Loss: 0.6615\n",
      "Epoch [12/50], Step [500/5013], Loss: 0.6606\n",
      "Epoch [12/50], Step [600/5013], Loss: 0.6739\n",
      "Epoch [12/50], Step [700/5013], Loss: 0.6708\n",
      "Epoch [12/50], Step [800/5013], Loss: 0.6724\n",
      "Epoch [12/50], Step [900/5013], Loss: 0.6491\n",
      "Epoch [12/50], Step [1000/5013], Loss: 0.6450\n",
      "Epoch [12/50], Step [1100/5013], Loss: 0.6520\n",
      "Epoch [12/50], Step [1200/5013], Loss: 0.6477\n",
      "Epoch [12/50], Step [1300/5013], Loss: 0.6338\n",
      "Epoch [12/50], Step [1400/5013], Loss: 0.6459\n",
      "Epoch [12/50], Step [1500/5013], Loss: 0.6329\n",
      "Epoch [12/50], Step [1600/5013], Loss: 0.6753\n",
      "Epoch [12/50], Step [1700/5013], Loss: 0.6725\n",
      "Epoch [12/50], Step [1800/5013], Loss: 0.6831\n",
      "Epoch [12/50], Step [1900/5013], Loss: 0.6626\n",
      "Epoch [12/50], Step [2000/5013], Loss: 0.6577\n",
      "Epoch [12/50], Step [2100/5013], Loss: 0.6586\n",
      "Epoch [12/50], Step [2200/5013], Loss: 0.6668\n",
      "Epoch [12/50], Step [2300/5013], Loss: 0.6563\n",
      "Epoch [12/50], Step [2400/5013], Loss: 0.6592\n",
      "Epoch [12/50], Step [2500/5013], Loss: 0.6525\n",
      "Epoch [12/50], Step [2600/5013], Loss: 0.6618\n",
      "Epoch [12/50], Step [2700/5013], Loss: 0.6787\n",
      "Epoch [12/50], Step [2800/5013], Loss: 0.6766\n",
      "Epoch [12/50], Step [2900/5013], Loss: 0.6600\n",
      "Epoch [12/50], Step [3000/5013], Loss: 0.6619\n",
      "Epoch [12/50], Step [3100/5013], Loss: 0.6669\n",
      "Epoch [12/50], Step [3200/5013], Loss: 0.6556\n",
      "Epoch [12/50], Step [3300/5013], Loss: 0.6419\n",
      "Epoch [12/50], Step [3400/5013], Loss: 0.6735\n",
      "Epoch [12/50], Step [3500/5013], Loss: 0.6513\n",
      "Epoch [12/50], Step [3600/5013], Loss: 0.6156\n",
      "Epoch [12/50], Step [3700/5013], Loss: 0.6544\n",
      "Epoch [12/50], Step [3800/5013], Loss: 0.6600\n",
      "Epoch [12/50], Step [3900/5013], Loss: 0.6819\n",
      "Epoch [12/50], Step [4000/5013], Loss: 0.6610\n",
      "Epoch [12/50], Step [4100/5013], Loss: 0.6405\n",
      "Epoch [12/50], Step [4200/5013], Loss: 0.6654\n",
      "Epoch [12/50], Step [4300/5013], Loss: 0.6858\n",
      "Epoch [12/50], Step [4400/5013], Loss: 0.6910\n",
      "Epoch [12/50], Step [4500/5013], Loss: 0.6646\n",
      "Epoch [12/50], Step [4600/5013], Loss: 0.6424\n",
      "Epoch [12/50], Step [4700/5013], Loss: 0.6859\n",
      "Epoch [12/50], Step [4800/5013], Loss: 0.6743\n",
      "Epoch [12/50], Step [4900/5013], Loss: 0.6184\n",
      "Epoch [12/50], Step [5000/5013], Loss: 0.6520\n",
      "Epoch [12/50] Average Loss: 0.6592\n",
      "Epoch [13/50], Step [100/5013], Loss: 0.6284\n",
      "Epoch [13/50], Step [200/5013], Loss: 0.6182\n",
      "Epoch [13/50], Step [300/5013], Loss: 0.6333\n",
      "Epoch [13/50], Step [400/5013], Loss: 0.6306\n",
      "Epoch [13/50], Step [500/5013], Loss: 0.6731\n",
      "Epoch [13/50], Step [600/5013], Loss: 0.6529\n",
      "Epoch [13/50], Step [700/5013], Loss: 0.6652\n",
      "Epoch [13/50], Step [800/5013], Loss: 0.6529\n",
      "Epoch [13/50], Step [900/5013], Loss: 0.6230\n",
      "Epoch [13/50], Step [1000/5013], Loss: 0.6833\n",
      "Epoch [13/50], Step [1100/5013], Loss: 0.6526\n",
      "Epoch [13/50], Step [1200/5013], Loss: 0.6756\n",
      "Epoch [13/50], Step [1300/5013], Loss: 0.6285\n",
      "Epoch [13/50], Step [1400/5013], Loss: 0.6589\n",
      "Epoch [13/50], Step [1500/5013], Loss: 0.6532\n",
      "Epoch [13/50], Step [1600/5013], Loss: 0.6849\n",
      "Epoch [13/50], Step [1700/5013], Loss: 0.6843\n",
      "Epoch [13/50], Step [1800/5013], Loss: 0.6371\n",
      "Epoch [13/50], Step [1900/5013], Loss: 0.6622\n",
      "Epoch [13/50], Step [2000/5013], Loss: 0.6410\n",
      "Epoch [13/50], Step [2100/5013], Loss: 0.6226\n",
      "Epoch [13/50], Step [2200/5013], Loss: 0.6295\n",
      "Epoch [13/50], Step [2300/5013], Loss: 0.6302\n",
      "Epoch [13/50], Step [2400/5013], Loss: 0.6267\n",
      "Epoch [13/50], Step [2500/5013], Loss: 0.6558\n",
      "Epoch [13/50], Step [2600/5013], Loss: 0.6209\n",
      "Epoch [13/50], Step [2700/5013], Loss: 0.6582\n",
      "Epoch [13/50], Step [2800/5013], Loss: 0.6528\n",
      "Epoch [13/50], Step [2900/5013], Loss: 0.6309\n",
      "Epoch [13/50], Step [3000/5013], Loss: 0.6465\n",
      "Epoch [13/50], Step [3100/5013], Loss: 0.6828\n",
      "Epoch [13/50], Step [3200/5013], Loss: 0.6482\n",
      "Epoch [13/50], Step [3300/5013], Loss: 0.6408\n",
      "Epoch [13/50], Step [3400/5013], Loss: 0.6230\n",
      "Epoch [13/50], Step [3500/5013], Loss: 0.6442\n",
      "Epoch [13/50], Step [3600/5013], Loss: 0.6461\n",
      "Epoch [13/50], Step [3700/5013], Loss: 0.6596\n",
      "Epoch [13/50], Step [3800/5013], Loss: 0.6549\n",
      "Epoch [13/50], Step [3900/5013], Loss: 0.6385\n",
      "Epoch [13/50], Step [4000/5013], Loss: 0.6778\n",
      "Epoch [13/50], Step [4100/5013], Loss: 0.6504\n",
      "Epoch [13/50], Step [4200/5013], Loss: 0.6755\n",
      "Epoch [13/50], Step [4300/5013], Loss: 0.6486\n",
      "Epoch [13/50], Step [4400/5013], Loss: 0.6448\n",
      "Epoch [13/50], Step [4500/5013], Loss: 0.6598\n",
      "Epoch [13/50], Step [4600/5013], Loss: 0.6523\n",
      "Epoch [13/50], Step [4700/5013], Loss: 0.6365\n",
      "Epoch [13/50], Step [4800/5013], Loss: 0.6071\n",
      "Epoch [13/50], Step [4900/5013], Loss: 0.6766\n",
      "Epoch [13/50], Step [5000/5013], Loss: 0.6509\n",
      "Epoch [13/50] Average Loss: 0.6485\n",
      "Epoch [14/50], Step [100/5013], Loss: 0.6246\n",
      "Epoch [14/50], Step [200/5013], Loss: 0.5848\n",
      "Epoch [14/50], Step [300/5013], Loss: 0.6345\n",
      "Epoch [14/50], Step [400/5013], Loss: 0.6530\n",
      "Epoch [14/50], Step [500/5013], Loss: 0.6042\n",
      "Epoch [14/50], Step [600/5013], Loss: 0.6646\n",
      "Epoch [14/50], Step [700/5013], Loss: 0.6466\n",
      "Epoch [14/50], Step [800/5013], Loss: 0.6164\n",
      "Epoch [14/50], Step [900/5013], Loss: 0.6358\n",
      "Epoch [14/50], Step [1000/5013], Loss: 0.6334\n",
      "Epoch [14/50], Step [1100/5013], Loss: 0.6611\n",
      "Epoch [14/50], Step [1200/5013], Loss: 0.6387\n",
      "Epoch [14/50], Step [1300/5013], Loss: 0.6372\n",
      "Epoch [14/50], Step [1400/5013], Loss: 0.6684\n",
      "Epoch [14/50], Step [1500/5013], Loss: 0.6282\n",
      "Epoch [14/50], Step [1600/5013], Loss: 0.5921\n",
      "Epoch [14/50], Step [1700/5013], Loss: 0.6056\n",
      "Epoch [14/50], Step [1800/5013], Loss: 0.6187\n",
      "Epoch [14/50], Step [1900/5013], Loss: 0.6539\n",
      "Epoch [14/50], Step [2000/5013], Loss: 0.6198\n",
      "Epoch [14/50], Step [2100/5013], Loss: 0.6604\n",
      "Epoch [14/50], Step [2200/5013], Loss: 0.6387\n",
      "Epoch [14/50], Step [2300/5013], Loss: 0.6470\n",
      "Epoch [14/50], Step [2400/5013], Loss: 0.6305\n",
      "Epoch [14/50], Step [2500/5013], Loss: 0.6295\n",
      "Epoch [14/50], Step [2600/5013], Loss: 0.6354\n",
      "Epoch [14/50], Step [2700/5013], Loss: 0.5992\n",
      "Epoch [14/50], Step [2800/5013], Loss: 0.6431\n",
      "Epoch [14/50], Step [2900/5013], Loss: 0.6399\n",
      "Epoch [14/50], Step [3000/5013], Loss: 0.6572\n",
      "Epoch [14/50], Step [3100/5013], Loss: 0.6543\n",
      "Epoch [14/50], Step [3200/5013], Loss: 0.6189\n",
      "Epoch [14/50], Step [3300/5013], Loss: 0.6928\n",
      "Epoch [14/50], Step [3400/5013], Loss: 0.6172\n",
      "Epoch [14/50], Step [3500/5013], Loss: 0.6565\n",
      "Epoch [14/50], Step [3600/5013], Loss: 0.6382\n",
      "Epoch [14/50], Step [3700/5013], Loss: 0.6550\n",
      "Epoch [14/50], Step [3800/5013], Loss: 0.6270\n",
      "Epoch [14/50], Step [3900/5013], Loss: 0.6294\n",
      "Epoch [14/50], Step [4000/5013], Loss: 0.6493\n",
      "Epoch [14/50], Step [4100/5013], Loss: 0.6446\n",
      "Epoch [14/50], Step [4200/5013], Loss: 0.6451\n",
      "Epoch [14/50], Step [4300/5013], Loss: 0.6362\n",
      "Epoch [14/50], Step [4400/5013], Loss: 0.6153\n",
      "Epoch [14/50], Step [4500/5013], Loss: 0.6315\n",
      "Epoch [14/50], Step [4600/5013], Loss: 0.6456\n",
      "Epoch [14/50], Step [4700/5013], Loss: 0.6290\n",
      "Epoch [14/50], Step [4800/5013], Loss: 0.6400\n",
      "Epoch [14/50], Step [4900/5013], Loss: 0.6219\n",
      "Epoch [14/50], Step [5000/5013], Loss: 0.6342\n",
      "Epoch [14/50] Average Loss: 0.6357\n",
      "Epoch [15/50], Step [100/5013], Loss: 0.6574\n",
      "Epoch [15/50], Step [200/5013], Loss: 0.6203\n",
      "Epoch [15/50], Step [300/5013], Loss: 0.6157\n",
      "Epoch [15/50], Step [400/5013], Loss: 0.6331\n",
      "Epoch [15/50], Step [500/5013], Loss: 0.6012\n",
      "Epoch [15/50], Step [600/5013], Loss: 0.6342\n",
      "Epoch [15/50], Step [700/5013], Loss: 0.6166\n",
      "Epoch [15/50], Step [800/5013], Loss: 0.6096\n",
      "Epoch [15/50], Step [900/5013], Loss: 0.6583\n",
      "Epoch [15/50], Step [1000/5013], Loss: 0.6169\n",
      "Epoch [15/50], Step [1100/5013], Loss: 0.6210\n",
      "Epoch [15/50], Step [1200/5013], Loss: 0.6574\n",
      "Epoch [15/50], Step [1300/5013], Loss: 0.6318\n",
      "Epoch [15/50], Step [1400/5013], Loss: 0.6046\n",
      "Epoch [15/50], Step [1500/5013], Loss: 0.6444\n",
      "Epoch [15/50], Step [1600/5013], Loss: 0.6198\n",
      "Epoch [15/50], Step [1700/5013], Loss: 0.6455\n",
      "Epoch [15/50], Step [1800/5013], Loss: 0.6099\n",
      "Epoch [15/50], Step [1900/5013], Loss: 0.6251\n",
      "Epoch [15/50], Step [2000/5013], Loss: 0.6207\n",
      "Epoch [15/50], Step [2100/5013], Loss: 0.6277\n",
      "Epoch [15/50], Step [2200/5013], Loss: 0.6484\n",
      "Epoch [15/50], Step [2300/5013], Loss: 0.6252\n",
      "Epoch [15/50], Step [2400/5013], Loss: 0.6393\n",
      "Epoch [15/50], Step [2500/5013], Loss: 0.6153\n",
      "Epoch [15/50], Step [2600/5013], Loss: 0.6151\n",
      "Epoch [15/50], Step [2700/5013], Loss: 0.6409\n",
      "Epoch [15/50], Step [2800/5013], Loss: 0.6188\n",
      "Epoch [15/50], Step [2900/5013], Loss: 0.6385\n",
      "Epoch [15/50], Step [3000/5013], Loss: 0.6160\n",
      "Epoch [15/50], Step [3100/5013], Loss: 0.6263\n",
      "Epoch [15/50], Step [3200/5013], Loss: 0.6244\n",
      "Epoch [15/50], Step [3300/5013], Loss: 0.6244\n",
      "Epoch [15/50], Step [3400/5013], Loss: 0.6664\n",
      "Epoch [15/50], Step [3500/5013], Loss: 0.6179\n",
      "Epoch [15/50], Step [3600/5013], Loss: 0.5865\n",
      "Epoch [15/50], Step [3700/5013], Loss: 0.6262\n",
      "Epoch [15/50], Step [3800/5013], Loss: 0.6215\n",
      "Epoch [15/50], Step [3900/5013], Loss: 0.6483\n",
      "Epoch [15/50], Step [4000/5013], Loss: 0.6323\n",
      "Epoch [15/50], Step [4100/5013], Loss: 0.6097\n",
      "Epoch [15/50], Step [4200/5013], Loss: 0.6321\n",
      "Epoch [15/50], Step [4300/5013], Loss: 0.6058\n",
      "Epoch [15/50], Step [4400/5013], Loss: 0.6503\n",
      "Epoch [15/50], Step [4500/5013], Loss: 0.6407\n",
      "Epoch [15/50], Step [4600/5013], Loss: 0.6270\n",
      "Epoch [15/50], Step [4700/5013], Loss: 0.6271\n",
      "Epoch [15/50], Step [4800/5013], Loss: 0.6320\n",
      "Epoch [15/50], Step [4900/5013], Loss: 0.6097\n",
      "Epoch [15/50], Step [5000/5013], Loss: 0.6069\n",
      "Epoch [15/50] Average Loss: 0.6269\n",
      "Epoch [16/50], Step [100/5013], Loss: 0.5845\n",
      "Epoch [16/50], Step [200/5013], Loss: 0.6122\n",
      "Epoch [16/50], Step [300/5013], Loss: 0.6166\n",
      "Epoch [16/50], Step [400/5013], Loss: 0.6133\n",
      "Epoch [16/50], Step [500/5013], Loss: 0.6117\n",
      "Epoch [16/50], Step [600/5013], Loss: 0.5997\n",
      "Epoch [16/50], Step [700/5013], Loss: 0.6044\n",
      "Epoch [16/50], Step [800/5013], Loss: 0.6076\n",
      "Epoch [16/50], Step [900/5013], Loss: 0.6146\n",
      "Epoch [16/50], Step [1000/5013], Loss: 0.6207\n",
      "Epoch [16/50], Step [1100/5013], Loss: 0.6435\n",
      "Epoch [16/50], Step [1200/5013], Loss: 0.6041\n",
      "Epoch [16/50], Step [1300/5013], Loss: 0.6072\n",
      "Epoch [16/50], Step [1400/5013], Loss: 0.6176\n",
      "Epoch [16/50], Step [1500/5013], Loss: 0.6227\n",
      "Epoch [16/50], Step [1600/5013], Loss: 0.5969\n",
      "Epoch [16/50], Step [1700/5013], Loss: 0.6050\n",
      "Epoch [16/50], Step [1800/5013], Loss: 0.6247\n",
      "Epoch [16/50], Step [1900/5013], Loss: 0.6301\n",
      "Epoch [16/50], Step [2000/5013], Loss: 0.6195\n",
      "Epoch [16/50], Step [2100/5013], Loss: 0.6299\n",
      "Epoch [16/50], Step [2200/5013], Loss: 0.6194\n",
      "Epoch [16/50], Step [2300/5013], Loss: 0.6047\n",
      "Epoch [16/50], Step [2400/5013], Loss: 0.6452\n",
      "Epoch [16/50], Step [2500/5013], Loss: 0.6228\n",
      "Epoch [16/50], Step [2600/5013], Loss: 0.6296\n",
      "Epoch [16/50], Step [2700/5013], Loss: 0.5950\n",
      "Epoch [16/50], Step [2800/5013], Loss: 0.6748\n",
      "Epoch [16/50], Step [2900/5013], Loss: 0.6163\n",
      "Epoch [16/50], Step [3000/5013], Loss: 0.6309\n",
      "Epoch [16/50], Step [3100/5013], Loss: 0.6698\n",
      "Epoch [16/50], Step [3200/5013], Loss: 0.6148\n",
      "Epoch [16/50], Step [3300/5013], Loss: 0.6306\n",
      "Epoch [16/50], Step [3400/5013], Loss: 0.6000\n",
      "Epoch [16/50], Step [3500/5013], Loss: 0.6115\n",
      "Epoch [16/50], Step [3600/5013], Loss: 0.6130\n",
      "Epoch [16/50], Step [3700/5013], Loss: 0.6433\n",
      "Epoch [16/50], Step [3800/5013], Loss: 0.6152\n",
      "Epoch [16/50], Step [3900/5013], Loss: 0.6209\n",
      "Epoch [16/50], Step [4000/5013], Loss: 0.6231\n",
      "Epoch [16/50], Step [4100/5013], Loss: 0.5946\n",
      "Epoch [16/50], Step [4200/5013], Loss: 0.6069\n",
      "Epoch [16/50], Step [4300/5013], Loss: 0.6087\n",
      "Epoch [16/50], Step [4400/5013], Loss: 0.6086\n",
      "Epoch [16/50], Step [4500/5013], Loss: 0.6244\n",
      "Epoch [16/50], Step [4600/5013], Loss: 0.6150\n",
      "Epoch [16/50], Step [4700/5013], Loss: 0.6107\n",
      "Epoch [16/50], Step [4800/5013], Loss: 0.5953\n",
      "Epoch [16/50], Step [4900/5013], Loss: 0.6327\n",
      "Epoch [16/50], Step [5000/5013], Loss: 0.6563\n",
      "Epoch [16/50] Average Loss: 0.6185\n",
      "Epoch [17/50], Step [100/5013], Loss: 0.6064\n",
      "Epoch [17/50], Step [200/5013], Loss: 0.6113\n",
      "Epoch [17/50], Step [300/5013], Loss: 0.6331\n",
      "Epoch [17/50], Step [400/5013], Loss: 0.5773\n",
      "Epoch [17/50], Step [500/5013], Loss: 0.6231\n",
      "Epoch [17/50], Step [600/5013], Loss: 0.6128\n",
      "Epoch [17/50], Step [700/5013], Loss: 0.5963\n",
      "Epoch [17/50], Step [800/5013], Loss: 0.6127\n",
      "Epoch [17/50], Step [900/5013], Loss: 0.6029\n",
      "Epoch [17/50], Step [1000/5013], Loss: 0.5916\n",
      "Epoch [17/50], Step [1100/5013], Loss: 0.6620\n",
      "Epoch [17/50], Step [1200/5013], Loss: 0.6161\n",
      "Epoch [17/50], Step [1300/5013], Loss: 0.6030\n",
      "Epoch [17/50], Step [1400/5013], Loss: 0.6038\n",
      "Epoch [17/50], Step [1500/5013], Loss: 0.6201\n",
      "Epoch [17/50], Step [1600/5013], Loss: 0.5836\n",
      "Epoch [17/50], Step [1700/5013], Loss: 0.6151\n",
      "Epoch [17/50], Step [1800/5013], Loss: 0.5793\n",
      "Epoch [17/50], Step [1900/5013], Loss: 0.6472\n",
      "Epoch [17/50], Step [2000/5013], Loss: 0.6213\n",
      "Epoch [17/50], Step [2100/5013], Loss: 0.6498\n",
      "Epoch [17/50], Step [2200/5013], Loss: 0.6598\n",
      "Epoch [17/50], Step [2300/5013], Loss: 0.6049\n",
      "Epoch [17/50], Step [2400/5013], Loss: 0.6089\n",
      "Epoch [17/50], Step [2500/5013], Loss: 0.5706\n",
      "Epoch [17/50], Step [2600/5013], Loss: 0.6010\n",
      "Epoch [17/50], Step [2700/5013], Loss: 0.5971\n",
      "Epoch [17/50], Step [2800/5013], Loss: 0.6083\n",
      "Epoch [17/50], Step [2900/5013], Loss: 0.6145\n",
      "Epoch [17/50], Step [3000/5013], Loss: 0.5747\n",
      "Epoch [17/50], Step [3100/5013], Loss: 0.5950\n",
      "Epoch [17/50], Step [3200/5013], Loss: 0.6177\n",
      "Epoch [17/50], Step [3300/5013], Loss: 0.6022\n",
      "Epoch [17/50], Step [3400/5013], Loss: 0.6178\n",
      "Epoch [17/50], Step [3500/5013], Loss: 0.6203\n",
      "Epoch [17/50], Step [3600/5013], Loss: 0.5862\n",
      "Epoch [17/50], Step [3700/5013], Loss: 0.6368\n",
      "Epoch [17/50], Step [3800/5013], Loss: 0.6126\n",
      "Epoch [17/50], Step [3900/5013], Loss: 0.6124\n",
      "Epoch [17/50], Step [4000/5013], Loss: 0.5828\n",
      "Epoch [17/50], Step [4100/5013], Loss: 0.6014\n",
      "Epoch [17/50], Step [4200/5013], Loss: 0.5832\n",
      "Epoch [17/50], Step [4300/5013], Loss: 0.6524\n",
      "Epoch [17/50], Step [4400/5013], Loss: 0.6253\n",
      "Epoch [17/50], Step [4500/5013], Loss: 0.6164\n",
      "Epoch [17/50], Step [4600/5013], Loss: 0.5982\n",
      "Epoch [17/50], Step [4700/5013], Loss: 0.5811\n",
      "Epoch [17/50], Step [4800/5013], Loss: 0.5923\n",
      "Epoch [17/50], Step [4900/5013], Loss: 0.6176\n",
      "Epoch [17/50], Step [5000/5013], Loss: 0.6346\n",
      "Epoch [17/50] Average Loss: 0.6099\n",
      "Epoch [18/50], Step [100/5013], Loss: 0.6073\n",
      "Epoch [18/50], Step [200/5013], Loss: 0.5862\n",
      "Epoch [18/50], Step [300/5013], Loss: 0.5927\n",
      "Epoch [18/50], Step [400/5013], Loss: 0.5939\n",
      "Epoch [18/50], Step [500/5013], Loss: 0.6006\n",
      "Epoch [18/50], Step [600/5013], Loss: 0.5955\n",
      "Epoch [18/50], Step [700/5013], Loss: 0.5718\n",
      "Epoch [18/50], Step [800/5013], Loss: 0.5871\n",
      "Epoch [18/50], Step [900/5013], Loss: 0.5955\n",
      "Epoch [18/50], Step [1000/5013], Loss: 0.5866\n",
      "Epoch [18/50], Step [1100/5013], Loss: 0.5856\n",
      "Epoch [18/50], Step [1200/5013], Loss: 0.5852\n",
      "Epoch [18/50], Step [1300/5013], Loss: 0.5676\n",
      "Epoch [18/50], Step [1400/5013], Loss: 0.5943\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "gc.collect()\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=50, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e2a57-83da-4cce-9109-c8bca7468376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b347f8-1044-4a3f-8d6e-4d5a8c00feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform evaluation\n",
    "gc.collect()\n",
    "evaluate_model(model, test_loader, criterion, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524fb01c-1bbd-4389-8512-ee71d106f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print((end-start)//60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a969983-6d1c-4954-905e-1d2875a473f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), r\"C:\\Users\\nadav.k\\Documents\\DS\\DL_classification\\saved_models\\two_stas_fulldata_manualparams.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67610d82-6005-4b03-842d-9d9dca2a74c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentinel1Dataset(Dataset):\n",
    "    def __init__(self, sen1_data, labels):\n",
    "        self.sen1_data = sen1_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract Sentinel-1 image and label\n",
    "        sen1_image = self.sen1_data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        sen1_image = torch.tensor(sen1_image, dtype=torch.float32).permute(2, 0, 1)\n",
    "\n",
    "        # Convert one-hot encoded label to class index\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        label = torch.argmax(label).long()\n",
    "\n",
    "        return sen1_image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ce0d4-4e3e-43dd-9c9f-eb4c65325fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = Sentinel1Dataset(train_sen1_data, train_labels)\n",
    "test_dataset = Sentinel1Dataset(test_sen1_data, test_labels)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f151d4-71e7-4edc-80e9-82c2748c90f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentinel1ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=17):\n",
    "        super(Sentinel1ConvNet, self).__init__()\n",
    "\n",
    "        # Sentinel-1 branch\n",
    "        self.conv1 = nn.Conv2d(8, 32, kernel_size=3, padding=1)\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.dropout2 = nn.Dropout(p=0.25)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n",
    "        self.fc1_dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc2_dropout = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sentinel-1 forward pass\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc1_dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc2_dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b69d7-cb5f-4555-ba70-b7b7279f5fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with visualization and memory clearing\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    train_losses = []  # List to store training loss for visualization\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i, (sen1, labels) in enumerate(train_loader):\n",
    "            sen1, labels = sen1.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sen1)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Clear memory for each batch (optional but not usually necessary here)\n",
    "            del outputs, loss\n",
    "\n",
    "            if i % 100 == 99:  # Print every 100 mini-batches\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Average loss for the epoch\n",
    "        epoch_loss /= len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}] Average Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        # Clear unused memory after each epoch\n",
    "        torch.cuda.empty_cache()  # Clear GPU memory\n",
    "        gc.collect()  # Trigger garbage collection for CPU memory\n",
    "\n",
    "    print('Training complete')\n",
    "\n",
    "    # Visualization of training loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o', label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75969c5-dd4a-4c6f-a9f6-2b2abadf6cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sen1, labels in test_loader:\n",
    "            sen1, labels = sen1.to(device), labels.to(device)\n",
    "            outputs = model(sen1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    print(f'Average Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Visualization (optional)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(['Loss', 'Accuracy'], [avg_loss, accuracy])\n",
    "    plt.title('Evaluation Results')\n",
    "    plt.ylabel('Value')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd73b46-3fe0-42a5-9000-11025e1a1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = Sentinel1ConvNet(num_classes=17)  # Adjust num_classes based on your dataset\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaca7df-b667-48f7-9e29-032712940654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "gc.collect()\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=50, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1b622-7fc0-4341-9844-fb5cb66f3c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform evaluation\n",
    "gc.collect()\n",
    "evaluate_model(model, test_loader, criterion, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc1822-175c-4fc3-a3b3-fc3fd26841c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), r\"C:\\Users\\nadav.k\\Documents\\DS\\DL_classification\\saved_models\\sentinal1_fulldata_manuel_params.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8683d571-b5ca-41f0-b748-fdcff78782e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print((end-start)//60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb9330-827b-45bf-9687-27bef20fcedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentinel2Dataset(Dataset):\n",
    "    def __init__(self, sen2_data, labels):\n",
    "        self.sen2_data = sen2_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract Sentinel-2 image and label\n",
    "        sen2_image = self.sen2_data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        sen2_image = torch.tensor(sen2_image, dtype=torch.float32).permute(2, 0, 1)\n",
    "\n",
    "        # Convert one-hot encoded label to class index\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        label = torch.argmax(label).long()\n",
    "\n",
    "        return sen2_image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dad97f-e87e-4736-b3d4-82b6eebd7b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = Sentinel2Dataset(train_sen2_data, train_labels)\n",
    "test_dataset = Sentinel2Dataset(test_sen2_data, test_labels)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ada7e9-6247-47fc-96c4-f9e597a3adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentinel2ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=17):\n",
    "        super(Sentinel2ConvNet, self).__init__()\n",
    "\n",
    "        # Sentinel-2 branch\n",
    "        self.conv1 = nn.Conv2d(10, 32, kernel_size=3, padding=1)  # Adjusted input channels to 10\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.dropout2 = nn.Dropout(p=0.25)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)  # Adjust as per the spatial dimensions of the feature map\n",
    "        self.fc1_dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc2_dropout = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sentinel-2 forward pass\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc1_dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc2_dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43200cb-e242-4835-9745-fcf9306d2c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with visualization and memory clearing for Sentinel-2\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    train_losses = []  # List to store training loss for visualization\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i, (sen2, labels) in enumerate(train_loader):\n",
    "            sen2, labels = sen2.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sen2)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Clear memory for each batch (optional but not usually necessary here)\n",
    "            del outputs, loss\n",
    "\n",
    "            if i % 100 == 99:  # Print every 100 mini-batches\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Average loss for the epoch\n",
    "        epoch_loss /= len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}] Average Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        # Clear unused memory after each epoch\n",
    "        torch.cuda.empty_cache()  # Clear GPU memory\n",
    "        gc.collect()  # Trigger garbage collection for CPU memory\n",
    "\n",
    "    print('Training complete')\n",
    "\n",
    "    # Visualization of training loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o', label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad8f0c-a1a5-40c1-a677-9055be8fb702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sen2, labels in test_loader:\n",
    "            sen2, labels = sen2.to(device), labels.to(device)\n",
    "            outputs = model(sen2)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    print(f'Average Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Visualization (optional)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(['Loss', 'Accuracy'], [avg_loss, accuracy])\n",
    "    plt.title('Evaluation Results')\n",
    "    plt.ylabel('Value')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d69585e-f274-469a-a608-7a34f910d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = Sentinel2ConvNet(num_classes=17)  # Adjust num_classes based on your dataset\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ece99dd-e713-402a-9356-2f076fbd81cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "gc.collect()\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=50, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5d6959-417a-468f-85ff-83c13d131b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform evaluation\n",
    "gc.collect()\n",
    "evaluate_model(model, test_loader, criterion, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3445efa4-422c-493a-a62a-89e25a60c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), r\"C:\\Users\\nadav.k\\Documents\\DS\\DL_classification\\saved_models\\sentinal2_fulldata_manuel_params.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7614130-6dc7-4409-b16c-cf6049861fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print((end-start)//60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
